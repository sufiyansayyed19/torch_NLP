{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 03: Word Embeddings\n",
                "\n",
                "**From Sparse to Dense: Distributed Representations of Words**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand the distributional hypothesis\n",
                "- âœ… Master Word2Vec (CBOW & Skip-gram) with negative sampling\n",
                "- âœ… Implement Skip-gram from scratch\n",
                "- âœ… Explore GloVe and FastText\n",
                "- âœ… Visualize and analyze word embeddings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 02: Text Representation](../02_text_representation/02_text_representation.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Intuition & Motivation\n",
                "\n",
                "### The Problem with One-Hot Encoding\n",
                "\n",
                "```\n",
                "cat  = [1, 0, 0, 0, 0, ...]\n",
                "dog  = [0, 1, 0, 0, 0, ...]\n",
                "king = [0, 0, 1, 0, 0, ...]\n",
                "```\n",
                "\n",
                "**Problems:**\n",
                "- No similarity: `cos(cat, dog) = 0`\n",
                "- Huge dimensions: vocab size = 50,000+\n",
                "- No meaning captured\n",
                "\n",
                "### The Distributional Hypothesis\n",
                "\n",
                "> **\"Words with similar contexts have similar meanings\"**  \n",
                "> â€” J.R. Firth (1957)\n",
                "\n",
                "```\n",
                "\"The cat sat on the mat\"\n",
                "\"The dog sat on the rug\"\n",
                "â†’ 'cat' and 'dog' are similar (same context)\n",
                "```\n",
                "\n",
                "### Word Embeddings: Dense Vectors\n",
                "\n",
                "```\n",
                "cat  = [0.2, -0.4, 0.7, 0.1, ...] (300 dims)\n",
                "dog  = [0.3, -0.3, 0.6, 0.2, ...]\n",
                "king = [0.8, 0.5, -0.2, 0.9, ...]\n",
                "```\n",
                "\n",
                "Now: `cos(cat, dog) â‰ˆ 0.8` (similar!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from collections import Counter\n",
                "from typing import List, Tuple\n",
                "import random\n",
                "\n",
                "# For visualization\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.manifold import TSNE\n",
                "\n",
                "print(\"Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Mathematical Foundation\n",
                "\n",
                "### 4.1 Word2Vec: Two Architectures\n",
                "\n",
                "| Model | Input | Output | Learns |\n",
                "|-------|-------|--------|--------|\n",
                "| **CBOW** | Context words | Center word | \"What word fits here?\" |\n",
                "| **Skip-gram** | Center word | Context words | \"What's around this word?\" |\n",
                "\n",
                "### 4.2 Skip-gram Objective\n",
                "\n",
                "Given center word $w_c$, maximize probability of context words:\n",
                "\n",
                "$$\\max \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w_{t+j} | w_t)$$\n",
                "\n",
                "Where:\n",
                "$$P(w_o | w_c) = \\frac{\\exp(u_o^\\top v_c)}{\\sum_{w \\in V} \\exp(u_w^\\top v_c)}$$\n",
                "\n",
                "- $v_c$ = embedding of center word\n",
                "- $u_o$ = output embedding of context word\n",
                "\n",
                "### 4.3 Negative Sampling\n",
                "\n",
                "Computing softmax over 50K words is expensive!\n",
                "\n",
                "**Solution**: Instead of predicting the right word, distinguish real pairs from fake pairs:\n",
                "\n",
                "$$\\log \\sigma(u_o^\\top v_c) + \\sum_{k=1}^{K} \\mathbb{E}_{w_k \\sim P_n(w)} [\\log \\sigma(-u_{w_k}^\\top v_c)]$$\n",
                "\n",
                "- Sample $K$ negative words (typically 5-20)\n",
                "- Train to distinguish real context from random"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Skip-gram with Negative Sampling from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SkipGram:\n",
                "    \"\"\"Skip-gram with Negative Sampling from scratch.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size: int, embedding_dim: int = 100, neg_samples: int = 5):\n",
                "        self.embedding_dim = embedding_dim\n",
                "        self.neg_samples = neg_samples\n",
                "        self.vocab_size = vocab_size\n",
                "        \n",
                "        # Two embedding matrices: input (center) and output (context)\n",
                "        # Initialize with small random values\n",
                "        self.W_in = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
                "        self.W_out = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
                "    \n",
                "    def sigmoid(self, x):\n",
                "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
                "    \n",
                "    def train_pair(self, center_idx: int, context_idx: int, neg_indices: List[int], lr: float = 0.01):\n",
                "        \"\"\"Train on one (center, context) pair with negative samples.\"\"\"\n",
                "        \n",
                "        # Get embeddings\n",
                "        v_c = self.W_in[center_idx]  # Center word embedding\n",
                "        u_o = self.W_out[context_idx]  # Context word embedding\n",
                "        \n",
                "        # Positive sample: should be 1\n",
                "        score_pos = np.dot(u_o, v_c)\n",
                "        sig_pos = self.sigmoid(score_pos)\n",
                "        \n",
                "        # Gradient for positive sample\n",
                "        grad_pos = (sig_pos - 1) * v_c\n",
                "        grad_center = (sig_pos - 1) * u_o\n",
                "        \n",
                "        # Negative samples: should be 0\n",
                "        for neg_idx in neg_indices:\n",
                "            u_neg = self.W_out[neg_idx]\n",
                "            score_neg = np.dot(u_neg, v_c)\n",
                "            sig_neg = self.sigmoid(score_neg)\n",
                "            \n",
                "            # Gradient for negative sample\n",
                "            self.W_out[neg_idx] -= lr * sig_neg * v_c\n",
                "            grad_center += sig_neg * u_neg\n",
                "        \n",
                "        # Update embeddings\n",
                "        self.W_out[context_idx] -= lr * grad_pos\n",
                "        self.W_in[center_idx] -= lr * grad_center\n",
                "    \n",
                "    def get_embedding(self, word_idx: int) -> np.ndarray:\n",
                "        \"\"\"Get word embedding (use input matrix).\"\"\"\n",
                "        return self.W_in[word_idx]\n",
                "    \n",
                "    def similarity(self, idx1: int, idx2: int) -> float:\n",
                "        \"\"\"Cosine similarity between two words.\"\"\"\n",
                "        v1 = self.W_in[idx1]\n",
                "        v2 = self.W_in[idx2]\n",
                "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training example\n",
                "corpus = [\n",
                "    \"the cat sat on the mat\",\n",
                "    \"the dog ran in the park\",\n",
                "    \"cats and dogs are pets\",\n",
                "    \"the cat chased the dog\",\n",
                "    \"dogs bark and cats meow\"\n",
                "]\n",
                "\n",
                "# Build vocabulary\n",
                "words = ' '.join(corpus).lower().split()\n",
                "word_counts = Counter(words)\n",
                "vocab = {word: idx for idx, word in enumerate(word_counts.keys())}\n",
                "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
                "vocab_size = len(vocab)\n",
                "\n",
                "print(f\"Vocabulary size: {vocab_size}\")\n",
                "print(f\"Vocabulary: {list(vocab.keys())}\")\n",
                "\n",
                "# Generate training pairs (center, context)\n",
                "def generate_pairs(corpus, vocab, window_size=2):\n",
                "    pairs = []\n",
                "    for sentence in corpus:\n",
                "        words = sentence.lower().split()\n",
                "        indices = [vocab[w] for w in words]\n",
                "        for i, center in enumerate(indices):\n",
                "            for j in range(max(0, i-window_size), min(len(indices), i+window_size+1)):\n",
                "                if i != j:\n",
                "                    pairs.append((center, indices[j]))\n",
                "    return pairs\n",
                "\n",
                "pairs = generate_pairs(corpus, vocab)\n",
                "print(f\"Training pairs: {len(pairs)}\")\n",
                "print(f\"Sample pairs: {[(idx_to_word[c], idx_to_word[ctx]) for c, ctx in pairs[:5]]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Skip-gram\n",
                "model = SkipGram(vocab_size, embedding_dim=50, neg_samples=5)\n",
                "\n",
                "epochs = 100\n",
                "for epoch in range(epochs):\n",
                "    random.shuffle(pairs)\n",
                "    for center, context in pairs:\n",
                "        # Sample negative words\n",
                "        neg_indices = random.sample([i for i in range(vocab_size) if i != context], model.neg_samples)\n",
                "        model.train_pair(center, context, neg_indices, lr=0.05)\n",
                "    \n",
                "    if (epoch + 1) % 20 == 0:\n",
                "        sim = model.similarity(vocab['cat'], vocab['dog'])\n",
                "        print(f\"Epoch {epoch+1}: cat-dog similarity = {sim:.3f}\")\n",
                "\n",
                "# Test similarities\n",
                "print(\"\\nSimilarities:\")\n",
                "for w1, w2 in [('cat', 'dog'), ('cat', 'the'), ('dogs', 'cats')]:\n",
                "    if w1 in vocab and w2 in vocab:\n",
                "        print(f\"  {w1} - {w2}: {model.similarity(vocab[w1], vocab[w2]):.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. PyTorch Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SkipGramPyTorch(nn.Module):\n",
                "    \"\"\"Skip-gram with PyTorch.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
                "        super().__init__()\n",
                "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
                "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
                "        \n",
                "        # Initialize\n",
                "        nn.init.xavier_uniform_(self.center_embeddings.weight)\n",
                "        nn.init.xavier_uniform_(self.context_embeddings.weight)\n",
                "    \n",
                "    def forward(self, center, context, neg_context):\n",
                "        \"\"\"\n",
                "        center: (batch,)\n",
                "        context: (batch,)\n",
                "        neg_context: (batch, num_neg)\n",
                "        \"\"\"\n",
                "        # Get embeddings\n",
                "        center_emb = self.center_embeddings(center)  # (batch, dim)\n",
                "        context_emb = self.context_embeddings(context)  # (batch, dim)\n",
                "        neg_emb = self.context_embeddings(neg_context)  # (batch, num_neg, dim)\n",
                "        \n",
                "        # Positive score\n",
                "        pos_score = torch.sum(center_emb * context_emb, dim=1)  # (batch,)\n",
                "        pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-10)\n",
                "        \n",
                "        # Negative scores\n",
                "        neg_score = torch.bmm(neg_emb, center_emb.unsqueeze(2)).squeeze(2)  # (batch, num_neg)\n",
                "        neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-10), dim=1)\n",
                "        \n",
                "        return (pos_loss + neg_loss).mean()\n",
                "    \n",
                "    def get_embedding(self, word_idx):\n",
                "        return self.center_embeddings.weight[word_idx].detach().numpy()\n",
                "\n",
                "# Quick test\n",
                "model_pt = SkipGramPyTorch(vocab_size, 50)\n",
                "print(f\"PyTorch model parameters: {sum(p.numel() for p in model_pt.parameters())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Using Pretrained Embeddings (gensim)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using gensim Word2Vec\n",
                "from gensim.models import Word2Vec\n",
                "\n",
                "# Prepare sentences\n",
                "sentences = [s.lower().split() for s in corpus]\n",
                "\n",
                "# Train Word2Vec\n",
                "w2v = Word2Vec(\n",
                "    sentences,\n",
                "    vector_size=100,\n",
                "    window=2,\n",
                "    min_count=1,\n",
                "    sg=1,  # Skip-gram\n",
                "    epochs=100\n",
                ")\n",
                "\n",
                "print(\"Gensim Word2Vec trained!\")\n",
                "print(f\"\\nMost similar to 'cat':\")\n",
                "for word, score in w2v.wv.most_similar('cat', topn=3):\n",
                "    print(f\"  {word}: {score:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. GloVe & FastText\n",
                "\n",
                "### GloVe (Global Vectors)\n",
                "\n",
                "- Uses **co-occurrence matrix** instead of sliding window\n",
                "- Objective: $w_i^\\top w_j + b_i + b_j = \\log(X_{ij})$\n",
                "- Combines count-based and prediction-based\n",
                "\n",
                "### FastText\n",
                "\n",
                "- Represents words as **sum of character n-grams**\n",
                "- \"where\" = `<wh, whe, her, ere, re>`\n",
                "- **Handles OOV words** (major advantage!)\n",
                "- Better for morphologically rich languages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# FastText with gensim\n",
                "from gensim.models import FastText\n",
                "\n",
                "ft = FastText(\n",
                "    sentences,\n",
                "    vector_size=100,\n",
                "    window=2,\n",
                "    min_count=1,\n",
                "    epochs=100\n",
                ")\n",
                "\n",
                "# FastText can handle OOV!\n",
                "print(\"FastText handles OOV words:\")\n",
                "print(f\"  'catlike' (OOV) embedding shape: {ft.wv['catlike'].shape}\")\n",
                "print(f\"  'doggy' (OOV) embedding shape: {ft.wv['doggy'].shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize embeddings with t-SNE\n",
                "words_to_plot = list(vocab.keys())\n",
                "embeddings = np.array([w2v.wv[w] for w in words_to_plot])\n",
                "\n",
                "# t-SNE reduction\n",
                "tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(words_to_plot)-1))\n",
                "embeddings_2d = tsne.fit_transform(embeddings)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)\n",
                "for i, word in enumerate(words_to_plot):\n",
                "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=10)\n",
                "plt.title('Word Embeddings Visualization (t-SNE)')\n",
                "plt.xlabel('Dimension 1')\n",
                "plt.ylabel('Dimension 2')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. ðŸ”¥ Real-World Usage\n",
                "\n",
                "### In 2024\n",
                "\n",
                "Static embeddings (Word2Vec, GloVe) are **mostly replaced** by contextual embeddings (BERT).\n",
                "\n",
                "**Still useful for:**\n",
                "- Quick prototyping\n",
                "- Understanding embedding concepts\n",
                "- Limited compute environments\n",
                "- Initialization for other models\n",
                "\n",
                "### Pretrained Sources\n",
                "\n",
                "| Source | Size | Languages |\n",
                "|--------|------|----------|\n",
                "| GloVe (Stanford) | 6B, 840B tokens | English |\n",
                "| FastText (Facebook) | 157 languages | Multilingual |\n",
                "| Word2Vec (Google News) | 3M words | English |\n",
                "\n",
                "### Key Rule\n",
                "> **Always use pretrained embeddings. Never train from scratch.**\n",
                "> (Unless you have a very specific domain)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Interview Questions\n",
                "\n",
                "**Q1: What's the difference between CBOW and Skip-gram?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- **CBOW**: Predicts center word from context (faster, better for frequent words)\n",
                "- **Skip-gram**: Predicts context from center word (better for rare words)\n",
                "</details>\n",
                "\n",
                "**Q2: How does negative sampling work?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Instead of softmax over vocab, train binary classifier to distinguish real (center, context) pairs from random pairs. Sample K negative examples proportional to word frequency.\n",
                "</details>\n",
                "\n",
                "**Q3: What are limitations of Word2Vec?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- Single vector per word (no polysemy: \"bank\" = river bank or money bank)\n",
                "- OOV words not handled\n",
                "- Static (no context)\n",
                "</details>\n",
                "\n",
                "**Q4: Why is FastText better for OOV?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "FastText represents words as sum of character n-grams. Even unseen words share n-grams with training words.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Summary\n",
                "\n",
                "- **Distributional Hypothesis**: Similar context â†’ similar meaning\n",
                "- **Word2Vec**: CBOW (contextâ†’word) or Skip-gram (wordâ†’context)\n",
                "- **Negative Sampling**: Efficient training via binary classification\n",
                "- **GloVe**: Co-occurrence matrix approach\n",
                "- **FastText**: Character n-grams, handles OOV\n",
                "- **2024**: Use pretrained, prefer contextual (BERT) when possible"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Exercises\n",
                "\n",
                "1. Implement CBOW from scratch\n",
                "2. Load pretrained GloVe and find word analogies (king - man + woman = ?)\n",
                "3. Compare Word2Vec vs FastText on domain-specific text\n",
                "4. Analyze bias in embeddings (gender associations)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. References\n",
                "\n",
                "- [Word2Vec Paper](https://arxiv.org/abs/1301.3781)\n",
                "- [GloVe Paper](https://nlp.stanford.edu/pubs/glove.pdf)\n",
                "- [FastText Paper](https://arxiv.org/abs/1607.04606)\n",
                "- [Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)\n",
                "\n",
                "---\n",
                "**Next:** [Module 04: PyTorch Embedding Layers](../04_pytorch_embeddings/04_pytorch_embeddings.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}