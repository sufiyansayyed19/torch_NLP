{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 06: LSTM (Long Short-Term Memory)\n",
                "\n",
                "**Solving Vanishing Gradients with Gating Mechanisms**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand LSTM architecture and all gates\n",
                "- âœ… Implement LSTM from scratch\n",
                "- âœ… Know how cell state solves vanishing gradients\n",
                "- âœ… Use PyTorch nn.LSTM effectively"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 05: RNN Fundamentals](../05_rnn_fundamentals/05_rnn_fundamentals.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Intuition & Motivation\n",
                "\n",
                "### The Problem with Vanilla RNN\n",
                "\n",
                "- Vanishing gradients for long sequences\n",
                "- Can't learn long-term dependencies\n",
                "\n",
                "### LSTM Solution: Cell State as \"Conveyor Belt\"\n",
                "\n",
                "```\n",
                "Cell State (C): â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>\n",
                "                    â†‘ add        â†‘ add        â†‘ add\n",
                "                    â”‚            â”‚            â”‚\n",
                "                  [Ã—forget]    [Ã—forget]    [Ã—forget]\n",
                "                    â”‚            â”‚            â”‚\n",
                "Hidden (h): â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€>\n",
                "```\n",
                "\n",
                "Key idea: Information flows through cell state with **additive** updates (not multiplicative like RNN)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(\"Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Mathematical Foundation\n",
                "\n",
                "### LSTM Gates (at each time step t)\n",
                "\n",
                "**1. Forget Gate** - What to forget from cell state:\n",
                "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
                "\n",
                "**2. Input Gate** - What new info to add:\n",
                "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
                "\n",
                "**3. Candidate Cell State** - New candidate values:\n",
                "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
                "\n",
                "**4. Cell State Update** - Forget old + add new:\n",
                "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
                "\n",
                "**5. Output Gate** - What to output:\n",
                "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
                "\n",
                "**6. Hidden State** - Filtered cell state:\n",
                "$$h_t = o_t \\odot \\tanh(C_t)$$\n",
                "\n",
                "Where $\\sigma$ is sigmoid, $\\odot$ is element-wise multiplication."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. LSTM from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTMCell:\n",
                "    \"\"\"LSTM cell from scratch (NumPy).\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size: int, hidden_size: int):\n",
                "        self.hidden_size = hidden_size\n",
                "        combined_size = input_size + hidden_size\n",
                "        \n",
                "        # Xavier initialization\n",
                "        scale = np.sqrt(2.0 / combined_size)\n",
                "        \n",
                "        # Forget gate\n",
                "        self.W_f = np.random.randn(hidden_size, combined_size) * scale\n",
                "        self.b_f = np.ones((hidden_size, 1))  # Start with forget bias = 1\n",
                "        \n",
                "        # Input gate\n",
                "        self.W_i = np.random.randn(hidden_size, combined_size) * scale\n",
                "        self.b_i = np.zeros((hidden_size, 1))\n",
                "        \n",
                "        # Candidate\n",
                "        self.W_c = np.random.randn(hidden_size, combined_size) * scale\n",
                "        self.b_c = np.zeros((hidden_size, 1))\n",
                "        \n",
                "        # Output gate\n",
                "        self.W_o = np.random.randn(hidden_size, combined_size) * scale\n",
                "        self.b_o = np.zeros((hidden_size, 1))\n",
                "    \n",
                "    def sigmoid(self, x):\n",
                "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
                "    \n",
                "    def forward(self, x, h_prev, c_prev):\n",
                "        \"\"\"\n",
                "        Single LSTM step.\n",
                "        \n",
                "        Args:\n",
                "            x: (input_size, 1)\n",
                "            h_prev: (hidden_size, 1)\n",
                "            c_prev: (hidden_size, 1)\n",
                "        \"\"\"\n",
                "        # Concatenate input and previous hidden\n",
                "        combined = np.vstack([h_prev, x])  # (hidden+input, 1)\n",
                "        \n",
                "        # Forget gate\n",
                "        f = self.sigmoid(self.W_f @ combined + self.b_f)\n",
                "        \n",
                "        # Input gate\n",
                "        i = self.sigmoid(self.W_i @ combined + self.b_i)\n",
                "        \n",
                "        # Candidate\n",
                "        c_tilde = np.tanh(self.W_c @ combined + self.b_c)\n",
                "        \n",
                "        # Cell state update\n",
                "        c = f * c_prev + i * c_tilde\n",
                "        \n",
                "        # Output gate\n",
                "        o = self.sigmoid(self.W_o @ combined + self.b_o)\n",
                "        \n",
                "        # Hidden state\n",
                "        h = o * np.tanh(c)\n",
                "        \n",
                "        return h, c, {'f': f, 'i': i, 'o': o, 'c_tilde': c_tilde}\n",
                "\n",
                "# Test LSTM cell\n",
                "cell = LSTMCell(input_size=10, hidden_size=20)\n",
                "x = np.random.randn(10, 1)\n",
                "h = np.zeros((20, 1))\n",
                "c = np.zeros((20, 1))\n",
                "\n",
                "h_new, c_new, gates = cell.forward(x, h, c)\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Hidden shape: {h_new.shape}\")\n",
                "print(f\"Cell shape: {c_new.shape}\")\n",
                "print(f\"Gate values (sample): f={gates['f'][0,0]:.3f}, i={gates['i'][0,0]:.3f}, o={gates['o'][0,0]:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTM:\n",
                "    \"\"\"Full LSTM layer from scratch.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size: int, hidden_size: int):\n",
                "        self.cell = LSTMCell(input_size, hidden_size)\n",
                "        self.hidden_size = hidden_size\n",
                "    \n",
                "    def forward(self, inputs, h0=None, c0=None):\n",
                "        \"\"\"\n",
                "        Process sequence.\n",
                "        \n",
                "        Args:\n",
                "            inputs: List of (input_size, 1) arrays\n",
                "        \"\"\"\n",
                "        if h0 is None:\n",
                "            h0 = np.zeros((self.hidden_size, 1))\n",
                "        if c0 is None:\n",
                "            c0 = np.zeros((self.hidden_size, 1))\n",
                "        \n",
                "        h, c = h0, c0\n",
                "        outputs = []\n",
                "        \n",
                "        for x in inputs:\n",
                "            h, c, _ = self.cell.forward(x, h, c)\n",
                "            outputs.append(h)\n",
                "        \n",
                "        return outputs, (h, c)\n",
                "\n",
                "# Test full LSTM\n",
                "lstm = LSTM(input_size=10, hidden_size=20)\n",
                "seq = [np.random.randn(10, 1) for _ in range(15)]\n",
                "outputs, (h_n, c_n) = lstm.forward(seq)\n",
                "\n",
                "print(f\"Sequence length: {len(seq)}\")\n",
                "print(f\"Number of outputs: {len(outputs)}\")\n",
                "print(f\"Final hidden shape: {h_n.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. PyTorch Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PyTorch LSTM\n",
                "lstm_pt = nn.LSTM(\n",
                "    input_size=10,\n",
                "    hidden_size=20,\n",
                "    num_layers=2,\n",
                "    batch_first=True,\n",
                "    dropout=0.1,\n",
                "    bidirectional=False\n",
                ")\n",
                "\n",
                "# Input: (batch, seq, features)\n",
                "x = torch.randn(32, 15, 10)\n",
                "\n",
                "# Initial states: (num_layers, batch, hidden)\n",
                "h0 = torch.zeros(2, 32, 20)\n",
                "c0 = torch.zeros(2, 32, 20)\n",
                "\n",
                "# Forward\n",
                "output, (h_n, c_n) = lstm_pt(x, (h0, c0))\n",
                "\n",
                "print(f\"Input: {x.shape}\")\n",
                "print(f\"Output: {output.shape} (all hidden states)\")\n",
                "print(f\"h_n: {h_n.shape} (final hidden per layer)\")\n",
                "print(f\"c_n: {c_n.shape} (final cell per layer)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Why LSTM Solves Vanishing Gradients"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare gradient flow: RNN vs LSTM\n",
                "def compare_gradients(seq_lengths):\n",
                "    rnn_norms, lstm_norms = [], []\n",
                "    \n",
                "    for seq_len in seq_lengths:\n",
                "        # RNN\n",
                "        rnn = nn.RNN(10, 20, batch_first=True)\n",
                "        x_rnn = torch.randn(1, seq_len, 10, requires_grad=True)\n",
                "        out_rnn, _ = rnn(x_rnn)\n",
                "        out_rnn[:, -1].sum().backward()\n",
                "        rnn_norms.append(x_rnn.grad[:, 0].norm().item())\n",
                "        \n",
                "        # LSTM\n",
                "        lstm = nn.LSTM(10, 20, batch_first=True)\n",
                "        x_lstm = torch.randn(1, seq_len, 10, requires_grad=True)\n",
                "        out_lstm, _ = lstm(x_lstm)\n",
                "        out_lstm[:, -1].sum().backward()\n",
                "        lstm_norms.append(x_lstm.grad[:, 0].norm().item())\n",
                "    \n",
                "    return rnn_norms, lstm_norms\n",
                "\n",
                "seq_lengths = [10, 25, 50, 100, 200]\n",
                "rnn_norms, lstm_norms = compare_gradients(seq_lengths)\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(seq_lengths, rnn_norms, 'o-', label='RNN')\n",
                "plt.plot(seq_lengths, lstm_norms, 's-', label='LSTM')\n",
                "plt.xlabel('Sequence Length')\n",
                "plt.ylabel('Gradient Norm')\n",
                "plt.title('Gradient Flow: RNN vs LSTM')\n",
                "plt.legend()\n",
                "plt.yscale('log')\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "print(\"LSTM maintains gradients much better over long sequences!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. LSTM for Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTMClassifier(nn.Module):\n",
                "    \"\"\"LSTM for text classification.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=1):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
                "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
                "                            batch_first=True, dropout=0.3 if num_layers > 1 else 0)\n",
                "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (batch, seq_len)\n",
                "        embedded = self.embedding(x)  # (batch, seq, embed)\n",
                "        _, (h_n, _) = self.lstm(embedded)  # h_n: (layers, batch, hidden)\n",
                "        out = self.fc(h_n[-1])  # Use last layer's hidden\n",
                "        return out\n",
                "\n",
                "# Test\n",
                "model = LSTMClassifier(vocab_size=5000, embed_dim=100, hidden_dim=128, num_classes=2)\n",
                "x = torch.randint(0, 5000, (32, 50))  # batch=32, seq=50\n",
                "out = model(x)\n",
                "print(f\"Input: {x.shape} â†’ Output: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. ðŸ”¥ Real-World Usage\n",
                "\n",
                "### When LSTM Still Makes Sense (2024)\n",
                "\n",
                "| Use Case | Why LSTM |\n",
                "|----------|----------|\n",
                "| Streaming data | Process token-by-token |\n",
                "| Edge devices | Lower memory than transformers |\n",
                "| Small datasets | Less prone to overfitting |\n",
                "| Time series | Often matches transformer performance |\n",
                "\n",
                "### LSTM vs Transformer\n",
                "\n",
                "| Aspect | LSTM | Transformer |\n",
                "|--------|------|-------------|\n",
                "| Parallelization | Sequential | Fully parallel |\n",
                "| Memory | O(1) per step | O(nÂ²) attention |\n",
                "| Long-range | Limited (~100-200) | Better with pos encoding |\n",
                "| Training speed | Slower | Faster |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Questions\n",
                "\n",
                "**Q1: Walk through all LSTM gates. What does each do?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- **Forget gate (f)**: Decides what to forget from cell state (0-1 per dimension)\n",
                "- **Input gate (i)**: Decides what new info to add\n",
                "- **Candidate (cÌƒ)**: Proposes new values to add\n",
                "- **Cell update**: C = fâŠ™C_prev + iâŠ™cÌƒ\n",
                "- **Output gate (o)**: Decides what parts of cell to output\n",
                "- **Hidden**: h = oâŠ™tanh(C)\n",
                "</details>\n",
                "\n",
                "**Q2: How does cell state help with vanishing gradients?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Cell state uses **additive** updates (C = fâŠ™C_prev + iâŠ™cÌƒ) instead of multiplicative. Gradients flow directly through the addition, creating a \"gradient highway\". The forget gate can be close to 1, allowing information to persist.\n",
                "</details>\n",
                "\n",
                "**Q3: What's the difference between hidden state and cell state?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- **Cell state (C)**: Long-term memory, flows with minimal transformation\n",
                "- **Hidden state (h)**: Short-term/working memory, filtered version of cell state (h = oâŠ™tanh(C))\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "- **LSTM**: Gated architecture solving vanishing gradients\n",
                "- **Gates**: Forget (f), Input (i), Output (o)\n",
                "- **Cell state**: Long-term memory with additive updates\n",
                "- **Key insight**: C = fâŠ™C_prev + iâŠ™cÌƒ (additive, not multiplicative)\n",
                "- **Practice**: Still useful for streaming and resource-constrained settings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Exercises\n",
                "\n",
                "1. Implement backward pass for LSTM from scratch\n",
                "2. Visualize gate activations over a sequence\n",
                "3. Compare LSTM vs RNN on long-range dependency task\n",
                "4. Implement peephole LSTM variant"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. References\n",
                "\n",
                "- [LSTM Original Paper (1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
                "- [Understanding LSTMs (Colah's Blog)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
                "- [PyTorch LSTM Docs](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
                "\n",
                "---\n",
                "**Next:** [Module 07: GRU](../07_gru/07_gru.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}