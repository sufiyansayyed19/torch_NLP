{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 13: Attention Mechanism\n",
                "\n",
                "**The Bridge to Transformers**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand attention intuition\n",
                "- âœ… Implement Bahdanau (additive) attention\n",
                "- âœ… Implement Luong (multiplicative) attention\n",
                "- âœ… Understand self-attention (foundation for Transformers)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 12: Sequence-to-Sequence](../12_seq2seq/12_seq2seq.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Why Attention?\n",
                "\n",
                "### The Bottleneck Problem\n",
                "\n",
                "In vanilla Seq2Seq:\n",
                "```\n",
                "\"The cat sat on the mat\" â†’ [single context vector] â†’ Translation\n",
                "```\n",
                "\n",
                "**Problem**: All information compressed into ONE vector!\n",
                "\n",
                "### Attention Solution\n",
                "\n",
                "```\n",
                "Decoder can \"look back\" at ALL encoder states:\n",
                "\n",
                "When generating \"chat\" (French for cat):\n",
                "  â†’ Attend to \"cat\" in source\n",
                "  \n",
                "When generating \"tapis\" (French for mat):\n",
                "  â†’ Attend to \"mat\" in source\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Attention Equation\n",
                "\n",
                "### Core Formula\n",
                "\n",
                "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\text{score}(Q, K)\\right) \\cdot V$$\n",
                "\n",
                "Where:\n",
                "- **Q** (Query): What we're looking for\n",
                "- **K** (Key): What we match against\n",
                "- **V** (Value): What we return\n",
                "\n",
                "### Score Functions\n",
                "\n",
                "| Type | Formula | Name |\n",
                "|------|---------|------|\n",
                "| Additive | $v^T \\tanh(W_q q + W_k k)$ | Bahdanau |\n",
                "| Dot-product | $q^T k$ | Luong (dot) |\n",
                "| Scaled dot | $\\frac{q^T k}{\\sqrt{d_k}}$ | Transformer |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Bahdanau (Additive) Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BahdanauAttention(nn.Module):\n",
                "    \"\"\"Additive attention (Bahdanau et al., 2015).\"\"\"\n",
                "    \n",
                "    def __init__(self, hidden_dim):\n",
                "        super().__init__()\n",
                "        self.W_q = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
                "        self.W_k = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
                "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
                "    \n",
                "    def forward(self, query, keys, mask=None):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            query: (batch, hidden) - decoder hidden state\n",
                "            keys: (batch, src_len, hidden) - encoder outputs\n",
                "            mask: (batch, src_len) - padding mask\n",
                "        Returns:\n",
                "            context: (batch, hidden)\n",
                "            weights: (batch, src_len)\n",
                "        \"\"\"\n",
                "        # query: (batch, 1, hidden)\n",
                "        query = query.unsqueeze(1)\n",
                "        \n",
                "        # Score: v^T tanh(Wq*q + Wk*k)\n",
                "        scores = self.v(torch.tanh(self.W_q(query) + self.W_k(keys)))  # (batch, src_len, 1)\n",
                "        scores = scores.squeeze(-1)  # (batch, src_len)\n",
                "        \n",
                "        if mask is not None:\n",
                "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
                "        \n",
                "        weights = F.softmax(scores, dim=1)  # (batch, src_len)\n",
                "        context = torch.bmm(weights.unsqueeze(1), keys).squeeze(1)  # (batch, hidden)\n",
                "        \n",
                "        return context, weights\n",
                "\n",
                "# Test\n",
                "attn = BahdanauAttention(hidden_dim=128)\n",
                "query = torch.randn(2, 128)  # batch=2\n",
                "keys = torch.randn(2, 10, 128)  # src_len=10\n",
                "context, weights = attn(query, keys)\n",
                "print(f\"Context: {context.shape}, Weights: {weights.shape}\")\n",
                "print(f\"Weights sum: {weights.sum(dim=1)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Luong (Multiplicative) Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LuongAttention(nn.Module):\n",
                "    \"\"\"Multiplicative attention (Luong et al., 2015).\"\"\"\n",
                "    \n",
                "    def __init__(self, hidden_dim, method='dot'):\n",
                "        super().__init__()\n",
                "        self.method = method\n",
                "        if method == 'general':\n",
                "            self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
                "    \n",
                "    def forward(self, query, keys, mask=None):\n",
                "        \"\"\"\n",
                "        query: (batch, hidden)\n",
                "        keys: (batch, src_len, hidden)\n",
                "        \"\"\"\n",
                "        if self.method == 'dot':\n",
                "            # score = q^T k\n",
                "            scores = torch.bmm(keys, query.unsqueeze(2)).squeeze(2)  # (batch, src_len)\n",
                "        elif self.method == 'general':\n",
                "            # score = q^T W k\n",
                "            scores = torch.bmm(keys, self.W(query).unsqueeze(2)).squeeze(2)\n",
                "        \n",
                "        if mask is not None:\n",
                "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
                "        \n",
                "        weights = F.softmax(scores, dim=1)\n",
                "        context = torch.bmm(weights.unsqueeze(1), keys).squeeze(1)\n",
                "        \n",
                "        return context, weights\n",
                "\n",
                "# Test\n",
                "for method in ['dot', 'general']:\n",
                "    attn = LuongAttention(128, method=method)\n",
                "    context, weights = attn(query, keys)\n",
                "    print(f\"{method}: Context {context.shape}, Weights {weights.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Scaled Dot-Product Attention (Transformers!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scaled_dot_product_attention(query, key, value, mask=None):\n",
                "    \"\"\"\n",
                "    Transformer-style attention.\n",
                "    \n",
                "    Args:\n",
                "        query: (batch, seq_q, d_k)\n",
                "        key: (batch, seq_k, d_k)\n",
                "        value: (batch, seq_k, d_v)\n",
                "    \"\"\"\n",
                "    d_k = query.size(-1)\n",
                "    \n",
                "    # scores = Q K^T / sqrt(d_k)\n",
                "    scores = torch.bmm(query, key.transpose(1, 2)) / (d_k ** 0.5)  # (batch, seq_q, seq_k)\n",
                "    \n",
                "    if mask is not None:\n",
                "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
                "    \n",
                "    weights = F.softmax(scores, dim=-1)\n",
                "    output = torch.bmm(weights, value)  # (batch, seq_q, d_v)\n",
                "    \n",
                "    return output, weights\n",
                "\n",
                "# Test\n",
                "Q = torch.randn(2, 5, 64)  # batch=2, seq=5, dim=64\n",
                "K = torch.randn(2, 10, 64)  # source seq=10\n",
                "V = torch.randn(2, 10, 64)\n",
                "\n",
                "output, weights = scaled_dot_product_attention(Q, K, V)\n",
                "print(f\"Output: {output.shape}, Weights: {weights.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Self-Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SelfAttention(nn.Module):\n",
                "    \"\"\"\n",
                "    Self-attention: Q, K, V all come from same sequence.\n",
                "    Foundation of Transformers!\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, embed_dim):\n",
                "        super().__init__()\n",
                "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
                "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
                "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
                "    \n",
                "    def forward(self, x, mask=None):\n",
                "        \"\"\"\n",
                "        x: (batch, seq, embed)\n",
                "        \"\"\"\n",
                "        Q = self.W_q(x)\n",
                "        K = self.W_k(x)\n",
                "        V = self.W_v(x)\n",
                "        \n",
                "        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
                "        return output, weights\n",
                "\n",
                "# Test\n",
                "self_attn = SelfAttention(embed_dim=64)\n",
                "x = torch.randn(2, 8, 64)  # batch=2, seq=8\n",
                "output, weights = self_attn(x)\n",
                "print(f\"Self-attention output: {output.shape}\")\n",
                "print(f\"Each token attends to all tokens: weights shape = {weights.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Visualizing Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_attention(weights, src_tokens, tgt_tokens=None):\n",
                "    \"\"\"Visualize attention weights.\"\"\"\n",
                "    weights = weights.detach().cpu().numpy()\n",
                "    \n",
                "    plt.figure(figsize=(8, 6))\n",
                "    plt.imshow(weights, cmap='Blues')\n",
                "    plt.colorbar()\n",
                "    \n",
                "    if tgt_tokens:\n",
                "        plt.yticks(range(len(tgt_tokens)), tgt_tokens)\n",
                "    plt.xticks(range(len(src_tokens)), src_tokens, rotation=45)\n",
                "    \n",
                "    plt.xlabel('Source')\n",
                "    plt.ylabel('Target')\n",
                "    plt.title('Attention Weights')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Simulated translation attention\n",
                "src_tokens = ['the', 'cat', 'sat', 'on', 'mat']\n",
                "tgt_tokens = ['le', 'chat', 'assis', 'sur', 'tapis']\n",
                "\n",
                "# Simulated diagonal-ish attention (ideal for translation)\n",
                "weights = torch.softmax(torch.randn(5, 5) + 2*torch.eye(5), dim=1)\n",
                "visualize_attention(weights, src_tokens, tgt_tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. ðŸ”¥ Real-World Usage\n",
                "\n",
                "### Attention Types Summary\n",
                "\n",
                "| Type | Use Case | Key Insight |\n",
                "|------|----------|-------------|\n",
                "| **Bahdanau** | Seq2Seq translation | Additive, learned |\n",
                "| **Luong** | Seq2Seq (simpler) | Multiplicative |\n",
                "| **Self-attention** | Transformers | Q=K=V from same input |\n",
                "| **Cross-attention** | Decoder attends to encoder | Q from decoder, K/V from encoder |\n",
                "\n",
                "### Transformers = Self-Attention + More\n",
                "- **BERT**: Encoder with self-attention\n",
                "- **GPT**: Decoder with causal self-attention\n",
                "- **T5**: Encoder-decoder with cross-attention"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Interview Questions\n",
                "\n",
                "**Q1: What is the difference between attention and self-attention?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- **Attention**: Query from one sequence (decoder), Keys/Values from another (encoder)\n",
                "- **Self-attention**: Q, K, V all come from the same sequence\n",
                "</details>\n",
                "\n",
                "**Q2: Why do we scale by sqrt(d_k)?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "For large d_k, dot products grow large in magnitude, pushing softmax into regions with tiny gradients. Dividing by sqrt(d_k) keeps variance stable.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Summary\n",
                "\n",
                "- **Attention**: Dynamic focusing on relevant parts\n",
                "- **Q, K, V**: Query, Key, Value paradigm\n",
                "- **Bahdanau**: Additive attention (v^T tanh(...))\n",
                "- **Luong**: Multiplicative (q^T k)\n",
                "- **Self-attention**: Same sequence for Q, K, V â†’ Transformers!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. References\n",
                "\n",
                "- [Bahdanau et al. 2015](https://arxiv.org/abs/1409.0473)\n",
                "- [Luong et al. 2015](https://arxiv.org/abs/1508.04025)\n",
                "- [Attention Is All You Need (2017)](https://arxiv.org/abs/1706.03762)\n",
                "\n",
                "---\n",
                "**Next:** [Module 14: Transformer Architecture](../14_transformer_architecture/14_transformer_architecture.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}