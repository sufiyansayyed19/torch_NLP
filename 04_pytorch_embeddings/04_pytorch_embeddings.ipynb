{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 04: PyTorch Embedding Layers\n",
                "\n",
                "**Mastering nn.Embedding for Deep Learning NLP**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand nn.Embedding as a lookup table\n",
                "- âœ… Initialize embeddings (random, pretrained)\n",
                "- âœ… Load GloVe/FastText into PyTorch\n",
                "- âœ… Handle padding correctly\n",
                "- âœ… Freeze vs fine-tune embeddings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 03: Word Embeddings](../03_word_embeddings/03_word_embeddings.ipynb)\n",
                "- PyTorch basics (tensors, nn.Module)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Intuition & Motivation\n",
                "\n",
                "### What is nn.Embedding?\n",
                "\n",
                "Simply a **lookup table** that maps indices to vectors:\n",
                "\n",
                "```\n",
                "Index:  0 â†’ [0.2, -0.4, 0.7, ...]\n",
                "        1 â†’ [0.3, -0.3, 0.6, ...]\n",
                "        2 â†’ [0.8, 0.5, -0.2, ...]\n",
                "```\n",
                "\n",
                "### Dimensions\n",
                "\n",
                "```\n",
                "Input:  [batch_size, seq_len]        (indices)\n",
                "Output: [batch_size, seq_len, embed_dim]  (vectors)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "from typing import Dict, List\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. nn.Embedding Basics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create embedding layer\n",
                "vocab_size = 1000\n",
                "embedding_dim = 300\n",
                "\n",
                "embedding = nn.Embedding(\n",
                "    num_embeddings=vocab_size,  # Size of vocabulary\n",
                "    embedding_dim=embedding_dim  # Dimension of each vector\n",
                ")\n",
                "\n",
                "print(f\"Embedding weight shape: {embedding.weight.shape}\")\n",
                "print(f\"Total parameters: {embedding.weight.numel():,}\")\n",
                "\n",
                "# Forward pass\n",
                "input_indices = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])  # (batch=2, seq=4)\n",
                "output = embedding(input_indices)\n",
                "print(f\"\\nInput shape: {input_indices.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Initialization Strategies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Default (Normal distribution)\n",
                "emb_default = nn.Embedding(1000, 300)\n",
                "print(f\"Default init - mean: {emb_default.weight.mean():.4f}, std: {emb_default.weight.std():.4f}\")\n",
                "\n",
                "# 2. Xavier/Glorot\n",
                "emb_xavier = nn.Embedding(1000, 300)\n",
                "nn.init.xavier_uniform_(emb_xavier.weight)\n",
                "print(f\"Xavier init - mean: {emb_xavier.weight.mean():.4f}, std: {emb_xavier.weight.std():.4f}\")\n",
                "\n",
                "# 3. Uniform in range\n",
                "emb_uniform = nn.Embedding(1000, 300)\n",
                "nn.init.uniform_(emb_uniform.weight, -0.1, 0.1)\n",
                "print(f\"Uniform init - mean: {emb_uniform.weight.mean():.4f}, std: {emb_uniform.weight.std():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Loading Pretrained Embeddings (GloVe)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_glove(glove_path: str, vocab: Dict[str, int], embedding_dim: int) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Load GloVe embeddings for vocabulary.\n",
                "    \n",
                "    Args:\n",
                "        glove_path: Path to GloVe file (e.g., glove.6B.300d.txt)\n",
                "        vocab: Dictionary mapping word -> index\n",
                "        embedding_dim: Dimension of embeddings\n",
                "    \n",
                "    Returns:\n",
                "        Embedding matrix of shape (vocab_size, embedding_dim)\n",
                "    \"\"\"\n",
                "    # Initialize with random vectors\n",
                "    embedding_matrix = np.random.randn(len(vocab), embedding_dim) * 0.01\n",
                "    found = 0\n",
                "    \n",
                "    # Read GloVe file\n",
                "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            values = line.split()\n",
                "            word = values[0]\n",
                "            if word in vocab:\n",
                "                idx = vocab[word]\n",
                "                embedding_matrix[idx] = np.array(values[1:], dtype=np.float32)\n",
                "                found += 1\n",
                "    \n",
                "    print(f\"Found {found}/{len(vocab)} words in GloVe ({100*found/len(vocab):.1f}%)\")\n",
                "    return embedding_matrix\n",
                "\n",
                "# Example usage (would need actual GloVe file)\n",
                "# vocab = {'the': 0, 'cat': 1, 'sat': 2, 'on': 3, 'mat': 4}\n",
                "# embeddings = load_glove('glove.6B.300d.txt', vocab, 300)\n",
                "\n",
                "print(\"GloVe loading function ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_embedding_layer(pretrained_weights: np.ndarray, freeze: bool = True) -> nn.Embedding:\n",
                "    \"\"\"\n",
                "    Create embedding layer from pretrained weights.\n",
                "    \n",
                "    Args:\n",
                "        pretrained_weights: NumPy array of shape (vocab_size, embed_dim)\n",
                "        freeze: If True, embeddings won't be updated during training\n",
                "    \"\"\"\n",
                "    vocab_size, embedding_dim = pretrained_weights.shape\n",
                "    embedding = nn.Embedding(vocab_size, embedding_dim)\n",
                "    \n",
                "    # Load weights\n",
                "    embedding.weight = nn.Parameter(\n",
                "        torch.from_numpy(pretrained_weights).float(),\n",
                "        requires_grad=not freeze\n",
                "    )\n",
                "    \n",
                "    print(f\"Created embedding: {vocab_size} x {embedding_dim}, frozen={freeze}\")\n",
                "    return embedding\n",
                "\n",
                "# Demo with random weights\n",
                "demo_weights = np.random.randn(1000, 300).astype(np.float32)\n",
                "frozen_emb = create_embedding_layer(demo_weights, freeze=True)\n",
                "trainable_emb = create_embedding_layer(demo_weights, freeze=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Padding and Masking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Padding index: embedding for padding tokens should be zeros\n",
                "PAD_IDX = 0\n",
                "\n",
                "embedding = nn.Embedding(\n",
                "    num_embeddings=1000,\n",
                "    embedding_dim=300,\n",
                "    padding_idx=PAD_IDX  # This index will always be zeros\n",
                ")\n",
                "\n",
                "# Check padding embedding\n",
                "print(f\"Padding embedding (should be zeros):\")\n",
                "print(f\"  Sum: {embedding.weight[PAD_IDX].sum().item()}\")\n",
                "print(f\"  Requires grad: {embedding.weight[PAD_IDX].requires_grad}\")\n",
                "\n",
                "# Example with padded sequence\n",
                "padded_input = torch.tensor([\n",
                "    [1, 2, 3, 0, 0],  # Sequence of length 3, padded to 5\n",
                "    [4, 5, 0, 0, 0]   # Sequence of length 2, padded to 5\n",
                "])\n",
                "\n",
                "output = embedding(padded_input)\n",
                "print(f\"\\nOutput shape: {output.shape}\")\n",
                "print(f\"Padding positions are zero vectors: {output[0, 3].sum().item() == 0}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Complete Embedding Module for NLP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextEmbedding(nn.Module):\n",
                "    \"\"\"\n",
                "    Text embedding module with optional pretrained weights.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        vocab_size: int,\n",
                "        embedding_dim: int,\n",
                "        padding_idx: int = 0,\n",
                "        pretrained_weights: np.ndarray = None,\n",
                "        freeze: bool = False,\n",
                "        dropout: float = 0.0\n",
                "    ):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.embedding = nn.Embedding(\n",
                "            vocab_size, embedding_dim, padding_idx=padding_idx\n",
                "        )\n",
                "        \n",
                "        # Load pretrained if provided\n",
                "        if pretrained_weights is not None:\n",
                "            self.embedding.weight = nn.Parameter(\n",
                "                torch.from_numpy(pretrained_weights).float(),\n",
                "                requires_grad=not freeze\n",
                "            )\n",
                "        \n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            x: (batch_size, seq_len) token indices\n",
                "        Returns:\n",
                "            (batch_size, seq_len, embedding_dim) embeddings\n",
                "        \"\"\"\n",
                "        return self.dropout(self.embedding(x))\n",
                "\n",
                "# Example\n",
                "text_emb = TextEmbedding(vocab_size=5000, embedding_dim=300, dropout=0.1)\n",
                "x = torch.randint(0, 5000, (32, 100))  # batch=32, seq_len=100\n",
                "output = text_emb(x)\n",
                "print(f\"Input: {x.shape} â†’ Output: {output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. ðŸ”¥ Real-World Usage\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "| Scenario | Recommendation |\n",
                "|----------|----------------|\n",
                "| Small data | Use pretrained, **freeze** |\n",
                "| Medium data | Use pretrained, **fine-tune** |\n",
                "| Large data | Random init or pretrained |\n",
                "| Domain-specific | Fine-tune or train from scratch |\n",
                "\n",
                "### Memory Optimization\n",
                "\n",
                "```python\n",
                "# Embeddings can be huge!\n",
                "# 50K vocab Ã— 300d Ã— 4 bytes = 60 MB\n",
                "\n",
                "# Solutions:\n",
                "# 1. Reduce vocab size\n",
                "# 2. Use smaller embedding dimension\n",
                "# 3. Quantize for inference\n",
                "```\n",
                "\n",
                "### Modern Approach\n",
                "\n",
                "> For transformers (BERT, GPT), use the model's built-in embeddings.\n",
                "> No need to load separate pretrained embeddings."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Questions\n",
                "\n",
                "**Q1: What is nn.Embedding? Is it the same as a linear layer?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "nn.Embedding is a lookup table. It's equivalent to `nn.Linear(vocab_size, embed_dim)` with one-hot input, but much more efficient since we don't need to create one-hot vectors.\n",
                "</details>\n",
                "\n",
                "**Q2: Why use padding_idx?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- Ensures padding tokens have zero embedding\n",
                "- Prevents gradients from flowing to padding\n",
                "- Important for correct sequence processing\n",
                "</details>\n",
                "\n",
                "**Q3: When to freeze vs fine-tune embeddings?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- **Freeze**: Small dataset, prevent overfitting\n",
                "- **Fine-tune**: Larger dataset, domain mismatch between pretrained and target\n",
                "- Common: Freeze initially, unfreeze later\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "- **nn.Embedding**: Efficient lookup table for word vectors\n",
                "- **Input/Output**: `[batch, seq]` â†’ `[batch, seq, dim]`\n",
                "- **Pretrained**: Load GloVe/FastText for better performance\n",
                "- **padding_idx**: Keep padding tokens as zeros\n",
                "- **Freeze/Fine-tune**: Depends on data size"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Exercises\n",
                "\n",
                "1. Load actual GloVe embeddings and compute word similarities\n",
                "2. Compare frozen vs fine-tuned on sentiment classification\n",
                "3. Implement EmbeddingBag for multi-hot inputs\n",
                "4. Visualize how embeddings change during training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. References\n",
                "\n",
                "- [PyTorch nn.Embedding Docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
                "- [GloVe Pretrained](https://nlp.stanford.edu/projects/glove/)\n",
                "- [FastText Pretrained](https://fasttext.cc/docs/en/english-vectors.html)\n",
                "\n",
                "---\n",
                "**Next:** [Module 05: RNN Fundamentals](../05_rnn_fundamentals/05_rnn_fundamentals.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}