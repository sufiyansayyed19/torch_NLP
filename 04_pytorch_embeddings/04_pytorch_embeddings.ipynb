{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeCZCxXJ9ZA2"
      },
      "source": [
        "# Module 04: PyTorch Embedding Layers\n",
        "\n",
        "**Mastering nn.Embedding for Deep Learning NLP**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke-AjiRv9ZA3"
      },
      "source": [
        "## 1. Objectives\n",
        "\n",
        "- âœ… Understand nn.Embedding as a lookup table\n",
        "- âœ… Initialize embeddings (random, pretrained)\n",
        "- âœ… Load GloVe/FastText into PyTorch\n",
        "- âœ… Handle padding correctly\n",
        "- âœ… Freeze vs fine-tune embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcFxS_G69ZA4"
      },
      "source": [
        "## 2. Prerequisites\n",
        "\n",
        "- [Module 03: Word Embeddings](../03_word_embeddings/03_word_embeddings.ipynb)\n",
        "- PyTorch basics (tensors, nn.Module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U6R9xJ09ZA6"
      },
      "source": [
        "## 3. Intuition & Motivation\n",
        "\n",
        "### What is nn.Embedding?\n",
        "\n",
        "Simply a **lookup table** that maps indices to vectors:\n",
        "\n",
        "```\n",
        "Index:  0 â†’ [0.2, -0.4, 0.7, ...]\n",
        "        1 â†’ [0.3, -0.3, 0.6, ...]\n",
        "        2 â†’ [0.8, 0.5, -0.2, ...]\n",
        "```\n",
        "\n",
        "### Dimensions\n",
        "\n",
        "```\n",
        "Input:  [batch_size, seq_len]        (indices)\n",
        "Output: [batch_size, seq_len, embed_dim]  (vectors)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYoFIPpB9ZA6",
        "outputId": "73d49a6b-7918-4a04-b3f9-267f2d449c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y84GsQGA9ZA7"
      },
      "source": [
        "## 4. nn.Embedding Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRZYHavg9ZA7",
        "outputId": "33d54938-36cb-408b-af05-f6e5aecc6c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding weight shape: torch.Size([1000, 300])\n",
            "Total parameters: 300,000\n",
            "\n",
            "Input shape: torch.Size([2, 4])\n",
            "Output shape: torch.Size([2, 4, 300])\n"
          ]
        }
      ],
      "source": [
        "# Create embedding layer\n",
        "vocab_size = 1000\n",
        "embedding_dim = 300\n",
        "\n",
        "embedding = nn.Embedding(\n",
        "    num_embeddings=vocab_size,  # Size of vocabulary\n",
        "    embedding_dim=embedding_dim  # Dimension of each vector\n",
        ")\n",
        "\n",
        "print(f\"Embedding weight shape: {embedding.weight.shape}\")\n",
        "print(f\"Total parameters: {embedding.weight.numel():,}\")\n",
        "\n",
        "# Forward pass\n",
        "input_indices = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])  # (batch=2, seq=4)\n",
        "output = embedding(input_indices)\n",
        "print(f\"\\nInput shape: {input_indices.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0XNO32L9ZA8"
      },
      "source": [
        "## 5. Initialization Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEXw3JHH9ZA8",
        "outputId": "7da0cc21-283b-45d7-e538-da287b8daf49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default init - mean: -0.0007, std: 1.0003\n",
            "Xavier init - mean: -0.0001, std: 0.0392\n",
            "Uniform init - mean: -0.0001, std: 0.0578\n"
          ]
        }
      ],
      "source": [
        "# 1. Default (Normal distribution)\n",
        "emb_default = nn.Embedding(1000, 300)\n",
        "print(f\"Default init - mean: {emb_default.weight.mean():.4f}, std: {emb_default.weight.std():.4f}\")\n",
        "\n",
        "# 2. Xavier/Glorot\n",
        "emb_xavier = nn.Embedding(1000, 300)\n",
        "nn.init.xavier_uniform_(emb_xavier.weight)\n",
        "print(f\"Xavier init - mean: {emb_xavier.weight.mean():.4f}, std: {emb_xavier.weight.std():.4f}\")\n",
        "\n",
        "# 3. Uniform in range\n",
        "emb_uniform = nn.Embedding(1000, 300)\n",
        "nn.init.uniform_(emb_uniform.weight, -0.1, 0.1)\n",
        "print(f\"Uniform init - mean: {emb_uniform.weight.mean():.4f}, std: {emb_uniform.weight.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnav9ufD9ZA8"
      },
      "source": [
        "## 6. Loading Pretrained Embeddings (GloVe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw0Ud3xF9ZA9",
        "outputId": "1f72ac0e-ca4a-4a1f-df7c-e9eec5f42b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe loading function ready!\n"
          ]
        }
      ],
      "source": [
        "def load_glove(glove_path: str, vocab: Dict[str, int], embedding_dim: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load GloVe embeddings for vocabulary.\n",
        "\n",
        "    Args:\n",
        "        glove_path: Path to GloVe file (e.g., glove.6B.300d.txt)\n",
        "        vocab: Dictionary mapping word -> index\n",
        "        embedding_dim: Dimension of embeddings\n",
        "\n",
        "    Returns:\n",
        "        Embedding matrix of shape (vocab_size, embedding_dim)\n",
        "    \"\"\"\n",
        "    # Initialize with random vectors\n",
        "    embedding_matrix = np.random.randn(len(vocab), embedding_dim) * 0.01\n",
        "    found = 0\n",
        "\n",
        "    # Read GloVe file\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            if word in vocab:\n",
        "                idx = vocab[word]\n",
        "                embedding_matrix[idx] = np.array(values[1:], dtype=np.float32)\n",
        "                found += 1\n",
        "\n",
        "    print(f\"Found {found}/{len(vocab)} words in GloVe ({100*found/len(vocab):.1f}%)\")\n",
        "    return embedding_matrix\n",
        "\n",
        "# Example usage (would need actual GloVe file)\n",
        "# vocab = {'the': 0, 'cat': 1, 'sat': 2, 'on': 3, 'mat': 4}\n",
        "# embeddings = load_glove('glove.6B.300d.txt', vocab, 300)\n",
        "\n",
        "print(\"GloVe loading function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kVuZotQ9ZA9",
        "outputId": "900317b5-c794-4141-b775-76b8b7014437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created embedding: 1000 x 300, frozen=True\n",
            "Created embedding: 1000 x 300, frozen=False\n"
          ]
        }
      ],
      "source": [
        "def create_embedding_layer(pretrained_weights: np.ndarray, freeze: bool = True) -> nn.Embedding:\n",
        "    \"\"\"\n",
        "    Create embedding layer from pretrained weights.\n",
        "\n",
        "    Args:\n",
        "        pretrained_weights: NumPy array of shape (vocab_size, embed_dim)\n",
        "        freeze: If True, embeddings won't be updated during training\n",
        "    \"\"\"\n",
        "    vocab_size, embedding_dim = pretrained_weights.shape\n",
        "    embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # Load weights\n",
        "    embedding.weight = nn.Parameter(\n",
        "        torch.from_numpy(pretrained_weights).float(),\n",
        "        requires_grad=not freeze\n",
        "    )\n",
        "\n",
        "    print(f\"Created embedding: {vocab_size} x {embedding_dim}, frozen={freeze}\")\n",
        "    return embedding\n",
        "\n",
        "# Demo with random weights\n",
        "demo_weights = np.random.randn(1000, 300).astype(np.float32)\n",
        "frozen_emb = create_embedding_layer(demo_weights, freeze=True)\n",
        "trainable_emb = create_embedding_layer(demo_weights, freeze=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9AjS4Az9ZA9"
      },
      "source": [
        "## 7. Padding and Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luTZc0wA9ZA9",
        "outputId": "1d3475a1-5394-4b1e-f775-a6c527a4d04f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding embedding (should be zeros):\n",
            "  Sum: 0.0\n",
            "  Requires grad: True\n",
            "\n",
            "Output shape: torch.Size([2, 5, 300])\n",
            "Padding positions are zero vectors: True\n"
          ]
        }
      ],
      "source": [
        "# Padding index: embedding for padding tokens should be zeros\n",
        "PAD_IDX = 0\n",
        "\n",
        "embedding = nn.Embedding(\n",
        "    num_embeddings=1000,\n",
        "    embedding_dim=300,\n",
        "    padding_idx=PAD_IDX  # This index will always be zeros\n",
        ")\n",
        "\n",
        "# Check padding embedding\n",
        "print(f\"Padding embedding (should be zeros):\")\n",
        "print(f\"  Sum: {embedding.weight[PAD_IDX].sum().item()}\")\n",
        "print(f\"  Requires grad: {embedding.weight[PAD_IDX].requires_grad}\")\n",
        "\n",
        "# Example with padded sequence\n",
        "padded_input = torch.tensor([\n",
        "    [1, 2, 3, 0, 0],  # Sequence of length 3, padded to 5\n",
        "    [4, 5, 0, 0, 0]   # Sequence of length 2, padded to 5\n",
        "])\n",
        "\n",
        "output = embedding(padded_input)\n",
        "print(f\"\\nOutput shape: {output.shape}\")\n",
        "print(f\"Padding positions are zero vectors: {output[0, 3].sum().item() == 0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwGFzbcQ9ZA9"
      },
      "source": [
        "## 8. Complete Embedding Module for NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECoEt4Fi9ZA9",
        "outputId": "63e979e6-be3c-4e65-c5ba-059e4c5fad78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: torch.Size([32, 100]) â†’ Output: torch.Size([32, 100, 300])\n"
          ]
        }
      ],
      "source": [
        "class TextEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Text embedding module with optional pretrained weights.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        padding_idx: int = 0,\n",
        "        pretrained_weights: np.ndarray = None,\n",
        "        freeze: bool = False,\n",
        "        dropout: float = 0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size, embedding_dim, padding_idx=padding_idx\n",
        "        )\n",
        "\n",
        "        # Load pretrained if provided\n",
        "        if pretrained_weights is not None:\n",
        "            self.embedding.weight = nn.Parameter(\n",
        "                torch.from_numpy(pretrained_weights).float(),\n",
        "                requires_grad=not freeze\n",
        "            )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, seq_len) token indices\n",
        "        Returns:\n",
        "            (batch_size, seq_len, embedding_dim) embeddings\n",
        "        \"\"\"\n",
        "        return self.dropout(self.embedding(x))\n",
        "\n",
        "# Example\n",
        "text_emb = TextEmbedding(vocab_size=5000, embedding_dim=300, dropout=0.1)\n",
        "x = torch.randint(0, 5000, (32, 100))  # batch=32, seq_len=100\n",
        "output = text_emb(x)\n",
        "print(f\"Input: {x.shape} â†’ Output: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGNSR4lQ9ZA-"
      },
      "source": [
        "## 9. ðŸ”¥ Real-World Usage\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "| Scenario | Recommendation |\n",
        "|----------|----------------|\n",
        "| Small data | Use pretrained, **freeze** |\n",
        "| Medium data | Use pretrained, **fine-tune** |\n",
        "| Large data | Random init or pretrained |\n",
        "| Domain-specific | Fine-tune or train from scratch |\n",
        "\n",
        "### Memory Optimization\n",
        "\n",
        "```python\n",
        "# Embeddings can be huge!\n",
        "# 50K vocab Ã— 300d Ã— 4 bytes = 60 MB\n",
        "\n",
        "# Solutions:\n",
        "# 1. Reduce vocab size\n",
        "# 2. Use smaller embedding dimension\n",
        "# 3. Quantize for inference\n",
        "```\n",
        "\n",
        "### Modern Approach\n",
        "\n",
        "> For transformers (BERT, GPT), use the model's built-in embeddings.\n",
        "> No need to load separate pretrained embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf8l-IDU9ZA-"
      },
      "source": [
        "## 10. Interview Questions\n",
        "\n",
        "**Q1: What is nn.Embedding? Is it the same as a linear layer?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "nn.Embedding is a lookup table. It's equivalent to `nn.Linear(vocab_size, embed_dim)` with one-hot input, but much more efficient since we don't need to create one-hot vectors.\n",
        "</details>\n",
        "\n",
        "**Q2: Why use padding_idx?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "- Ensures padding tokens have zero embedding\n",
        "- Prevents gradients from flowing to padding\n",
        "- Important for correct sequence processing\n",
        "</details>\n",
        "\n",
        "**Q3: When to freeze vs fine-tune embeddings?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "- **Freeze**: Small dataset, prevent overfitting\n",
        "- **Fine-tune**: Larger dataset, domain mismatch between pretrained and target\n",
        "- Common: Freeze initially, unfreeze later\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjOuZAlA9ZA-"
      },
      "source": [
        "## 11. Summary\n",
        "\n",
        "- **nn.Embedding**: Efficient lookup table for word vectors\n",
        "- **Input/Output**: `[batch, seq]` â†’ `[batch, seq, dim]`\n",
        "- **Pretrained**: Load GloVe/FastText for better performance\n",
        "- **padding_idx**: Keep padding tokens as zeros\n",
        "- **Freeze/Fine-tune**: Depends on data size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ZlQOzE9ZA-"
      },
      "source": [
        "## 12. Exercises\n",
        "\n",
        "1. Load actual GloVe embeddings and compute word similarities\n",
        "2. Compare frozen vs fine-tuned on sentiment classification\n",
        "3. Implement EmbeddingBag for multi-hot inputs\n",
        "4. Visualize how embeddings change during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L95YGKhZ9ZA-"
      },
      "source": [
        "## 13. References\n",
        "\n",
        "- [PyTorch nn.Embedding Docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "- [GloVe Pretrained](https://nlp.stanford.edu/projects/glove/)\n",
        "- [FastText Pretrained](https://fasttext.cc/docs/en/english-vectors.html)\n",
        "\n",
        "---\n",
        "**Next:** [Module 05: RNN Fundamentals](../05_rnn_fundamentals/05_rnn_fundamentals.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}