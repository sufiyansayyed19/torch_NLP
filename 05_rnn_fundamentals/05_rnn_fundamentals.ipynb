{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 05: RNN Fundamentals\n",
                "\n",
                "**Understanding Recurrent Neural Networks and Their Limitations**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- ‚úÖ Understand why sequences need special treatment\n",
                "- ‚úÖ Master vanilla RNN architecture and equations\n",
                "- ‚úÖ Implement RNN from scratch\n",
                "- ‚úÖ Understand BPTT and vanishing gradients\n",
                "- ‚úÖ Use PyTorch nn.RNN"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 04: PyTorch Embeddings](../04_pytorch_embeddings/04_pytorch_embeddings.ipynb)\n",
                "- Basic backpropagation knowledge"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Intuition & Motivation\n",
                "\n",
                "### Why Sequences Need Special Treatment\n",
                "\n",
                "**Problem with feedforward networks:**\n",
                "- Fixed input size\n",
                "- No notion of order\n",
                "- Can't handle \"The cat sat\" vs \"sat cat The\"\n",
                "\n",
                "**What RNNs provide:**\n",
                "- Variable length inputs\n",
                "- Order matters\n",
                "- \"Memory\" through hidden state\n",
                "\n",
                "### The Core Idea\n",
                "\n",
                "```\n",
                "Process sequence one step at a time, maintaining a \"memory\" (hidden state)\n",
                "\n",
                "x‚ÇÅ ‚Üí [RNN] ‚Üí h‚ÇÅ\n",
                "              ‚Üì\n",
                "x‚ÇÇ ‚Üí [RNN] ‚Üí h‚ÇÇ\n",
                "              ‚Üì\n",
                "x‚ÇÉ ‚Üí [RNN] ‚Üí h‚ÇÉ ‚Üí output\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(\"Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Mathematical Foundation\n",
                "\n",
                "### Vanilla RNN Equations\n",
                "\n",
                "At each time step $t$:\n",
                "\n",
                "$$h_t = \\tanh(W_{hh} \\cdot h_{t-1} + W_{xh} \\cdot x_t + b_h)$$\n",
                "\n",
                "$$y_t = W_{hy} \\cdot h_t + b_y$$\n",
                "\n",
                "Where:\n",
                "- $x_t$ : Input at time $t$ ‚Äî shape: `(input_size,)`\n",
                "- $h_t$ : Hidden state at time $t$ ‚Äî shape: `(hidden_size,)`\n",
                "- $y_t$ : Output at time $t$ ‚Äî shape: `(output_size,)`\n",
                "- $W_{hh}$ : Hidden-to-hidden weights ‚Äî shape: `(hidden_size, hidden_size)`\n",
                "- $W_{xh}$ : Input-to-hidden weights ‚Äî shape: `(hidden_size, input_size)`\n",
                "- $W_{hy}$ : Hidden-to-output weights ‚Äî shape: `(output_size, hidden_size)`\n",
                "\n",
                "### Key Properties\n",
                "\n",
                "- **Parameter sharing**: Same weights used at every time step\n",
                "- **Hidden state**: Carries information across time steps\n",
                "- **tanh**: Squashes values to [-1, 1]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. RNN from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VanillaRNN:\n",
                "    \"\"\"Vanilla RNN from scratch (NumPy).\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
                "        self.hidden_size = hidden_size\n",
                "        \n",
                "        # Initialize weights (Xavier)\n",
                "        scale = np.sqrt(2.0 / (input_size + hidden_size))\n",
                "        self.W_xh = np.random.randn(hidden_size, input_size) * scale\n",
                "        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale\n",
                "        self.W_hy = np.random.randn(output_size, hidden_size) * scale\n",
                "        self.b_h = np.zeros((hidden_size, 1))\n",
                "        self.b_y = np.zeros((output_size, 1))\n",
                "    \n",
                "    def forward(self, inputs, h_prev=None):\n",
                "        \"\"\"\n",
                "        Forward pass through sequence.\n",
                "        \n",
                "        Args:\n",
                "            inputs: List of input vectors, each shape (input_size, 1)\n",
                "            h_prev: Initial hidden state, shape (hidden_size, 1)\n",
                "        \n",
                "        Returns:\n",
                "            outputs: List of output vectors\n",
                "            hidden_states: List of hidden states\n",
                "        \"\"\"\n",
                "        if h_prev is None:\n",
                "            h_prev = np.zeros((self.hidden_size, 1))\n",
                "        \n",
                "        outputs = []\n",
                "        hidden_states = [h_prev]\n",
                "        \n",
                "        for x in inputs:\n",
                "            # RNN step: h_t = tanh(W_xh @ x + W_hh @ h_{t-1} + b_h)\n",
                "            h = np.tanh(\n",
                "                self.W_xh @ x + \n",
                "                self.W_hh @ h_prev + \n",
                "                self.b_h\n",
                "            )\n",
                "            \n",
                "            # Output: y_t = W_hy @ h_t + b_y\n",
                "            y = self.W_hy @ h + self.b_y\n",
                "            \n",
                "            outputs.append(y)\n",
                "            hidden_states.append(h)\n",
                "            h_prev = h\n",
                "        \n",
                "        return outputs, hidden_states\n",
                "\n",
                "# Test\n",
                "rnn = VanillaRNN(input_size=10, hidden_size=20, output_size=5)\n",
                "seq_len = 15\n",
                "inputs = [np.random.randn(10, 1) for _ in range(seq_len)]\n",
                "\n",
                "outputs, hiddens = rnn.forward(inputs)\n",
                "print(f\"Sequence length: {seq_len}\")\n",
                "print(f\"Output shape per step: {outputs[0].shape}\")\n",
                "print(f\"Hidden state shape: {hiddens[0].shape}\")\n",
                "print(f\"Number of hidden states: {len(hiddens)} (includes initial)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. PyTorch Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PyTorch RNN\n",
                "rnn_pt = nn.RNN(\n",
                "    input_size=10,\n",
                "    hidden_size=20,\n",
                "    num_layers=1,\n",
                "    batch_first=True,  # Input: (batch, seq, features)\n",
                "    bidirectional=False\n",
                ")\n",
                "\n",
                "# Input: (batch_size, seq_len, input_size)\n",
                "x = torch.randn(32, 15, 10)  # batch=32, seq=15, input=10\n",
                "\n",
                "# Initial hidden: (num_layers, batch_size, hidden_size)\n",
                "h0 = torch.zeros(1, 32, 20)\n",
                "\n",
                "# Forward\n",
                "output, h_n = rnn_pt(x, h0)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}  (all hidden states)\")\n",
                "print(f\"h_n shape: {h_n.shape}  (final hidden state)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom RNN Cell in PyTorch\n",
                "class RNNCell(nn.Module):\n",
                "    \"\"\"Single RNN cell.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size: int, hidden_size: int):\n",
                "        super().__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "        self.W_xh = nn.Linear(input_size, hidden_size, bias=False)\n",
                "        self.W_hh = nn.Linear(hidden_size, hidden_size)\n",
                "    \n",
                "    def forward(self, x, h_prev):\n",
                "        \"\"\"Single step: x (batch, input), h_prev (batch, hidden)\"\"\"\n",
                "        return torch.tanh(self.W_xh(x) + self.W_hh(h_prev))\n",
                "\n",
                "class MyRNN(nn.Module):\n",
                "    \"\"\"Full RNN using our cell.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size: int, hidden_size: int):\n",
                "        super().__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "        self.cell = RNNCell(input_size, hidden_size)\n",
                "    \n",
                "    def forward(self, x, h0=None):\n",
                "        \"\"\"x: (batch, seq, input)\"\"\"\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "        \n",
                "        if h0 is None:\n",
                "            h0 = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
                "        \n",
                "        outputs = []\n",
                "        h = h0\n",
                "        \n",
                "        for t in range(seq_len):\n",
                "            h = self.cell(x[:, t, :], h)\n",
                "            outputs.append(h)\n",
                "        \n",
                "        return torch.stack(outputs, dim=1), h\n",
                "\n",
                "# Test\n",
                "my_rnn = MyRNN(10, 20)\n",
                "out, h_final = my_rnn(x)\n",
                "print(f\"My RNN output: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Backpropagation Through Time (BPTT)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### How Gradients Flow\n",
                "\n",
                "```\n",
                "Loss at t=3\n",
                "    ‚Üì\n",
                "h‚ÇÉ ‚Üê W_hh ‚Üê h‚ÇÇ ‚Üê W_hh ‚Üê h‚ÇÅ ‚Üê W_hh ‚Üê h‚ÇÄ\n",
                "```\n",
                "\n",
                "Gradient for $W_{hh}$ must be computed across ALL time steps.\n",
                "\n",
                "### The Problem: Vanishing Gradients\n",
                "\n",
                "For long sequences:\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial h_0} = \\frac{\\partial L}{\\partial h_T} \\cdot \\prod_{t=1}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
                "\n",
                "Each term $\\frac{\\partial h_t}{\\partial h_{t-1}} = W_{hh}^T \\cdot \\text{diag}(1 - h_t^2)$\n",
                "\n",
                "If eigenvalues of $W_{hh} < 1$: gradients **vanish** exponentially\n",
                "If eigenvalues of $W_{hh} > 1$: gradients **explode** exponentially"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize gradient flow\n",
                "def compute_gradient_norms(model, seq_lengths):\n",
                "    \"\"\"Compute gradient norms for different sequence lengths.\"\"\"\n",
                "    norms = []\n",
                "    \n",
                "    for seq_len in seq_lengths:\n",
                "        model.zero_grad()\n",
                "        x = torch.randn(1, seq_len, 10, requires_grad=True)\n",
                "        h0 = torch.zeros(1, 1, 20, requires_grad=True)\n",
                "        \n",
                "        output, _ = model(x, h0)\n",
                "        loss = output[:, -1, :].sum()  # Loss on last output\n",
                "        loss.backward()\n",
                "        \n",
                "        # Gradient norm w.r.t first input\n",
                "        grad_norm = x.grad[:, 0, :].norm().item()\n",
                "        norms.append(grad_norm)\n",
                "    \n",
                "    return norms\n",
                "\n",
                "seq_lengths = [5, 10, 20, 50, 100]\n",
                "rnn_test = nn.RNN(10, 20, batch_first=True)\n",
                "norms = compute_gradient_norms(rnn_test, seq_lengths)\n",
                "\n",
                "plt.figure(figsize=(8, 4))\n",
                "plt.plot(seq_lengths, norms, 'o-')\n",
                "plt.xlabel('Sequence Length')\n",
                "plt.ylabel('Gradient Norm')\n",
                "plt.title('Vanishing Gradients in Vanilla RNN')\n",
                "plt.yscale('log')\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "print(\"Notice: Gradients decrease exponentially with sequence length!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. üî• Real-World Usage\n",
                "\n",
                "### In 2024: Vanilla RNNs are NEVER used in practice!\n",
                "\n",
                "**Why learn them?**\n",
                "- Foundation for LSTM/GRU\n",
                "- Understand sequence modeling concepts\n",
                "- Interview questions\n",
                "\n",
                "**What to use instead:**\n",
                "| Task | Model |\n",
                "|------|-------|\n",
                "| Sequence modeling | LSTM, GRU |\n",
                "| Classification | Transformers (BERT) |\n",
                "| Generation | Transformers (GPT) |\n",
                "| Streaming data | LSTM/GRU |\n",
                "\n",
                "### Historical Importance\n",
                "- 1986: BPTT introduced\n",
                "- 1990s: Vanishing gradient problem identified\n",
                "- 1997: LSTM invented to solve it"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Common Mistakes\n",
                "\n",
                "| Mistake | Fix |\n",
                "|---------|-----|\n",
                "| Forgetting to initialize h‚ÇÄ | Use zeros or learnable |\n",
                "| Wrong batch_first setting | Check tensor shapes |\n",
                "| Using vanilla RNN for long sequences | Use LSTM/GRU |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Questions\n",
                "\n",
                "**Q1: Walk me through how an RNN processes a sequence**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "1. Initialize hidden state h‚ÇÄ (usually zeros)\n",
                "2. For each input x‚Çú: compute h‚Çú = tanh(W_xh¬∑x‚Çú + W_hh¬∑h‚Çú‚Çã‚ÇÅ + b)\n",
                "3. Same weights used at every step (parameter sharing)\n",
                "4. Final hidden state contains summary of sequence\n",
                "</details>\n",
                "\n",
                "**Q2: Why do RNNs struggle with long sequences?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Vanishing gradients: During BPTT, gradients are multiplied by W_hh repeatedly. If eigenvalues < 1, gradients shrink exponentially. Information from early inputs is lost.\n",
                "</details>\n",
                "\n",
                "**Q3: What is BPTT?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Backpropagation Through Time: Unroll the RNN across time steps and apply standard backprop. Gradients flow backwards through all time steps.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "- **RNN**: Process sequences with shared weights and hidden state\n",
                "- **Equation**: $h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b)$\n",
                "- **BPTT**: Backprop unrolled through time\n",
                "- **Vanishing gradients**: Can't learn long dependencies\n",
                "- **Practice**: Use LSTM/GRU instead"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Exercises\n",
                "\n",
                "1. Implement BPTT from scratch for the VanillaRNN class\n",
                "2. Visualize hidden states over time for different sequences\n",
                "3. Compare gradient norms: vanilla RNN vs LSTM\n",
                "4. Train RNN on simple sequence prediction task"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. References\n",
                "\n",
                "- [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
                "- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
                "- [PyTorch RNN Docs](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
                "\n",
                "---\n",
                "**Next:** [Module 06: LSTM](../06_lstm/06_lstm.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}