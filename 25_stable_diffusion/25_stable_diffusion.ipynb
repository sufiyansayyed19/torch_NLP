{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 25: Stable Diffusion\n",
                "\n",
                "**Text-to-Image Generation with Latent Diffusion**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- ‚úÖ Understand diffusion model theory\n",
                "- ‚úÖ Learn Stable Diffusion architecture\n",
                "- ‚úÖ Master the Diffusers library\n",
                "- ‚úÖ Implement text-to-image generation\n",
                "- ‚úÖ Explore ControlNet and fine-tuning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 24: Multimodal Learning](../24_multimodal/24_multimodal.ipynb)\n",
                "- Basic understanding of probability and neural networks"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Diffusion Models - Theory\n",
                "\n",
                "### Core Intuition\n",
                "\n",
                "Diffusion models learn to **reverse a gradual noising process**:\n",
                "\n",
                "```\n",
                "Forward Process (Fixed):\n",
                "Clean Image ‚îÄ‚îÄ‚Üí Add Noise ‚îÄ‚îÄ‚Üí Add Noise ‚îÄ‚îÄ‚Üí ... ‚îÄ‚îÄ‚Üí Pure Noise\n",
                "    x‚ÇÄ      ‚Üí      x‚ÇÅ     ‚Üí      x‚ÇÇ     ‚Üí ... ‚Üí      x‚Çú\n",
                "\n",
                "Reverse Process (Learned):\n",
                "Pure Noise ‚îÄ‚îÄ‚Üí Denoise ‚îÄ‚îÄ‚Üí Denoise ‚îÄ‚îÄ‚Üí ... ‚îÄ‚îÄ‚Üí Clean Image\n",
                "    x‚Çú     ‚Üí    x‚Çú‚Çã‚ÇÅ   ‚Üí    x‚Çú‚Çã‚ÇÇ   ‚Üí ... ‚Üí      x‚ÇÄ\n",
                "```\n",
                "\n",
                "### Forward Process (Adding Noise)\n",
                "\n",
                "At each timestep $t$, we add Gaussian noise:\n",
                "\n",
                "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$$\n",
                "\n",
                "We can jump directly to any timestep:\n",
                "\n",
                "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t) I)$$\n",
                "\n",
                "Where $\\bar{\\alpha}_t = \\prod_{s=1}^t (1 - \\beta_s)$\n",
                "\n",
                "### Reverse Process (Denoising)\n",
                "\n",
                "The model learns to predict the noise $\\epsilon$ at each step:\n",
                "\n",
                "$$\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2 \\right]$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install: pip install diffusers accelerate transformers torch\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleDiffusion:\n",
                "    \"\"\"Simplified diffusion process for understanding.\n",
                "    \n",
                "    This implements the forward and reverse diffusion math.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
                "        self.num_timesteps = num_timesteps\n",
                "        \n",
                "        # Linear noise schedule\n",
                "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
                "        self.alphas = 1.0 - self.betas\n",
                "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
                "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
                "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
                "    \n",
                "    def add_noise(self, x_0, t, noise=None):\n",
                "        \"\"\"Forward process: add noise to clean data.\n",
                "        \n",
                "        x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * noise\n",
                "        \"\"\"\n",
                "        if noise is None:\n",
                "            noise = torch.randn_like(x_0)\n",
                "        \n",
                "        sqrt_alpha = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
                "        sqrt_one_minus_alpha = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
                "        \n",
                "        return sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise\n",
                "    \n",
                "    def remove_noise(self, x_t, t, predicted_noise):\n",
                "        \"\"\"Reverse process: estimate x_{t-1} from x_t.\"\"\"\n",
                "        alpha = self.alphas[t].view(-1, 1, 1, 1)\n",
                "        alpha_cumprod = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
                "        beta = self.betas[t].view(-1, 1, 1, 1)\n",
                "        \n",
                "        # Predict x_0 from noise\n",
                "        sqrt_one_minus_alpha = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
                "        x_0_pred = (x_t - sqrt_one_minus_alpha * predicted_noise) / self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
                "        \n",
                "        # Compute mean of reverse distribution\n",
                "        mean = (1 / torch.sqrt(alpha)) * (x_t - (beta / sqrt_one_minus_alpha) * predicted_noise)\n",
                "        \n",
                "        # Add noise (except for t=0)\n",
                "        if t[0] > 0:\n",
                "            noise = torch.randn_like(x_t)\n",
                "            std = torch.sqrt(beta)\n",
                "            return mean + std * noise\n",
                "        return mean\n",
                "\n",
                "# Visualize noising process\n",
                "diffusion = SimpleDiffusion()\n",
                "x_0 = torch.randn(1, 3, 64, 64)  # Fake \"clean\" image\n",
                "\n",
                "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
                "timesteps = [0, 250, 500, 750, 999]\n",
                "\n",
                "for ax, t in zip(axes, timesteps):\n",
                "    x_t = diffusion.add_noise(x_0, torch.tensor([t]))\n",
                "    # Normalize for visualization\n",
                "    img = (x_t[0].permute(1, 2, 0).numpy() + 1) / 2\n",
                "    img = np.clip(img, 0, 1)\n",
                "    ax.imshow(img)\n",
                "    ax.set_title(f't={t}')\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.suptitle('Forward Diffusion Process')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Stable Diffusion Architecture\n",
                "\n",
                "### Key Innovation: Latent Diffusion\n",
                "\n",
                "Instead of diffusing in **pixel space** (512√ó512√ó3 = 786K dimensions), work in **latent space** (64√ó64√ó4 = 16K dimensions)!\n",
                "\n",
                "```\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ                    Stable Diffusion Pipeline                  ‚îÇ\n",
                "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
                "‚îÇ                                                               ‚îÇ\n",
                "‚îÇ  Text Prompt ‚îÄ‚îÄ‚Üí [CLIP Text Encoder] ‚îÄ‚îÄ‚Üí Text Embeddings     ‚îÇ\n",
                "‚îÇ                                               ‚Üì               ‚îÇ\n",
                "‚îÇ  Random Noise ‚îÄ‚îÄ‚Üí [U-Net] ‚Üê‚îÄ‚îÄ Cross-Attention                ‚îÇ\n",
                "‚îÇ                      ‚Üì                                        ‚îÇ\n",
                "‚îÇ              [Scheduler: DDPM/DDIM/...]                       ‚îÇ\n",
                "‚îÇ                      ‚Üì                                        ‚îÇ\n",
                "‚îÇ              Denoised Latents                                 ‚îÇ\n",
                "‚îÇ                      ‚Üì                                        ‚îÇ\n",
                "‚îÇ              [VAE Decoder] ‚îÄ‚îÄ‚Üí Generated Image                ‚îÇ\n",
                "‚îÇ                                                               ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```\n",
                "\n",
                "### Components\n",
                "\n",
                "| Component | Purpose | Size |\n",
                "|-----------|---------|------|\n",
                "| VAE | Compress/decompress images | ~80M params |\n",
                "| U-Net | Predict noise at each step | ~860M params |\n",
                "| CLIP Text Encoder | Encode text prompts | ~123M params |\n",
                "| Scheduler | Control denoising steps | N/A |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# U-Net building blocks\n",
                "\n",
                "class ResidualBlock(nn.Module):\n",
                "    \"\"\"Residual block with time embedding.\"\"\"\n",
                "    \n",
                "    def __init__(self, in_channels, out_channels, time_emb_dim):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
                "        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n",
                "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
                "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
                "        \n",
                "        if in_channels != out_channels:\n",
                "            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)\n",
                "        else:\n",
                "            self.shortcut = nn.Identity()\n",
                "    \n",
                "    def forward(self, x, t_emb):\n",
                "        h = self.norm1(F.silu(self.conv1(x)))\n",
                "        h = h + self.time_mlp(t_emb)[:, :, None, None]  # Add time embedding\n",
                "        h = self.norm2(F.silu(self.conv2(h)))\n",
                "        return h + self.shortcut(x)\n",
                "\n",
                "\n",
                "class CrossAttention(nn.Module):\n",
                "    \"\"\"Cross-attention for text conditioning.\"\"\"\n",
                "    \n",
                "    def __init__(self, dim, context_dim, n_heads=8):\n",
                "        super().__init__()\n",
                "        self.n_heads = n_heads\n",
                "        self.head_dim = dim // n_heads\n",
                "        \n",
                "        self.to_q = nn.Linear(dim, dim)\n",
                "        self.to_k = nn.Linear(context_dim, dim)\n",
                "        self.to_v = nn.Linear(context_dim, dim)\n",
                "        self.to_out = nn.Linear(dim, dim)\n",
                "    \n",
                "    def forward(self, x, context):\n",
                "        batch, seq_len, dim = x.shape\n",
                "        \n",
                "        q = self.to_q(x)\n",
                "        k = self.to_k(context)\n",
                "        v = self.to_v(context)\n",
                "        \n",
                "        # Reshape for multi-head attention\n",
                "        q = q.view(batch, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
                "        k = k.view(batch, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
                "        v = v.view(batch, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
                "        \n",
                "        # Attention\n",
                "        attn = torch.softmax(q @ k.transpose(-2, -1) / (self.head_dim ** 0.5), dim=-1)\n",
                "        out = attn @ v\n",
                "        \n",
                "        out = out.transpose(1, 2).reshape(batch, seq_len, dim)\n",
                "        return self.to_out(out)\n",
                "\n",
                "print(\"U-Net blocks defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Using Diffusers Library"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
                "import torch\n",
                "\n",
                "# Load Stable Diffusion (use smaller versions for limited VRAM)\n",
                "model_id = \"stabilityai/stable-diffusion-2-1-base\"  # ~5GB\n",
                "# model_id = \"CompVis/stable-diffusion-v1-4\"  # Older but smaller\n",
                "\n",
                "# For low VRAM:\n",
                "pipe = StableDiffusionPipeline.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.float16,  # Use FP16 to save memory\n",
                "    safety_checker=None  # Disable for speed (use responsibly!)\n",
                ")\n",
                "\n",
                "# Use faster scheduler\n",
                "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
                "\n",
                "# Move to GPU if available\n",
                "pipe = pipe.to(device)\n",
                "\n",
                "# Enable memory optimizations\n",
                "pipe.enable_attention_slicing()  # Reduces memory usage\n",
                "# pipe.enable_xformers_memory_efficient_attention()  # Even better (needs xformers)\n",
                "\n",
                "print(\"Stable Diffusion pipeline loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic text-to-image generation\n",
                "\n",
                "prompt = \"A majestic lion in a cosmic nebula, digital art, highly detailed\"\n",
                "negative_prompt = \"blurry, low quality, distorted\"\n",
                "\n",
                "# Generate image\n",
                "with torch.inference_mode():\n",
                "    image = pipe(\n",
                "        prompt=prompt,\n",
                "        negative_prompt=negative_prompt,\n",
                "        num_inference_steps=25,  # More steps = better quality\n",
                "        guidance_scale=7.5,      # How closely to follow prompt\n",
                "        width=512,\n",
                "        height=512\n",
                "    ).images[0]\n",
                "\n",
                "# Display\n",
                "plt.figure(figsize=(8, 8))\n",
                "plt.imshow(image)\n",
                "plt.title(prompt[:50] + \"...\")\n",
                "plt.axis('off')\n",
                "plt.show()\n",
                "\n",
                "# Save\n",
                "image.save(\"generated_image.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Guidance Scale & Classifier-Free Guidance\n",
                "\n",
                "### Theory\n",
                "\n",
                "Classifier-Free Guidance (CFG) balances quality vs prompt adherence:\n",
                "\n",
                "$$\\tilde{\\epsilon} = \\epsilon_\\theta(x_t, \\emptyset) + s \\cdot (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\emptyset))$$\n",
                "\n",
                "Where:\n",
                "- $\\epsilon_\\theta(x_t, c)$ = noise prediction with text condition\n",
                "- $\\epsilon_\\theta(x_t, \\emptyset)$ = unconditional prediction (no text)\n",
                "- $s$ = guidance scale (typically 7-15)\n",
                "\n",
                "### Effect of Guidance Scale\n",
                "\n",
                "| Scale | Effect |\n",
                "|-------|--------|\n",
                "| 1 | No guidance, ignores prompt |\n",
                "| 5-7 | Balanced |\n",
                "| 10-15 | Strong prompt adherence |\n",
                "| 20+ | Over-saturated, artifacts |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare different guidance scales\n",
                "\n",
                "prompt = \"A serene Japanese garden with cherry blossoms\"\n",
                "scales = [1.0, 5.0, 7.5, 12.0]\n",
                "\n",
                "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
                "\n",
                "for ax, scale in zip(axes, scales):\n",
                "    with torch.inference_mode():\n",
                "        image = pipe(\n",
                "            prompt=prompt,\n",
                "            num_inference_steps=20,\n",
                "            guidance_scale=scale,\n",
                "            generator=torch.Generator(device).manual_seed(42)  # Reproducible\n",
                "        ).images[0]\n",
                "    \n",
                "    ax.imshow(image)\n",
                "    ax.set_title(f\"Scale: {scale}\")\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.suptitle(f\"Guidance Scale Comparison: '{prompt[:40]}...'\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Schedulers (Samplers)\n",
                "\n",
                "Different schedulers offer speed/quality tradeoffs:\n",
                "\n",
                "| Scheduler | Steps | Quality | Speed |\n",
                "|-----------|-------|---------|-------|\n",
                "| DDPM | 1000 | Best | Slow |\n",
                "| DDIM | 50-100 | Great | Fast |\n",
                "| DPM++ 2M | 20-30 | Great | Fast |\n",
                "| Euler Ancestral | 20-30 | Good | Fast |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from diffusers import (\n",
                "    DDPMScheduler,\n",
                "    DDIMScheduler,\n",
                "    DPMSolverMultistepScheduler,\n",
                "    EulerAncestralDiscreteScheduler\n",
                ")\n",
                "\n",
                "schedulers = {\n",
                "    \"DDIM\": DDIMScheduler,\n",
                "    \"DPM++ 2M\": DPMSolverMultistepScheduler,\n",
                "    \"Euler A\": EulerAncestralDiscreteScheduler\n",
                "}\n",
                "\n",
                "prompt = \"A futuristic city at sunset, cyberpunk style\"\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
                "\n",
                "for ax, (name, scheduler_class) in zip(axes, schedulers.items()):\n",
                "    pipe.scheduler = scheduler_class.from_config(pipe.scheduler.config)\n",
                "    \n",
                "    with torch.inference_mode():\n",
                "        image = pipe(\n",
                "            prompt=prompt,\n",
                "            num_inference_steps=25,\n",
                "            generator=torch.Generator(device).manual_seed(42)\n",
                "        ).images[0]\n",
                "    \n",
                "    ax.imshow(image)\n",
                "    ax.set_title(name)\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.suptitle(\"Scheduler Comparison (25 steps)\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Image-to-Image Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from diffusers import StableDiffusionImg2ImgPipeline\n",
                "from PIL import Image\n",
                "import requests\n",
                "from io import BytesIO\n",
                "\n",
                "# Load img2img pipeline\n",
                "img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.float16\n",
                ").to(device)\n",
                "\n",
                "# Load an input image\n",
                "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Camponotus_flavomarginatus_ant.jpg/320px-Camponotus_flavomarginatus_ant.jpg\"\n",
                "response = requests.get(url)\n",
                "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
                "init_image = init_image.resize((512, 512))\n",
                "\n",
                "# Transform with a prompt\n",
                "prompt = \"A robotic ant made of chrome and neon, sci-fi style\"\n",
                "\n",
                "with torch.inference_mode():\n",
                "    output = img2img_pipe(\n",
                "        prompt=prompt,\n",
                "        image=init_image,\n",
                "        strength=0.75,  # 0=no change, 1=complete transformation\n",
                "        num_inference_steps=30\n",
                "    ).images[0]\n",
                "\n",
                "# Compare\n",
                "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
                "axes[0].imshow(init_image)\n",
                "axes[0].set_title(\"Original\")\n",
                "axes[1].imshow(output)\n",
                "axes[1].set_title(\"Transformed\")\n",
                "for ax in axes:\n",
                "    ax.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. ControlNet for Guided Generation\n",
                "\n",
                "ControlNet adds **spatial conditioning** to guide generation:\n",
                "\n",
                "```\n",
                "Control Input (edge/pose/depth)  ‚îÄ‚îÄ‚Üí ControlNet ‚îÄ‚îÄ‚Üí \n",
                "                                        ‚Üì\n",
                "Text Prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí U-Net ‚îÄ‚îÄ‚Üí Image\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
                "from diffusers.utils import load_image\n",
                "import cv2\n",
                "import numpy as np\n",
                "\n",
                "# Load ControlNet for Canny edges\n",
                "controlnet = ControlNetModel.from_pretrained(\n",
                "    \"lllyasviel/sd-controlnet-canny\",\n",
                "    torch_dtype=torch.float16\n",
                ")\n",
                "\n",
                "controlnet_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
                "    \"runwayml/stable-diffusion-v1-5\",\n",
                "    controlnet=controlnet,\n",
                "    torch_dtype=torch.float16\n",
                ").to(device)\n",
                "\n",
                "def get_canny_edges(image, low=100, high=200):\n",
                "    \"\"\"Extract Canny edges from image.\"\"\"\n",
                "    image = np.array(image)\n",
                "    edges = cv2.Canny(image, low, high)\n",
                "    edges = np.stack([edges] * 3, axis=-1)\n",
                "    return Image.fromarray(edges)\n",
                "\n",
                "# Create edges from input\n",
                "canny_image = get_canny_edges(init_image)\n",
                "\n",
                "# Generate with edge control\n",
                "prompt = \"A beautiful butterfly, vibrant colors, macro photography\"\n",
                "\n",
                "with torch.inference_mode():\n",
                "    controlled_output = controlnet_pipe(\n",
                "        prompt=prompt,\n",
                "        image=canny_image,\n",
                "        num_inference_steps=25\n",
                "    ).images[0]\n",
                "\n",
                "# Show results\n",
                "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
                "axes[0].imshow(init_image)\n",
                "axes[0].set_title(\"Original\")\n",
                "axes[1].imshow(canny_image)\n",
                "axes[1].set_title(\"Canny Edges\")\n",
                "axes[2].imshow(controlled_output)\n",
                "axes[2].set_title(\"ControlNet Output\")\n",
                "for ax in axes:\n",
                "    ax.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Fine-Tuning with LoRA\n",
                "\n",
                "Train custom styles with minimal compute using LoRA:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a LoRA-fine-tuned model\n",
                "from diffusers import StableDiffusionPipeline\n",
                "\n",
                "# Example: Loading a LoRA for a specific style\n",
                "# pipe.load_lora_weights(\"path/to/lora/weights\")\n",
                "\n",
                "# For training LoRA, use the diffusers train_dreambooth_lora.py script:\n",
                "# accelerate launch train_dreambooth_lora.py \\\n",
                "#   --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1\" \\\n",
                "#   --instance_data_dir=\"./my_images\" \\\n",
                "#   --instance_prompt=\"photo of sks dog\" \\\n",
                "#   --output_dir=\"./lora_weights\" \\\n",
                "#   --train_batch_size=1 \\\n",
                "#   --max_train_steps=500\n",
                "\n",
                "print(\"LoRA fine-tuning example (see diffusers documentation for full training)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Prompt Engineering for Images\n",
                "\n",
                "### Effective Prompts\n",
                "\n",
                "| Element | Example |\n",
                "|---------|--------|\n",
                "| Subject | \"A majestic dragon\" |\n",
                "| Style | \"digital art, oil painting, anime\" |\n",
                "| Quality | \"highly detailed, 4k, masterpiece\" |\n",
                "| Lighting | \"golden hour, dramatic lighting\" |\n",
                "| Camera | \"wide angle, portrait, macro\" |\n",
                "\n",
                "### Template\n",
                "```\n",
                "[subject], [style], [quality modifiers], [lighting], [artist reference]\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prompt examples\n",
                "prompts = [\n",
                "    # Basic\n",
                "    \"A cat\",\n",
                "    \n",
                "    # With style\n",
                "    \"A cat, watercolor painting style\",\n",
                "    \n",
                "    # With quality\n",
                "    \"A cat, watercolor painting, highly detailed, vibrant colors\",\n",
                "    \n",
                "    # Full prompt\n",
                "    \"A majestic cat sitting on a throne, watercolor painting, highly detailed, \"\n",
                "    \"golden hour lighting, by Studio Ghibli, masterpiece, 4k\"\n",
                "]\n",
                "\n",
                "print(\"Prompt Progression:\")\n",
                "for i, p in enumerate(prompts, 1):\n",
                "    print(f\"\\n{i}. {p}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Interview Questions\n",
                "\n",
                "**Q1: Explain the forward and reverse diffusion process.**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- Forward: Gradually add Gaussian noise to data over T steps until pure noise\n",
                "- Reverse: Learn to predict/remove noise at each step to reconstruct clean data\n",
                "- The model learns the noise distribution at each timestep\n",
                "</details>\n",
                "\n",
                "**Q2: Why does Stable Diffusion work in latent space?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Latent space is much smaller (64√ó64√ó4 vs 512√ó512√ó3), making:\n",
                "- Training faster and cheaper\n",
                "- Inference faster (fewer pixels to denoise)\n",
                "- The VAE preserves perceptually important features\n",
                "</details>\n",
                "\n",
                "**Q3: What is classifier-free guidance?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "A technique to control prompt adherence without a separate classifier:\n",
                "- Train model with random prompt dropout\n",
                "- At inference: blend conditional and unconditional predictions\n",
                "- Higher guidance = stronger prompt following, but can over-saturate\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Summary\n",
                "\n",
                "| Concept | Key Point |\n",
                "|---------|----------|\n",
                "| Diffusion | Learn to reverse gradual noising |\n",
                "| Latent Diffusion | Work in compressed space |\n",
                "| CFG | Balance quality vs prompt adherence |\n",
                "| Schedulers | Trade speed for quality |\n",
                "| ControlNet | Add spatial conditioning |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. References\n",
                "\n",
                "- [DDPM Paper](https://arxiv.org/abs/2006.11239)\n",
                "- [Latent Diffusion Paper](https://arxiv.org/abs/2112.10752)\n",
                "- [Diffusers Library](https://huggingface.co/docs/diffusers/)\n",
                "- [ControlNet Paper](https://arxiv.org/abs/2302.05543)\n",
                "\n",
                "---\n",
                "**üéâ Congratulations! You've completed the full NLP + Multimodal curriculum!**\n",
                "\n",
                "Return to [Module 00: NLP Pipeline Overview](../00_nlp_pipeline/00_nlp_pipeline_overview.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}