{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 15: BERT & Encoder Models\n",
                "\n",
                "**Bidirectional Encoder Representations from Transformers**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- ‚úÖ Understand BERT architecture\n",
                "- ‚úÖ Know pretraining objectives (MLM, NSP)\n",
                "- ‚úÖ Fine-tune BERT with HuggingFace\n",
                "- ‚úÖ Know BERT variants (RoBERTa, DistilBERT, ALBERT)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 14: Transformer Architecture](../14_transformer_architecture/14_transformer_architecture.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. BERT Architecture\n",
                "\n",
                "### Key Insight\n",
                "BERT = **Encoder-only** Transformer, trained **bidirectionally**\n",
                "\n",
                "```\n",
                "[CLS] The cat sat [MASK] the mat [SEP]\n",
                "  ‚Üì    ‚Üì   ‚Üì   ‚Üì    ‚Üì     ‚Üì   ‚Üì   ‚Üì\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ         Transformer Encoder         ‚îÇ\n",
                "‚îÇ        (12 or 24 layers)            ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "  ‚Üì    ‚Üì   ‚Üì   ‚Üì    ‚Üì     ‚Üì   ‚Üì   ‚Üì\n",
                " cls  h1  h2  h3   h4    h5  h6  sep\n",
                "```\n",
                "\n",
                "### Model Sizes\n",
                "\n",
                "| Model | Layers | Hidden | Heads | Params |\n",
                "|-------|--------|--------|-------|--------|\n",
                "| BERT-base | 12 | 768 | 12 | 110M |\n",
                "| BERT-large | 24 | 1024 | 16 | 340M |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Pretraining Objectives\n",
                "\n",
                "### Masked Language Modeling (MLM)\n",
                "```\n",
                "Input:  \"The cat [MASK] on the mat\"\n",
                "Output: Predict \"sat\" at [MASK] position\n",
                "```\n",
                "- Mask 15% of tokens randomly\n",
                "- 80% replace with [MASK], 10% random word, 10% keep\n",
                "\n",
                "### Next Sentence Prediction (NSP)\n",
                "```\n",
                "[CLS] Sentence A [SEP] Sentence B [SEP]\n",
                "‚Üí Predict: Is B the next sentence after A?\n",
                "```\n",
                "- Binary classification on [CLS] token\n",
                "- 50% real next sentence, 50% random"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Using BERT with HuggingFace"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load tokenizer and model\n",
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
                "model = BertModel.from_pretrained('bert-base-uncased')\n",
                "\n",
                "# Tokenize\n",
                "text = \"The cat sat on the mat.\"\n",
                "inputs = tokenizer(text, return_tensors='pt')\n",
                "\n",
                "print(\"Tokenized:\")\n",
                "print(f\"  input_ids: {inputs['input_ids']}\")\n",
                "print(f\"  tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
                "\n",
                "# Forward pass\n",
                "with torch.no_grad():\n",
                "    outputs = model(**inputs)\n",
                "\n",
                "print(f\"\\nOutputs:\")\n",
                "print(f\"  last_hidden_state: {outputs.last_hidden_state.shape}\")\n",
                "print(f\"  pooler_output ([CLS]): {outputs.pooler_output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. BERT for Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Method 1: Use built-in classification head\n",
                "classifier = BertForSequenceClassification.from_pretrained(\n",
                "    'bert-base-uncased', \n",
                "    num_labels=2\n",
                ")\n",
                "\n",
                "inputs = tokenizer(\"This movie is great!\", return_tensors='pt')\n",
                "with torch.no_grad():\n",
                "    outputs = classifier(**inputs)\n",
                "\n",
                "print(f\"Logits: {outputs.logits}\")\n",
                "print(f\"Prediction: {'Positive' if outputs.logits.argmax() == 1 else 'Negative'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Method 2: Custom classification head\n",
                "class BertClassifier(nn.Module):\n",
                "    def __init__(self, num_classes=2, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.fc = nn.Linear(768, num_classes)  # 768 = BERT hidden size\n",
                "    \n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
                "        cls_output = outputs.pooler_output  # [CLS] token\n",
                "        return self.fc(self.dropout(cls_output))\n",
                "\n",
                "# Test\n",
                "model = BertClassifier(num_classes=3)\n",
                "inputs = tokenizer(\"Hello world\", return_tensors='pt')\n",
                "logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
                "print(f\"Custom classifier output: {logits.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Fine-tuning Best Practices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fine-tuning hyperparameters\n",
                "from transformers import AdamW, get_linear_schedule_with_warmup\n",
                "\n",
                "# Typical settings\n",
                "learning_rate = 2e-5  # Much smaller than training from scratch!\n",
                "epochs = 3\n",
                "batch_size = 16\n",
                "warmup_steps = 500\n",
                "\n",
                "# Optimizer with weight decay\n",
                "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
                "\n",
                "# Learning rate scheduler\n",
                "total_steps = 1000  # Depends on dataset size\n",
                "scheduler = get_linear_schedule_with_warmup(\n",
                "    optimizer, \n",
                "    num_warmup_steps=warmup_steps,\n",
                "    num_training_steps=total_steps\n",
                ")\n",
                "\n",
                "print(\"Fine-tuning setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. BERT Variants\n",
                "\n",
                "| Model | Key Change | Use Case |\n",
                "|-------|------------|----------|\n",
                "| **RoBERTa** | No NSP, more data, dynamic masking | Better performance |\n",
                "| **DistilBERT** | 6 layers, knowledge distillation | 60% smaller, 2x faster |\n",
                "| **ALBERT** | Parameter sharing, factorization | Much smaller |\n",
                "| **ELECTRA** | Replaced token detection | More efficient pretraining |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using different models\n",
                "models_to_try = [\n",
                "    'bert-base-uncased',\n",
                "    'roberta-base',\n",
                "    'distilbert-base-uncased',\n",
                "]\n",
                "\n",
                "for model_name in models_to_try:\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    model = AutoModel.from_pretrained(model_name)\n",
                "    params = sum(p.numel() for p in model.parameters()) / 1e6\n",
                "    print(f\"{model_name}: {params:.1f}M parameters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. üî• Real-World Usage\n",
                "\n",
                "### When to Use BERT\n",
                "\n",
                "| Task | BERT Variant |\n",
                "|------|-------------|\n",
                "| Classification | BERT + [CLS] |\n",
                "| NER/Tagging | BERT + token outputs |\n",
                "| QA | BERT + start/end heads |\n",
                "| Sentence Similarity | Sentence-BERT |\n",
                "| Production (fast) | DistilBERT |\n",
                "\n",
                "### 2024 Landscape\n",
                "- BERT still widely used for **classification**\n",
                "- For generation ‚Üí use GPT or T5\n",
                "- For embeddings ‚Üí consider newer models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Questions\n",
                "\n",
                "**Q1: How does BERT differ from GPT?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- **BERT**: Encoder-only, bidirectional, MLM pretraining, good for understanding\n",
                "- **GPT**: Decoder-only, left-to-right, LM pretraining, good for generation\n",
                "</details>\n",
                "\n",
                "**Q2: What is the [CLS] token for?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Special token at the start. Its final hidden state is used as aggregate sequence representation for classification tasks. Trained via NSP objective.\n",
                "</details>\n",
                "\n",
                "**Q3: Why fine-tune with low learning rate?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Pretrained weights are already good. High LR would destroy them. Typical: 2e-5 to 5e-5. Also use warmup to avoid early instability.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "- **BERT**: Bidirectional encoder, pretrained on MLM + NSP\n",
                "- **[CLS] token**: Sequence-level representation\n",
                "- **Fine-tuning**: Low LR (2e-5), warmup, 2-4 epochs\n",
                "- **Variants**: RoBERTa (better), DistilBERT (faster)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. References\n",
                "\n",
                "- [BERT Paper](https://arxiv.org/abs/1810.04805)\n",
                "- [HuggingFace BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
                "- [The Illustrated BERT](https://jalammar.github.io/illustrated-bert/)\n",
                "\n",
                "---\n",
                "**Next:** [Module 16: GPT & Decoder Models](../16_gpt/16_gpt.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}