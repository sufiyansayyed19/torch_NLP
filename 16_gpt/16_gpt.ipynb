{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 16: GPT & Decoder Models\n",
                "\n",
                "**Generative Pre-trained Transformers**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- ‚úÖ Understand GPT architecture\n",
                "- ‚úÖ Know causal language modeling\n",
                "- ‚úÖ Implement generation strategies\n",
                "- ‚úÖ Use HuggingFace GPT-2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 15: BERT](../15_bert/15_bert.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. GPT vs BERT\n",
                "\n",
                "| Aspect | BERT | GPT |\n",
                "|--------|------|-----|\n",
                "| Architecture | Encoder | Decoder |\n",
                "| Direction | Bidirectional | Left-to-right (causal) |\n",
                "| Pretraining | MLM + NSP | Causal LM |\n",
                "| Best for | Understanding | Generation |\n",
                "\n",
                "### GPT Architecture\n",
                "```\n",
                "  Input:  \"The cat sat\"\n",
                "           ‚Üì   ‚Üì   ‚Üì\n",
                "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "    ‚îÇ Masked Self-Attn ‚îÇ  ‚Üê Can only attend left\n",
                "    ‚îÇ      + FFN       ‚îÇ\n",
                "    ‚îÇ   (N layers)     ‚îÇ\n",
                "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "           ‚Üì   ‚Üì   ‚Üì\n",
                "  Output: Predict next token at each position\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Causal Language Modeling\n",
                "\n",
                "### Training Objective\n",
                "Predict next token given all previous tokens:\n",
                "\n",
                "$$\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(x_t | x_1, ..., x_{t-1})$$\n",
                "\n",
                "### Causal Mask\n",
                "```\n",
                "Position can attend to:\n",
                "     1  2  3  4\n",
                "1  [ 1  0  0  0 ]  ‚Üê Position 1 sees only itself\n",
                "2  [ 1  1  0  0 ]  ‚Üê Position 2 sees 1,2\n",
                "3  [ 1  1  1  0 ]  ‚Üê Position 3 sees 1,2,3\n",
                "4  [ 1  1  1  1 ]  ‚Üê Position 4 sees all\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Using GPT-2 with HuggingFace"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load GPT-2\n",
                "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
                "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
                "model.eval()\n",
                "\n",
                "# Set pad token (GPT-2 doesn't have one by default)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "print(f\"GPT-2 Parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
                "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple generation\n",
                "prompt = \"The meaning of life is\"\n",
                "inputs = tokenizer(prompt, return_tensors='pt')\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        inputs['input_ids'],\n",
                "        max_length=50,\n",
                "        do_sample=True,\n",
                "        temperature=0.7,\n",
                "        top_k=50,\n",
                "        top_p=0.95,\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "\n",
                "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "print(generated)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Generation Strategies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_with_strategy(prompt, strategy='greedy', **kwargs):\n",
                "    \"\"\"Generate text with different strategies.\"\"\"\n",
                "    inputs = tokenizer(prompt, return_tensors='pt')\n",
                "    \n",
                "    gen_kwargs = {\n",
                "        'max_length': 50,\n",
                "        'pad_token_id': tokenizer.eos_token_id\n",
                "    }\n",
                "    \n",
                "    if strategy == 'greedy':\n",
                "        gen_kwargs['do_sample'] = False\n",
                "    elif strategy == 'beam':\n",
                "        gen_kwargs['num_beams'] = kwargs.get('num_beams', 5)\n",
                "        gen_kwargs['do_sample'] = False\n",
                "    elif strategy == 'sample':\n",
                "        gen_kwargs['do_sample'] = True\n",
                "        gen_kwargs['temperature'] = kwargs.get('temperature', 1.0)\n",
                "    elif strategy == 'top_k':\n",
                "        gen_kwargs['do_sample'] = True\n",
                "        gen_kwargs['top_k'] = kwargs.get('top_k', 50)\n",
                "    elif strategy == 'top_p':\n",
                "        gen_kwargs['do_sample'] = True\n",
                "        gen_kwargs['top_p'] = kwargs.get('top_p', 0.95)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(inputs['input_ids'], **gen_kwargs)\n",
                "    \n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Compare strategies\n",
                "prompt = \"Once upon a time\"\n",
                "print(\"=\" * 50)\n",
                "for strategy in ['greedy', 'beam', 'top_k', 'top_p']:\n",
                "    print(f\"\\n{strategy.upper()}:\")\n",
                "    print(generate_with_strategy(prompt, strategy))\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Generation Strategies Explained\n",
                "\n",
                "| Strategy | Description | When to Use |\n",
                "|----------|-------------|-------------|\n",
                "| **Greedy** | Pick highest prob token | Deterministic, boring |\n",
                "| **Beam Search** | Keep top-k sequences | Better quality, diverse |\n",
                "| **Sampling** | Sample from distribution | Creative, may be incoherent |\n",
                "| **Top-k** | Sample from top-k tokens | Balance quality/diversity |\n",
                "| **Top-p (nucleus)** | Sample until cumulative prob ‚â• p | Most commonly used |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Temperature effect\n",
                "prompt = \"The future of AI is\"\n",
                "\n",
                "print(\"Temperature effect:\")\n",
                "for temp in [0.3, 0.7, 1.0, 1.5]:\n",
                "    result = generate_with_strategy(prompt, 'sample', temperature=temp)\n",
                "    print(f\"\\nTemp={temp}: {result}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. GPT Model Family\n",
                "\n",
                "| Model | Params | Context | Year |\n",
                "|-------|--------|---------|------|\n",
                "| GPT-1 | 117M | 512 | 2018 |\n",
                "| GPT-2 | 1.5B | 1024 | 2019 |\n",
                "| GPT-3 | 175B | 2048 | 2020 |\n",
                "| GPT-4 | ~1.7T? | 128K | 2023 |\n",
                "\n",
                "### Available on HuggingFace\n",
                "```python\n",
                "'gpt2'        # 124M\n",
                "'gpt2-medium' # 355M\n",
                "'gpt2-large'  # 774M\n",
                "'gpt2-xl'     # 1.5B\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. üî• Real-World Usage\n",
                "\n",
                "### When to Use GPT-style Models\n",
                "\n",
                "| Task | Use |\n",
                "|------|-----|\n",
                "| Text generation | ‚úÖ GPT |\n",
                "| Chatbots | ‚úÖ GPT (instruction-tuned) |\n",
                "| Code generation | ‚úÖ CodeGPT, Codex |\n",
                "| Classification | ‚ùå Use BERT |\n",
                "\n",
                "### 2024 Practice\n",
                "- **API**: Use GPT-4 API for best results\n",
                "- **Local**: LLaMA-2, Mistral, Phi-2\n",
                "- **Fine-tuning**: LoRA for efficiency"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Questions\n",
                "\n",
                "**Q1: What is the difference between GPT and BERT?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- GPT: Decoder-only, left-to-right, trained with causal LM, for generation\n",
                "- BERT: Encoder-only, bidirectional, trained with MLM, for understanding\n",
                "</details>\n",
                "\n",
                "**Q2: What is top-p (nucleus) sampling?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Sample from smallest set of tokens whose cumulative probability ‚â• p. Adapts to token distribution‚Äîuses few tokens when confident, more when uncertain.\n",
                "</details>\n",
                "\n",
                "**Q3: Why is temperature used?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Temperature scales logits before softmax. Low temp ‚Üí sharper distribution (confident), high temp ‚Üí flatter (random). Controls creativity vs consistency tradeoff.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "- **GPT**: Decoder-only Transformer for generation\n",
                "- **Causal LM**: Predict next token, can only see past\n",
                "- **Sampling**: Temperature, top-k, top-p (nucleus)\n",
                "- **In practice**: Use APIs or fine-tune smaller models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. References\n",
                "\n",
                "- [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
                "- [GPT-3 Paper](https://arxiv.org/abs/2005.14165)\n",
                "- [HuggingFace Generation](https://huggingface.co/docs/transformers/generation_strategies)\n",
                "\n",
                "---\n",
                "**Next:** [Module 17: HuggingFace Ecosystem](../17_huggingface/17_huggingface.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}