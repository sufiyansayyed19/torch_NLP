{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkIEw3Z86o4y"
      },
      "source": [
        "# Module 09: Text Classification with RNNs\n",
        "\n",
        "**End-to-End Sentiment Analysis Pipeline**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knlw3rZx6o41"
      },
      "source": [
        "## 1. Objectives\n",
        "\n",
        "- âœ… Build complete text classification pipeline\n",
        "- âœ… Handle variable-length sequences correctly\n",
        "- âœ… Train on IMDB sentiment dataset\n",
        "- âœ… Evaluate with proper metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctKi9cPg6o42"
      },
      "source": [
        "## 2. Prerequisites\n",
        "\n",
        "- [Module 08: Bidirectional & Deep RNNs](../08_bidirectional_deep_rnns/08_bidirectional_deep_rnns.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmd3ukv26o43",
        "outputId": "bc7b280c-20d7-4475-ea64-a6df11a9c972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q5ZBKDp6o46"
      },
      "source": [
        "## 3. Dataset & Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY3srWBK6o47",
        "outputId": "2e416e98-8e23-49ae-8c5e-11513586d302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 44\n"
          ]
        }
      ],
      "source": [
        "# Sample data (in practice, use torchtext or HuggingFace datasets)\n",
        "train_texts = [\n",
        "    \"This movie was absolutely fantastic and amazing\",\n",
        "    \"Terrible film waste of time and money\",\n",
        "    \"I loved every moment of this masterpiece\",\n",
        "    \"Awful acting and horrible plot\",\n",
        "    \"Best movie I have seen in years\",\n",
        "    \"Boring and predictable would not recommend\",\n",
        "    \"Brilliant cinematography and great performances\",\n",
        "    \"Complete disaster avoid at all costs\"\n",
        "]\n",
        "train_labels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative\n",
        "\n",
        "# Build vocabulary\n",
        "class Vocabulary:\n",
        "    def __init__(self, min_freq=1):\n",
        "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "    def build(self, texts):\n",
        "        word_counts = Counter()\n",
        "        for text in texts:\n",
        "            word_counts.update(text.lower().split())\n",
        "\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= self.min_freq:\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.word2idx.get(w, 1) for w in text.lower().split()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "vocab = Vocabulary()\n",
        "vocab.build(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ7TUKFr6o48",
        "outputId": "498506de-27b4-453e-a500-7244354dbacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts shape: torch.Size([4, 7])\n",
            "Labels: tensor([1, 1, 0, 1])\n",
            "Lengths: tensor([5, 7, 7, 7])\n"
          ]
        }
      ],
      "source": [
        "# Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab):\n",
        "        self.texts = [torch.tensor(vocab.encode(t)) for t in texts]\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# Collate function for variable lengths\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    lengths = torch.tensor([len(t) for t in texts])\n",
        "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "    return texts_padded, torch.stack(list(labels)), lengths\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TextDataset(train_texts, train_labels, vocab)\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Test\n",
        "batch = next(iter(loader))\n",
        "print(f\"Texts shape: {batch[0].shape}\")\n",
        "print(f\"Labels: {batch[1]}\")\n",
        "print(f\"Lengths: {batch[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aaO7zFx6o49"
      },
      "source": [
        "## 4. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0MZD3vL6o4-",
        "outputId": "e78f9e24-0f46-4a6e-8a33-3d6da4a66b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: 668,338\n"
          ]
        }
      ],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    \"\"\"BiLSTM for sentiment classification.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes,\n",
        "                 num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim, hidden_dim, num_layers,\n",
        "            batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(lstm_output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # x: (batch, seq), lengths: (batch,)\n",
        "        embedded = self.embedding(x)  # (batch, seq, embed)\n",
        "\n",
        "        # Pack for efficiency with variable lengths\n",
        "        packed = pack_padded_sequence(\n",
        "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        packed_out, (h_n, _) = self.lstm(packed)\n",
        "\n",
        "        # Use final hidden states from both directions\n",
        "        # h_n: (num_layers * num_directions, batch, hidden)\n",
        "        if self.lstm.bidirectional:\n",
        "            h_fwd = h_n[-2]  # Last layer forward\n",
        "            h_bwd = h_n[-1]  # Last layer backward\n",
        "            h = torch.cat([h_fwd, h_bwd], dim=-1)\n",
        "        else:\n",
        "            h = h_n[-1]\n",
        "\n",
        "        return self.fc(h)\n",
        "\n",
        "# Create model\n",
        "model = SentimentClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=100,\n",
        "    hidden_dim=128,\n",
        "    num_classes=2\n",
        ").to(device)\n",
        "\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7ZdLli26o4_"
      },
      "source": [
        "## 5. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_V38YaJ6o4_",
        "outputId": "6741ccad-f7ff-480c-8189-2e002555707e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=0.6957, Acc=37.50%\n",
            "Epoch 2: Loss=0.6763, Acc=87.50%\n",
            "Epoch 3: Loss=0.6494, Acc=100.00%\n",
            "Epoch 4: Loss=0.6261, Acc=100.00%\n",
            "Epoch 5: Loss=0.5783, Acc=100.00%\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for texts, labels, lengths in loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts, lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, labels, lengths in loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts, lengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "# Training\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss, train_acc = train_epoch(model, loader, optimizer, criterion)\n",
        "    print(f\"Epoch {epoch+1}: Loss={train_loss:.4f}, Acc={train_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G_THL-c6o4_"
      },
      "source": [
        "## 6. Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfJgUBp_6o5A",
        "outputId": "328d96a3-e922-4a41-87fb-20508c39f1ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is absolutely wonderful\n",
            "  â†’ Positive (58.54%)\n",
            "\n",
            "Terrible waste of time\n",
            "  â†’ Negative (51.85%)\n",
            "\n",
            "Not bad but could be better\n",
            "  â†’ Positive (54.23%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def predict(model, vocab, text):\n",
        "    model.eval()\n",
        "    tokens = torch.tensor([vocab.encode(text)]).to(device)\n",
        "    lengths = torch.tensor([tokens.size(1)])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(tokens, lengths)\n",
        "        prob = torch.softmax(output, dim=1)\n",
        "        pred = output.argmax(dim=1).item()\n",
        "\n",
        "    return 'Positive' if pred == 1 else 'Negative', prob[0][pred].item()\n",
        "\n",
        "# Test predictions\n",
        "test_texts = [\n",
        "    \"This is absolutely wonderful\",\n",
        "    \"Terrible waste of time\",\n",
        "    \"Not bad but could be better\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    label, conf = predict(model, vocab, text)\n",
        "    print(f\"{text}\")\n",
        "    print(f\"  â†’ {label} ({conf:.2%})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VL0eOTF6o5A"
      },
      "source": [
        "## 7. ðŸ”¥ Real-World Usage\n",
        "\n",
        "### Model Selection (2024)\n",
        "\n",
        "| Data Size | Compute | Model |\n",
        "|-----------|---------|-------|\n",
        "| < 1K | Low | TF-IDF + LogReg |\n",
        "| 1K-10K | Low | BiLSTM |\n",
        "| 1K-10K | Medium | DistilBERT |\n",
        "| > 10K | Any | BERT/RoBERTa |\n",
        "| Any | High + API | GPT-4 (zero-shot) |\n",
        "\n",
        "### Production Tips\n",
        "\n",
        "- Always start with **TF-IDF baseline**\n",
        "- Use **gradient clipping** for RNNs\n",
        "- **Pack sequences** for efficiency\n",
        "- Consider **BERT** for best accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dd9_pKN6o5A"
      },
      "source": [
        "## 8. Interview Questions\n",
        "\n",
        "**Q1: How do you handle variable-length sequences?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "- Pad sequences to same length\n",
        "- Use `pack_padded_sequence` for efficiency\n",
        "- Store lengths for proper handling\n",
        "</details>\n",
        "\n",
        "**Q2: Why use gradient clipping for RNNs?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "Prevents exploding gradients which cause training instability. Clips gradients to max norm (typically 1.0-5.0).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX29rhFD6o5B"
      },
      "source": [
        "## 9. Summary\n",
        "\n",
        "- **Pipeline**: Vocab â†’ Dataset â†’ DataLoader â†’ Model â†’ Train\n",
        "- **Handle variable lengths**: `pack_padded_sequence`\n",
        "- **Gradient clipping**: Essential for RNN training\n",
        "- **BiLSTM**: Use for classification tasks\n",
        "- **In practice**: Consider BERT for best results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSia5FQg6o5B"
      },
      "source": [
        "## 10. References\n",
        "\n",
        "- [PyTorch Packing](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)\n",
        "- [IMDB Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "\n",
        "---\n",
        "**Next:** [Module 10: Named Entity Recognition](../10_ner/10_ner.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}