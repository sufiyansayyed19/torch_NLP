{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 28: LlamaIndex\n",
                "\n",
                "**Data Framework for LLM Applications**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand LlamaIndex architecture\n",
                "- âœ… Master data connectors and loaders\n",
                "- âœ… Build different index types\n",
                "- âœ… Implement query engines\n",
                "- âœ… Create production RAG pipelines"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 21: RAG](../21_rag/21_rag.ipynb)\n",
                "- [Module 26: LangChain](../26_langchain/26_langchain.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. LlamaIndex vs LangChain\n",
                "\n",
                "### Key Differences\n",
                "\n",
                "| Aspect | LlamaIndex | LangChain |\n",
                "|--------|------------|----------|\n",
                "| Focus | Data indexing & retrieval | Agent orchestration |\n",
                "| Strength | RAG pipelines | General LLM apps |\n",
                "| Philosophy | Data-centric | Chain-centric |\n",
                "| Best For | Knowledge bases | Multi-step agents |\n",
                "\n",
                "**Use Together**: LlamaIndex for retrieval + LangChain for orchestration!\n",
                "\n",
                "### Architecture\n",
                "\n",
                "```\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚                    LlamaIndex Pipeline                       â”‚\n",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "â”‚                                                              â”‚\n",
                "â”‚  [Data Sources] â”€â”€â†’ [Loaders] â”€â”€â†’ [Documents]               â”‚\n",
                "â”‚                                        â†“                     â”‚\n",
                "â”‚                                  [Node Parser]               â”‚\n",
                "â”‚                                        â†“                     â”‚\n",
                "â”‚                                    [Nodes]                   â”‚\n",
                "â”‚                                        â†“                     â”‚\n",
                "â”‚                    [Embedding Model] + [Index]               â”‚\n",
                "â”‚                                        â†“                     â”‚\n",
                "â”‚                                  [Query Engine]              â”‚\n",
                "â”‚                                        â†“                     â”‚\n",
                "â”‚                                    [Response]                â”‚\n",
                "â”‚                                                              â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install: pip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n",
                "\n",
                "# Set API key\n",
                "# import os\n",
                "# os.environ[\"OPENAI_API_KEY\"] = \"your-key\"\n",
                "\n",
                "from llama_index.core import (\n",
                "    VectorStoreIndex,\n",
                "    SimpleDirectoryReader,\n",
                "    Document,\n",
                "    Settings\n",
                ")\n",
                "from llama_index.llms.openai import OpenAI\n",
                "from llama_index.embeddings.openai import OpenAIEmbedding\n",
                "\n",
                "print(\"LlamaIndex imported!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Documents and Nodes\n",
                "\n",
                "### Theory\n",
                "\n",
                "| Concept | Description |\n",
                "|---------|-------------|\n",
                "| **Document** | Raw text with metadata |\n",
                "| **Node** | Chunk of document with relationships |\n",
                "| **Node Parser** | Splits documents into nodes |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_index.core import Document\n",
                "from llama_index.core.node_parser import SentenceSplitter\n",
                "\n",
                "# Create documents manually\n",
                "documents = [\n",
                "    Document(\n",
                "        text=\"LlamaIndex is a data framework for LLM applications. It provides tools for data ingestion, indexing, and querying.\",\n",
                "        metadata={\"source\": \"intro\", \"category\": \"overview\"}\n",
                "    ),\n",
                "    Document(\n",
                "        text=\"Vector indexes are the most common type. They embed documents and enable semantic search. Other index types include list, tree, and keyword.\",\n",
                "        metadata={\"source\": \"indexes\", \"category\": \"technical\"}\n",
                "    ),\n",
                "    Document(\n",
                "        text=\"Query engines combine retrieval with response synthesis. They can handle simple questions or complex multi-step queries.\",\n",
                "        metadata={\"source\": \"query\", \"category\": \"technical\"}\n",
                "    )\n",
                "]\n",
                "\n",
                "print(f\"Created {len(documents)} documents\")\n",
                "for doc in documents:\n",
                "    print(f\"  - {doc.metadata['source']}: {doc.text[:50]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parse documents into nodes\n",
                "parser = SentenceSplitter(\n",
                "    chunk_size=100,\n",
                "    chunk_overlap=20\n",
                ")\n",
                "\n",
                "nodes = parser.get_nodes_from_documents(documents)\n",
                "\n",
                "print(f\"Split into {len(nodes)} nodes:\")\n",
                "for i, node in enumerate(nodes):\n",
                "    print(f\"  Node {i}: {node.text[:60]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Loaders\n",
                "\n",
                "### Built-in Loaders\n",
                "\n",
                "| Loader | Source |\n",
                "|--------|--------|\n",
                "| SimpleDirectoryReader | Local files |\n",
                "| PDFReader | PDF documents |\n",
                "| WikipediaReader | Wikipedia articles |\n",
                "| NotionReader | Notion pages |\n",
                "| SlackReader | Slack messages |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load from directory\n",
                "# reader = SimpleDirectoryReader(\"./data\")\n",
                "# documents = reader.load_data()\n",
                "\n",
                "# Load specific file types\n",
                "# reader = SimpleDirectoryReader(\n",
                "#     input_dir=\"./data\",\n",
                "#     required_exts=[\".pdf\", \".txt\", \".md\"]\n",
                "# )\n",
                "\n",
                "print(\"Directory reader example (uncomment to use)\")\n",
                "\n",
                "# Using LlamaHub for more loaders\n",
                "# pip install llama-index-readers-web\n",
                "# from llama_index.readers.web import SimpleWebPageReader\n",
                "# loader = SimpleWebPageReader()\n",
                "# documents = loader.load_data([\"https://example.com\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom loader example\n",
                "from llama_index.core.readers.base import BaseReader\n",
                "from typing import List\n",
                "\n",
                "class CustomCSVReader(BaseReader):\n",
                "    \"\"\"Custom reader for CSV files.\"\"\"\n",
                "    \n",
                "    def load_data(self, file_path: str) -> List[Document]:\n",
                "        import csv\n",
                "        \n",
                "        documents = []\n",
                "        with open(file_path, 'r') as f:\n",
                "            reader = csv.DictReader(f)\n",
                "            for row in reader:\n",
                "                text = \" | \".join(f\"{k}: {v}\" for k, v in row.items())\n",
                "                documents.append(Document(\n",
                "                    text=text,\n",
                "                    metadata={\"source\": file_path}\n",
                "                ))\n",
                "        return documents\n",
                "\n",
                "print(\"Custom CSV reader defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Index Types\n",
                "\n",
                "### Theory\n",
                "\n",
                "| Index Type | How it Works | Best For |\n",
                "|------------|-------------|----------|\n",
                "| **VectorStoreIndex** | Embeds chunks, similarity search | Default choice |\n",
                "| **SummaryIndex** | Traverses all nodes | Small datasets |\n",
                "| **TreeIndex** | Hierarchical summarization | Q&A over docs |\n",
                "| **KeywordTableIndex** | Keyword extraction | Keyword search |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_index.core import VectorStoreIndex, Settings\n",
                "from llama_index.llms.openai import OpenAI\n",
                "from llama_index.embeddings.openai import OpenAIEmbedding\n",
                "\n",
                "# Configure settings\n",
                "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
                "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
                "\n",
                "# Create vector index\n",
                "index = VectorStoreIndex.from_documents(documents)\n",
                "\n",
                "print(\"Vector index created!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary Index - reads all nodes\n",
                "from llama_index.core import SummaryIndex\n",
                "\n",
                "summary_index = SummaryIndex.from_documents(documents)\n",
                "\n",
                "print(\"Summary index created (good for small datasets)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Persist index to disk\n",
                "index.storage_context.persist(persist_dir=\"./storage\")\n",
                "\n",
                "# Load from disk later\n",
                "from llama_index.core import StorageContext, load_index_from_storage\n",
                "\n",
                "# storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
                "# loaded_index = load_index_from_storage(storage_context)\n",
                "\n",
                "print(\"Index persisted to ./storage\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Query Engines\n",
                "\n",
                "### Theory\n",
                "\n",
                "Query engines handle:\n",
                "1. **Retrieval**: Find relevant nodes\n",
                "2. **Postprocessing**: Rerank, filter\n",
                "3. **Response Synthesis**: Generate answer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic query engine\n",
                "query_engine = index.as_query_engine(\n",
                "    similarity_top_k=2  # Retrieve top 2 chunks\n",
                ")\n",
                "\n",
                "# Query\n",
                "response = query_engine.query(\"What is LlamaIndex?\")\n",
                "\n",
                "print(f\"Answer: {response}\")\n",
                "print(f\"\\nSource nodes:\")\n",
                "for node in response.source_nodes:\n",
                "    print(f\"  - Score: {node.score:.3f}\")\n",
                "    print(f\"    Text: {node.text[:80]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Query with custom response mode\n",
                "from llama_index.core.response_synthesizers import ResponseMode\n",
                "\n",
                "# Response modes:\n",
                "# - REFINE: Iteratively refine answer with each chunk\n",
                "# - COMPACT: Compact chunks before synthesizing\n",
                "# - TREE_SUMMARIZE: Build tree and summarize\n",
                "# - SIMPLE_SUMMARIZE: Simple concatenation\n",
                "\n",
                "query_engine = index.as_query_engine(\n",
                "    response_mode=ResponseMode.COMPACT,\n",
                "    similarity_top_k=3\n",
                ")\n",
                "\n",
                "response = query_engine.query(\"Explain different index types.\")\n",
                "print(f\"Compact response: {response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Retrievers and Postprocessors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom retriever\n",
                "retriever = index.as_retriever(\n",
                "    similarity_top_k=5\n",
                ")\n",
                "\n",
                "# Retrieve nodes\n",
                "nodes = retriever.retrieve(\"What are query engines?\")\n",
                "\n",
                "print(f\"Retrieved {len(nodes)} nodes:\")\n",
                "for node in nodes:\n",
                "    print(f\"  - Score: {node.score:.3f} | {node.text[:60]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Postprocessors for reranking and filtering\n",
                "from llama_index.core.postprocessor import (\n",
                "    SimilarityPostprocessor,\n",
                "    KeywordNodePostprocessor\n",
                ")\n",
                "\n",
                "# Filter by minimum similarity\n",
                "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.5)\n",
                "\n",
                "# Apply to nodes\n",
                "filtered_nodes = postprocessor.postprocess_nodes(nodes)\n",
                "print(f\"After filtering: {len(filtered_nodes)} nodes\")\n",
                "\n",
                "# Keyword filter\n",
                "keyword_filter = KeywordNodePostprocessor(\n",
                "    required_keywords=[\"query\"],\n",
                "    exclude_keywords=[\"tree\"]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Chat Engine (Conversational)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chat engine with memory\n",
                "chat_engine = index.as_chat_engine(\n",
                "    chat_mode=\"condense_plus_context\",  # Condenses history + retrieves context\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "# Multi-turn conversation\n",
                "response1 = chat_engine.chat(\"What is LlamaIndex?\")\n",
                "print(f\"Bot: {response1}\\n\")\n",
                "\n",
                "response2 = chat_engine.chat(\"What indexes does it support?\")\n",
                "print(f\"Bot: {response2}\\n\")\n",
                "\n",
                "response3 = chat_engine.chat(\"Which one should I use for semantic search?\")\n",
                "print(f\"Bot: {response3}\")\n",
                "\n",
                "# Reset conversation\n",
                "chat_engine.reset()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Advanced: Routers and Agents"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
                "from llama_index.core.query_engine import RouterQueryEngine\n",
                "from llama_index.core.selectors import LLMSingleSelector\n",
                "\n",
                "# Create multiple indexes\n",
                "tech_docs = [d for d in documents if d.metadata.get(\"category\") == \"technical\"]\n",
                "overview_docs = [d for d in documents if d.metadata.get(\"category\") == \"overview\"]\n",
                "\n",
                "tech_index = VectorStoreIndex.from_documents(tech_docs)\n",
                "overview_index = VectorStoreIndex.from_documents(overview_docs)\n",
                "\n",
                "# Create tools\n",
                "tech_tool = QueryEngineTool(\n",
                "    query_engine=tech_index.as_query_engine(),\n",
                "    metadata=ToolMetadata(\n",
                "        name=\"technical_docs\",\n",
                "        description=\"Technical documentation about indexes and queries\"\n",
                "    )\n",
                ")\n",
                "\n",
                "overview_tool = QueryEngineTool(\n",
                "    query_engine=overview_index.as_query_engine(),\n",
                "    metadata=ToolMetadata(\n",
                "        name=\"overview_docs\",\n",
                "        description=\"General overview and introduction\"\n",
                "    )\n",
                ")\n",
                "\n",
                "# Router query engine\n",
                "router_engine = RouterQueryEngine(\n",
                "    selector=LLMSingleSelector.from_defaults(),\n",
                "    query_engine_tools=[tech_tool, overview_tool]\n",
                ")\n",
                "\n",
                "# Routes to appropriate index\n",
                "response = router_engine.query(\"Give me an overview of LlamaIndex\")\n",
                "print(f\"Routed response: {response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Production Tips\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "| Aspect | Recommendation |\n",
                "|--------|---------------|\n",
                "| Chunking | 512-1024 tokens, 20% overlap |\n",
                "| Embedding | text-embedding-3-small (cost) or large (quality) |\n",
                "| Retrieval | Start with k=3-5, tune based on relevance |\n",
                "| Reranking | Add cross-encoder reranker for quality |\n",
                "| Caching | Cache embeddings and LLM responses |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Production configuration\n",
                "from llama_index.core import Settings\n",
                "\n",
                "# Optimize for production\n",
                "Settings.chunk_size = 512\n",
                "Settings.chunk_overlap = 50\n",
                "Settings.num_output = 512  # Max response tokens\n",
                "\n",
                "# Use caching\n",
                "# from llama_index.core.storage.docstore import SimpleDocumentStore\n",
                "# from llama_index.core.storage.index_store import SimpleIndexStore\n",
                "\n",
                "print(\"Production settings configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Interview Questions\n",
                "\n",
                "**Q1: When should you use LlamaIndex vs LangChain?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- **LlamaIndex**: Complex RAG, multi-document Q&A, structured data indexing\n",
                "- **LangChain**: Agent workflows, tool use, multi-step reasoning\n",
                "- **Together**: LlamaIndex retriever + LangChain agent\n",
                "</details>\n",
                "\n",
                "**Q2: What's the difference between Documents and Nodes?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- **Document**: Raw source text with metadata (file, URL, etc.)\n",
                "- **Node**: Chunk of document with:\n",
                "  - Parent/child relationships\n",
                "  - Embeddings\n",
                "  - Source reference\n",
                "\n",
                "Nodes enable better retrieval and context tracking.\n",
                "</details>\n",
                "\n",
                "**Q3: How would you improve RAG relevance?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "1. Better chunking (semantic, not fixed-size)\n",
                "2. Hybrid search (dense + sparse)\n",
                "3. Reranking with cross-encoder\n",
                "4. Query expansion/rewriting\n",
                "5. Metadata filtering\n",
                "6. Parent document retrieval\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Summary\n",
                "\n",
                "| Component | Purpose |\n",
                "|-----------|--------|\n",
                "| Documents/Nodes | Data representation |\n",
                "| Loaders | Ingest from sources |\n",
                "| Indexes | Organize for retrieval |\n",
                "| Query Engines | Retrieve + synthesize |\n",
                "| Chat Engines | Conversational interface |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. References\n",
                "\n",
                "- [LlamaIndex Docs](https://docs.llamaindex.ai/)\n",
                "- [LlamaHub (Data Loaders)](https://llamahub.ai/)\n",
                "- [LlamaIndex GitHub](https://github.com/run-llama/llama_index)\n",
                "\n",
                "---\n",
                "**ðŸŽ‰ Congratulations! You've completed the full NLP + LLM Orchestration curriculum!**\n",
                "\n",
                "Return to [Module 00: NLP Pipeline Overview](../00_nlp_pipeline/00_nlp_pipeline_overview.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}