{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX5O2VT60sei"
      },
      "source": [
        "# Module 20: Debugging PyTorch Models\n",
        "\n",
        "**Finding and Fixing Issues in Neural Networks**\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- Master model inspection techniques\n",
        "- Debug shape mismatches systematically\n",
        "- Identify common PyTorch errors and their fixes\n",
        "- Use hooks for debugging forward/backward passes\n",
        "- Profile memory and performance\n",
        "- Handle device (CPU/GPU) issues\n",
        "- Debug data pipelines\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CR2ogoH0sek",
        "outputId": "534b0edc-456f-423e-a7f6-a8043223ec38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG7Qxs070sel"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 1: Model Inspection\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 Viewing Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVTkkbYL0sel",
        "outputId": "5a93b375-1ccc-46ca-d944-31b419ecdeec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "MODEL ARCHITECTURE\n",
            "==================================================\n",
            "SampleNet(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class SampleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = SampleNet()\n",
        "\n",
        "# Method 1: Simple print\n",
        "print(\"=\" * 50)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\" * 50)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v48LW3Av0sel",
        "outputId": "2af0599d-4479-4e62-f5b9-b2185e898247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "NAMED MODULES\n",
            "==================================================\n",
            "conv1: Conv2d\n",
            "bn1: BatchNorm2d\n",
            "conv2: Conv2d\n",
            "pool: MaxPool2d\n",
            "fc1: Linear\n",
            "fc2: Linear\n",
            "relu: ReLU\n",
            "dropout: Dropout\n"
          ]
        }
      ],
      "source": [
        "# Method 2: Named modules (shows hierarchy)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"NAMED MODULES\")\n",
        "print(\"=\" * 50)\n",
        "for name, module in model.named_modules():\n",
        "    if name:  # Skip root module\n",
        "        print(f\"{name}: {module.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LMU4D1w0sem",
        "outputId": "67607eb0-0de3-48fa-f3ea-1e3168081859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PARAMETERS\n",
            "==================================================\n",
            "conv1.weight                   | Shape: [16, 3, 3, 3]        | Trainable: True\n",
            "conv1.bias                     | Shape: [16]                 | Trainable: True\n",
            "bn1.weight                     | Shape: [16]                 | Trainable: True\n",
            "bn1.bias                       | Shape: [16]                 | Trainable: True\n",
            "conv2.weight                   | Shape: [32, 16, 3, 3]       | Trainable: True\n",
            "conv2.bias                     | Shape: [32]                 | Trainable: True\n",
            "fc1.weight                     | Shape: [128, 2048]          | Trainable: True\n",
            "fc1.bias                       | Shape: [128]                | Trainable: True\n",
            "fc2.weight                     | Shape: [10, 128]            | Trainable: True\n",
            "fc2.bias                       | Shape: [10]                 | Trainable: True\n",
            "\n",
            "Total parameters: 268,682\n",
            "Trainable parameters: 268,682\n"
          ]
        }
      ],
      "source": [
        "# Method 3: Named parameters with shapes\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"PARAMETERS\")\n",
        "print(\"=\" * 50)\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name:30} | Shape: {str(list(param.shape)):20} | Trainable: {param.requires_grad}\")\n",
        "    total_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziNswFVW0sem",
        "outputId": "11c26dbc-d9d5-46ac-d5f7-9e56a6d92344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "BUFFERS (non-trainable state)\n",
            "==================================================\n",
            "bn1.running_mean               | Shape: [16]\n",
            "bn1.running_var                | Shape: [16]\n",
            "bn1.num_batches_tracked        | Shape: []\n"
          ]
        }
      ],
      "source": [
        "# Method 4: Named buffers (non-trainable state like BatchNorm running stats)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"BUFFERS (non-trainable state)\")\n",
        "print(\"=\" * 50)\n",
        "for name, buffer in model.named_buffers():\n",
        "    print(f\"{name:30} | Shape: {list(buffer.shape)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlUxMoiE0sem"
      },
      "source": [
        "## 1.2 Accessing Specific Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CU6956i0sem",
        "outputId": "f83081bb-5f65-4ef2-d07e-e4739de34292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access conv1:\n",
            "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "conv1 weight shape: torch.Size([16, 3, 3, 3])\n",
            "conv1 bias shape: torch.Size([16])\n",
            "\n",
            "fc1 layer: Linear(in_features=2048, out_features=128, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# Access layer by name\n",
        "print(\"Access conv1:\")\n",
        "print(model.conv1)\n",
        "print(f\"conv1 weight shape: {model.conv1.weight.shape}\")\n",
        "print(f\"conv1 bias shape: {model.conv1.bias.shape}\")\n",
        "\n",
        "# Access using getattr (useful for dynamic access)\n",
        "layer_name = 'fc1'\n",
        "layer = getattr(model, layer_name)\n",
        "print(f\"\\n{layer_name} layer: {layer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHh1lTCE0sem",
        "outputId": "4617280e-a9f6-489b-a1ef-4c82d6a4af7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Children (immediate sub-modules):\n",
            "0: Conv2d\n",
            "1: BatchNorm2d\n",
            "2: Conv2d\n",
            "3: MaxPool2d\n",
            "4: Linear\n",
            "5: Linear\n",
            "6: ReLU\n",
            "7: Dropout\n"
          ]
        }
      ],
      "source": [
        "# Get children (immediate sub-modules)\n",
        "print(\"Children (immediate sub-modules):\")\n",
        "for i, child in enumerate(model.children()):\n",
        "    print(f\"{i}: {child.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjlXty0V0sem"
      },
      "source": [
        "## 1.3 Model Summary (like Keras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au1fsxjc0sem",
        "outputId": "ba5f949e-8c3a-49cb-8c57-27a7a33cce02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Layer                Output Shape                       Params\n",
            "======================================================================\n",
            "Conv2d               [1, 16, 32, 32]                       448\n",
            "BatchNorm2d          [1, 16, 32, 32]                        32\n",
            "ReLU                 [1, 16, 32, 32]                         0\n",
            "MaxPool2d            [1, 16, 16, 16]                         0\n",
            "Conv2d               [1, 32, 16, 16]                     4,640\n",
            "ReLU                 [1, 32, 16, 16]                         0\n",
            "MaxPool2d            [1, 32, 8, 8]                           0\n",
            "Linear               [1, 128]                          262,272\n",
            "ReLU                 [1, 128]                                0\n",
            "Dropout              [1, 128]                                0\n",
            "Linear               [1, 10]                             1,290\n",
            "======================================================================\n",
            "Total params: 268,682\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "def model_summary(model, input_size, batch_size=1):\n",
        "    \"\"\"\n",
        "    Generate a Keras-like model summary.\n",
        "    \"\"\"\n",
        "    def register_hook(module):\n",
        "        def hook(module, input, output):\n",
        "            class_name = module.__class__.__name__\n",
        "\n",
        "            # Get output shape\n",
        "            if isinstance(output, (list, tuple)):\n",
        "                output_shape = [list(o.shape) for o in output]\n",
        "            else:\n",
        "                output_shape = list(output.shape)\n",
        "\n",
        "            # Count parameters\n",
        "            params = sum(p.numel() for p in module.parameters(recurse=False))\n",
        "\n",
        "            summary.append({\n",
        "                'name': class_name,\n",
        "                'output_shape': output_shape,\n",
        "                'params': params\n",
        "            })\n",
        "\n",
        "        if not isinstance(module, nn.Sequential) and \\\n",
        "           not isinstance(module, nn.ModuleList) and \\\n",
        "           module != model:\n",
        "            hooks.append(module.register_forward_hook(hook))\n",
        "\n",
        "    summary = []\n",
        "    hooks = []\n",
        "\n",
        "    model.apply(register_hook)\n",
        "\n",
        "    # Create dummy input and run forward pass\n",
        "    x = torch.zeros(batch_size, *input_size)\n",
        "    model(x)\n",
        "\n",
        "    # Remove hooks\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    # Print summary\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"{'Layer':20} {'Output Shape':25} {'Params':>15}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    total_params = 0\n",
        "    for layer in summary:\n",
        "        print(f\"{layer['name']:20} {str(layer['output_shape']):25} {layer['params']:>15,}\")\n",
        "        total_params += layer['params']\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# Example usage\n",
        "model_summary(model, input_size=(3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSc8TAiE0sen"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 2: Shape Debugging\n",
        "\n",
        "---\n",
        "\n",
        "**The #1 source of PyTorch errors!**\n",
        "\n",
        "## 2.1 Common Shape Errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qndEK9eK0sen",
        "outputId": "2aafffd9-3176-4113-fb51-78f5fe6f0ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR 1: Matrix dimension mismatch\n",
            "----------------------------------------\n",
            "Error: mat1 and mat2 shapes cannot be multiplied (3x4 and 5x6)\n",
            "\n",
            "FIX: For A @ B, A.shape[1] must equal B.shape[0]\n",
            "A.shape = torch.Size([3, 4]), B.shape = torch.Size([5, 6])\n",
            "4 != 5, so this fails!\n"
          ]
        }
      ],
      "source": [
        "# Error 1: Matrix multiplication dimension mismatch\n",
        "print(\"ERROR 1: Matrix dimension mismatch\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    A = torch.randn(3, 4)\n",
        "    B = torch.randn(5, 6)  # Wrong! Should be (4, n)\n",
        "    C = A @ B\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nFIX: For A @ B, A.shape[1] must equal B.shape[0]\")\n",
        "    print(f\"A.shape = {A.shape}, B.shape = {B.shape}\")\n",
        "    print(f\"4 != 5, so this fails!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2vhS1YD0sen",
        "outputId": "4dc5b407-be47-4c4e-a6c3-aa91d2220ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ERROR 2: Wrong input dimensions for Conv2d\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Error 2: Conv2d expects 4D input\n",
        "print(\"\\nERROR 2: Wrong input dimensions for Conv2d\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "conv = nn.Conv2d(3, 16, 3)\n",
        "\n",
        "try:\n",
        "    x = torch.randn(3, 32, 32)  # Missing batch dimension!\n",
        "    out = conv(x)\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nFIX: Conv2d expects (batch, channels, height, width)\")\n",
        "    print(f\"Your input shape: {x.shape}\")\n",
        "    print(f\"Should be: (batch_size, 3, 32, 32)\")\n",
        "\n",
        "    # Correct way\n",
        "    x_correct = x.unsqueeze(0)  # Add batch dimension\n",
        "    print(f\"\\nFixed shape: {x_correct.shape}\")\n",
        "    out = conv(x_correct)\n",
        "    print(f\"Output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izi1sedU0sen",
        "outputId": "03b51088-fa26-4adf-ce9d-4ea11387e647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ERROR 3: Wrong flatten size for Linear layer\n",
            "----------------------------------------\n",
            "Error: mat1 and mat2 shapes cannot be multiplied (1x14400 and 1000x10)\n",
            "\n",
            "DEBUG INFO:\n",
            "Conv output shape: torch.Size([1, 16, 30, 30])\n",
            "Flattened size: 14400\n",
            "\n",
            "FIX: Change nn.Linear(1000, 10) to nn.Linear(14400, 10)\n"
          ]
        }
      ],
      "source": [
        "# Error 3: Flatten dimension mismatch with Linear\n",
        "print(\"\\nERROR 3: Wrong flatten size for Linear layer\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "class BrokenNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 16, 3)\n",
        "        self.fc = nn.Linear(1000, 10)  # Wrong size!\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        return self.fc(x)\n",
        "\n",
        "try:\n",
        "    broken = BrokenNet()\n",
        "    x = torch.randn(1, 3, 32, 32)\n",
        "    out = broken(x)\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "    # Debug: find the actual flatten size\n",
        "    conv_out = broken.conv(x)\n",
        "    flatten_size = conv_out.view(1, -1).size(1)\n",
        "    print(f\"\\nDEBUG INFO:\")\n",
        "    print(f\"Conv output shape: {conv_out.shape}\")\n",
        "    print(f\"Flattened size: {flatten_size}\")\n",
        "    print(f\"\\nFIX: Change nn.Linear(1000, 10) to nn.Linear({flatten_size}, 10)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGygaYz80sen"
      },
      "source": [
        "## 2.2 Shape Debugging Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtzpJ1pD0sen",
        "outputId": "5dcdbbfc-d6c8-412b-d153-9ae61e9c7047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer                Input Shape               Output Shape             \n",
            "======================================================================\n",
            "Conv2d               [2, 3, 32, 32]            [2, 16, 32, 32]          \n",
            "BatchNorm2d          [2, 16, 32, 32]           [2, 16, 32, 32]          \n",
            "ReLU                 [2, 16, 32, 32]           [2, 16, 32, 32]          \n",
            "MaxPool2d            [2, 16, 32, 32]           [2, 16, 16, 16]          \n",
            "Conv2d               [2, 16, 16, 16]           [2, 32, 16, 16]          \n",
            "ReLU                 [2, 32, 16, 16]           [2, 32, 16, 16]          \n",
            "MaxPool2d            [2, 32, 16, 16]           [2, 32, 8, 8]            \n",
            "Linear               [2, 2048]                 [2, 128]                 \n",
            "ReLU                 [2, 128]                  [2, 128]                 \n",
            "Dropout              [2, 128]                  [2, 128]                 \n",
            "Linear               [2, 128]                  [2, 10]                  \n"
          ]
        }
      ],
      "source": [
        "def shape_debugger(model, input_tensor):\n",
        "    \"\"\"\n",
        "    Track shape changes through each layer.\n",
        "    \"\"\"\n",
        "    shapes = []\n",
        "\n",
        "    def hook(module, input, output):\n",
        "        if isinstance(output, torch.Tensor):\n",
        "            shapes.append({\n",
        "                'layer': module.__class__.__name__,\n",
        "                'input_shape': list(input[0].shape) if isinstance(input, tuple) else list(input.shape),\n",
        "                'output_shape': list(output.shape)\n",
        "            })\n",
        "\n",
        "    hooks = []\n",
        "    for layer in model.modules():\n",
        "        if layer != model:\n",
        "            hooks.append(layer.register_forward_hook(hook))\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "    except Exception as e:\n",
        "        print(f\"Forward pass failed: {e}\")\n",
        "        print(\"\\nShapes before failure:\")\n",
        "\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    print(f\"{'Layer':20} {'Input Shape':25} {'Output Shape':25}\")\n",
        "    print(\"=\" * 70)\n",
        "    for s in shapes:\n",
        "        print(f\"{s['layer']:20} {str(s['input_shape']):25} {str(s['output_shape']):25}\")\n",
        "\n",
        "    return shapes\n",
        "\n",
        "# Use it\n",
        "model = SampleNet()\n",
        "x = torch.randn(2, 3, 32, 32)\n",
        "shapes = shape_debugger(model, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tOzGJPIj0sen"
      },
      "outputs": [],
      "source": [
        "# Quick shape check decorator\n",
        "def debug_shapes(func):\n",
        "    \"\"\"\n",
        "    Decorator to print input/output shapes of forward method.\n",
        "    \"\"\"\n",
        "    def wrapper(self, x, *args, **kwargs):\n",
        "        print(f\"Input shape: {x.shape}\")\n",
        "        output = func(self, x, *args, **kwargs)\n",
        "        print(f\"Output shape: {output.shape}\")\n",
        "        return output\n",
        "    return wrapper\n",
        "\n",
        "# Usage: Add @debug_shapes above forward method during debugging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9F3IunP0sen"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 3: Common PyTorch Errors & Fixes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzIYIaAo0sen",
        "outputId": "999f7de6-2c59-4958-cde5-9b811a65dfb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR 4: Device mismatch\n",
            "----------------------------------------\n",
            "CUDA not available - skipping this example\n",
            "\n",
            "But the fix is: always use model.to(device) and x.to(device)\n"
          ]
        }
      ],
      "source": [
        "# Error 4: Device mismatch\n",
        "print(\"ERROR 4: Device mismatch\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        model_gpu = nn.Linear(10, 5).cuda()\n",
        "        x_cpu = torch.randn(2, 10)  # On CPU!\n",
        "        out = model_gpu(x_cpu)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"\\nFIX: Ensure model and data are on the same device\")\n",
        "        print(\"x = x.to(device)  OR  x = x.cuda()\")\n",
        "else:\n",
        "    print(\"CUDA not available - skipping this example\")\n",
        "    print(\"\\nBut the fix is: always use model.to(device) and x.to(device)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYlZ7Rw10sen",
        "outputId": "84361b5b-f88b-4d47-e359-cfa241ce4803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ERROR 5: In-place operation on gradient-required tensor\n",
            "----------------------------------------\n",
            "Error: a leaf Variable that requires grad is being used in an in-place operation.\n",
            "\n",
            "FIX: Use out-of-place operations\n",
            "Instead of: x += 1\n",
            "Use: x = x + 1\n"
          ]
        }
      ],
      "source": [
        "# Error 5: In-place operation on leaf variable\n",
        "print(\"\\nERROR 5: In-place operation on gradient-required tensor\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    x = torch.randn(3, requires_grad=True)\n",
        "    x += 1  # In-place operation!\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nFIX: Use out-of-place operations\")\n",
        "    print(\"Instead of: x += 1\")\n",
        "    print(\"Use: x = x + 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-znqmT50seo",
        "outputId": "089dfbfb-bf96-4848-bcc6-fb26d637f7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ERROR 6: Gradient accumulation (forgetting zero_grad)\n",
            "----------------------------------------\n",
            "Iteration 0: grad mean = -0.3570\n",
            "Iteration 1: grad mean = -0.7141\n",
            "Iteration 2: grad mean = -1.0711\n",
            "\n",
            "Notice: gradients are ACCUMULATING, not being reset!\n",
            "FIX: Add optimizer.zero_grad() at the start of each iteration\n"
          ]
        }
      ],
      "source": [
        "# Error 6: Forgetting to call optimizer.zero_grad()\n",
        "print(\"\\nERROR 6: Gradient accumulation (forgetting zero_grad)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "model = nn.Linear(5, 2)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "x = torch.randn(3, 5)\n",
        "y = torch.randn(3, 2)\n",
        "\n",
        "# Wrong: no zero_grad()\n",
        "for i in range(3):\n",
        "    loss = criterion(model(x), y)\n",
        "    loss.backward()\n",
        "    if i == 0:\n",
        "        first_grad = model.weight.grad.clone()\n",
        "    print(f\"Iteration {i}: grad mean = {model.weight.grad.mean():.4f}\")\n",
        "\n",
        "print(\"\\nNotice: gradients are ACCUMULATING, not being reset!\")\n",
        "print(\"FIX: Add optimizer.zero_grad() at the start of each iteration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "visXuciI0seo",
        "outputId": "9de8258f-dc76-476e-d218-166c4c126cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ERROR 7: Forgetting model.eval() during inference\n",
            "----------------------------------------\n",
            "Training mode outputs: [-0.6422128081321716, -2.0309603214263916, -0.7500288486480713, -1.548919439315796, -0.830426812171936]\n",
            "Notice: outputs vary due to dropout!\n",
            "\n",
            "Eval mode outputs: [-1.3059141635894775, -1.3059141635894775, -1.3059141635894775, -1.3059141635894775, -1.3059141635894775]\n",
            "Notice: outputs are consistent!\n",
            "\n",
            "FIX: Always use model.eval() before inference and model.train() for training\n"
          ]
        }
      ],
      "source": [
        "# Error 7: Model not in eval mode during inference\n",
        "print(\"\\nERROR 7: Forgetting model.eval() during inference\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 20),\n",
        "    nn.Dropout(0.5),  # Behaves differently in train vs eval!\n",
        "    nn.Linear(20, 5)\n",
        ")\n",
        "\n",
        "x = torch.randn(1, 10)\n",
        "\n",
        "# In training mode\n",
        "model.train()\n",
        "outputs_train = [model(x).sum().item() for _ in range(5)]\n",
        "print(f\"Training mode outputs: {outputs_train}\")\n",
        "print(\"Notice: outputs vary due to dropout!\")\n",
        "\n",
        "# In eval mode\n",
        "model.eval()\n",
        "outputs_eval = [model(x).sum().item() for _ in range(5)]\n",
        "print(f\"\\nEval mode outputs: {outputs_eval}\")\n",
        "print(\"Notice: outputs are consistent!\")\n",
        "\n",
        "print(\"\\nFIX: Always use model.eval() before inference and model.train() for training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqSNKiAg0seo",
        "outputId": "df46a63a-d421-45b4-f086-d44e8d7dd4ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ERROR 8: Double softmax (common mistake!)\n",
            "----------------------------------------\n",
            "CrossEntropyLoss ALREADY includes Softmax internally!\n",
            "If you apply Softmax in your model, you're doing it twice.\n",
            "\n",
            "FIX: Return raw logits from the model\n",
            "Apply Softmax only during inference for probabilities\n"
          ]
        }
      ],
      "source": [
        "# Error 8: Softmax + CrossEntropyLoss\n",
        "print(\"\\nERROR 8: Double softmax (common mistake!)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# WRONG\n",
        "class WrongModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(10, 3)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return self.softmax(x)  # DON'T DO THIS with CrossEntropyLoss!\n",
        "\n",
        "print(\"CrossEntropyLoss ALREADY includes Softmax internally!\")\n",
        "print(\"If you apply Softmax in your model, you're doing it twice.\")\n",
        "print(\"\\nFIX: Return raw logits from the model\")\n",
        "print(\"Apply Softmax only during inference for probabilities\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuR-yYUL0seo",
        "outputId": "30987215-6ae5-4fd2-b825-41db20f81463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ERROR 9: Unexpected broadcasting\n",
            "----------------------------------------\n",
            "a.shape: torch.Size([3, 4])\n",
            "b.shape: torch.Size([4])\n",
            "(a + b).shape: torch.Size([3, 4])\n",
            "\n",
            "c.shape: torch.Size([3, 1])\n",
            "d.shape: torch.Size([1, 4])\n",
            "(c + d).shape: torch.Size([3, 4])  # Broadcasted to 3x4!\n",
            "\n",
            "FIX: Be explicit about shapes. Use .unsqueeze() or .expand() to control broadcasting.\n"
          ]
        }
      ],
      "source": [
        "# Error 9: Broadcasting confusion\n",
        "print(\"\\nERROR 9: Unexpected broadcasting\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "a = torch.randn(3, 4)\n",
        "b = torch.randn(4)  # Will broadcast!\n",
        "\n",
        "print(f\"a.shape: {a.shape}\")\n",
        "print(f\"b.shape: {b.shape}\")\n",
        "print(f\"(a + b).shape: {(a + b).shape}\")\n",
        "\n",
        "# This can cause silent bugs!\n",
        "c = torch.randn(3, 1)\n",
        "d = torch.randn(1, 4)\n",
        "print(f\"\\nc.shape: {c.shape}\")\n",
        "print(f\"d.shape: {d.shape}\")\n",
        "print(f\"(c + d).shape: {(c + d).shape}  # Broadcasted to 3x4!\")\n",
        "\n",
        "print(\"\\nFIX: Be explicit about shapes. Use .unsqueeze() or .expand() to control broadcasting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_hgSJfr0seo"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 4: Using Hooks for Debugging\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsrnjeTu0seo",
        "outputId": "c61a3240-4b47-467f-ed0a-02fe4bc0e6b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer                Shape                      Mean        Std   NaN?   Inf?\n",
            "================================================================================\n",
            "conv1                [2, 16, 32, 32]         -0.0246     0.5847  False  False\n",
            "bn1                  [2, 16, 32, 32]          0.0000     1.0000  False  False\n",
            "relu                 [2, 128]                 0.1400     0.1968  False  False\n",
            "pool                 [2, 32, 8, 8]            0.3886     0.4691  False  False\n",
            "conv2                [2, 32, 16, 16]         -0.1615     0.7085  False  False\n",
            "fc1                  [2, 128]                 0.0111     0.3366  False  False\n",
            "dropout              [2, 128]                 0.1287     0.2877  False  False\n",
            "fc2                  [2, 10]                  0.0044     0.1541  False  False\n"
          ]
        }
      ],
      "source": [
        "# Forward hooks: debug activations\n",
        "class ActivationDebugger:\n",
        "    def __init__(self, model):\n",
        "        self.activations = {}\n",
        "        self.hooks = []\n",
        "\n",
        "        for name, module in model.named_modules():\n",
        "            if name:  # Skip root\n",
        "                hook = module.register_forward_hook(self._get_hook(name))\n",
        "                self.hooks.append(hook)\n",
        "\n",
        "    def _get_hook(self, name):\n",
        "        def hook(module, input, output):\n",
        "            if isinstance(output, torch.Tensor):\n",
        "                self.activations[name] = {\n",
        "                    'shape': output.shape,\n",
        "                    'mean': output.mean().item(),\n",
        "                    'std': output.std().item(),\n",
        "                    'min': output.min().item(),\n",
        "                    'max': output.max().item(),\n",
        "                    'has_nan': torch.isnan(output).any().item(),\n",
        "                    'has_inf': torch.isinf(output).any().item()\n",
        "                }\n",
        "        return hook\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "\n",
        "    def print_report(self):\n",
        "        print(f\"{'Layer':20} {'Shape':20} {'Mean':>10} {'Std':>10} {'NaN?':>6} {'Inf?':>6}\")\n",
        "        print(\"=\" * 80)\n",
        "        for name, stats in self.activations.items():\n",
        "            print(f\"{name:20} {str(list(stats['shape'])):20} {stats['mean']:>10.4f} {stats['std']:>10.4f} {str(stats['has_nan']):>6} {str(stats['has_inf']):>6}\")\n",
        "\n",
        "# Usage\n",
        "model = SampleNet()\n",
        "debugger = ActivationDebugger(model)\n",
        "\n",
        "x = torch.randn(2, 3, 32, 32)\n",
        "with torch.no_grad():\n",
        "    out = model(x)\n",
        "\n",
        "debugger.print_report()\n",
        "debugger.remove_hooks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctInyr3K0seo",
        "outputId": "9199865d-7f3f-4a8e-b14e-c1ec952149b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gradient Flow Analysis:\n",
            "Layer                   Grad Mean     Grad Max   NaN?   Inf?\n",
            "============================================================\n",
            "fc2                      0.089077     0.447347  False  False\n",
            "dropout                  0.019701     0.047303  False  False\n",
            "relu                     0.000220     0.004372  False  False\n",
            "fc1                      0.008683     0.081869  False  False\n",
            "pool                     0.000878     0.004372  False  False\n",
            "conv2                    0.000504     0.011987  False  False\n",
            "bn1                      0.000205     0.004372  False  False\n",
            "conv1                    0.000385     0.008243  False  False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3409445208.py:42: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
            "  loss.backward()\n"
          ]
        }
      ],
      "source": [
        "# Backward hooks: debug gradients\n",
        "class GradientDebugger:\n",
        "    def __init__(self, model):\n",
        "        self.gradients = {}\n",
        "        self.hooks = []\n",
        "\n",
        "        for name, module in model.named_modules():\n",
        "            if name:\n",
        "                hook = module.register_full_backward_hook(self._get_hook(name))\n",
        "                self.hooks.append(hook)\n",
        "\n",
        "    def _get_hook(self, name):\n",
        "        def hook(module, grad_input, grad_output):\n",
        "            if grad_output[0] is not None:\n",
        "                grad = grad_output[0]\n",
        "                self.gradients[name] = {\n",
        "                    'shape': grad.shape,\n",
        "                    'mean': grad.abs().mean().item(),\n",
        "                    'max': grad.abs().max().item(),\n",
        "                    'has_nan': torch.isnan(grad).any().item(),\n",
        "                    'has_inf': torch.isinf(grad).any().item()\n",
        "                }\n",
        "        return hook\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "\n",
        "    def print_report(self):\n",
        "        print(f\"{'Layer':20} {'Grad Mean':>12} {'Grad Max':>12} {'NaN?':>6} {'Inf?':>6}\")\n",
        "        print(\"=\" * 60)\n",
        "        for name, stats in self.gradients.items():\n",
        "            print(f\"{name:20} {stats['mean']:>12.6f} {stats['max']:>12.6f} {str(stats['has_nan']):>6} {str(stats['has_inf']):>6}\")\n",
        "\n",
        "# Usage\n",
        "model = SampleNet()\n",
        "grad_debugger = GradientDebugger(model)\n",
        "\n",
        "x = torch.randn(2, 3, 32, 32)\n",
        "y = torch.randint(0, 10, (2,))\n",
        "loss = nn.CrossEntropyLoss()(model(x), y)\n",
        "loss.backward()\n",
        "\n",
        "print(\"\\nGradient Flow Analysis:\")\n",
        "grad_debugger.print_report()\n",
        "grad_debugger.remove_hooks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF1nAZlH0seo"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 5: Memory Debugging\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuBFDxmp0seo",
        "outputId": "fe0826ef-8718-4629-f037-d27d97e5156a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensors in memory: 43\n",
            "CUDA not available\n"
          ]
        }
      ],
      "source": [
        "def memory_stats():\n",
        "    \"\"\"Print current GPU memory usage.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
        "        print(f\"GPU Memory: Allocated={allocated:.1f}MB, Reserved={reserved:.1f}MB\")\n",
        "    else:\n",
        "        print(\"CUDA not available\")\n",
        "\n",
        "def count_tensors():\n",
        "    \"\"\"Count all tensors in memory.\"\"\"\n",
        "    import gc\n",
        "    tensors = []\n",
        "    for obj in gc.get_objects():\n",
        "        try:\n",
        "            if torch.is_tensor(obj):\n",
        "                tensors.append((type(obj), obj.size(), obj.device))\n",
        "        except:\n",
        "            pass\n",
        "    return len(tensors)\n",
        "\n",
        "print(f\"Tensors in memory: {count_tensors()}\")\n",
        "memory_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rTbuTGs0seo",
        "outputId": "a901717c-6f0b-4657-9b4e-11a74739d253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common Memory Leak Patterns:\n",
            "\n",
            "1. Storing tensors in a list during training loop:\n",
            "   BAD: losses.append(loss)\n",
            "   GOOD: losses.append(loss.item())\n",
            "\n",
            "2. Not using torch.no_grad() during inference:\n",
            "   BAD: output = model(x)\n",
            "   GOOD: with torch.no_grad(): output = model(x)\n",
            "\n",
            "3. Not detaching tensors before numpy conversion:\n",
            "   BAD: x.numpy()\n",
            "   GOOD: x.detach().cpu().numpy()\n",
            "\n",
            "4. Keeping references to intermediate activations:\n",
            "   Use del to explicitly remove\n"
          ]
        }
      ],
      "source": [
        "# Common memory leaks\n",
        "print(\"Common Memory Leak Patterns:\\n\")\n",
        "\n",
        "print(\"1. Storing tensors in a list during training loop:\")\n",
        "print(\"   BAD: losses.append(loss)\")\n",
        "print(\"   GOOD: losses.append(loss.item())\\n\")\n",
        "\n",
        "print(\"2. Not using torch.no_grad() during inference:\")\n",
        "print(\"   BAD: output = model(x)\")\n",
        "print(\"   GOOD: with torch.no_grad(): output = model(x)\\n\")\n",
        "\n",
        "print(\"3. Not detaching tensors before numpy conversion:\")\n",
        "print(\"   BAD: x.numpy()\")\n",
        "print(\"   GOOD: x.detach().cpu().numpy()\\n\")\n",
        "\n",
        "print(\"4. Keeping references to intermediate activations:\")\n",
        "print(\"   Use del to explicitly remove\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJwyoj9L0sep",
        "outputId": "38c68dce-f65a-43df-c58d-989f14b7b7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer                        Activation Memory (MB)\n",
            "=======================================================\n",
            "Conv2d                                       1.0000\n",
            "BatchNorm2d                                  1.0000\n",
            "ReLU                                         1.0000\n",
            "MaxPool2d                                    0.2500\n",
            "Conv2d                                       0.5000\n",
            "ReLU                                         0.5000\n",
            "MaxPool2d                                    0.1250\n",
            "Linear                                       0.0078\n",
            "ReLU                                         0.0078\n",
            "Dropout                                      0.0078\n",
            "Linear                                       0.0006\n",
            "=======================================================\n",
            "TOTAL                                        4.3990\n"
          ]
        }
      ],
      "source": [
        "# Memory profiling per layer\n",
        "def profile_memory(model, input_shape, device='cpu'):\n",
        "    \"\"\"\n",
        "    Profile memory usage per layer.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    x = torch.randn(*input_shape, device=device)\n",
        "\n",
        "    memory_usage = []\n",
        "\n",
        "    def hook(module, input, output):\n",
        "        # Estimate memory of output tensor\n",
        "        if isinstance(output, torch.Tensor):\n",
        "            # 4 bytes per float32\n",
        "            mem = output.numel() * 4 / 1024**2  # MB\n",
        "            memory_usage.append((module.__class__.__name__, mem))\n",
        "\n",
        "    hooks = []\n",
        "    for module in model.modules():\n",
        "        if module != model:\n",
        "            hooks.append(module.register_forward_hook(hook))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    print(f\"{'Layer':25} {'Activation Memory (MB)':>25}\")\n",
        "    print(\"=\" * 55)\n",
        "    total = 0\n",
        "    for name, mem in memory_usage:\n",
        "        print(f\"{name:25} {mem:>25.4f}\")\n",
        "        total += mem\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"{'TOTAL':25} {total:>25.4f}\")\n",
        "\n",
        "model = SampleNet()\n",
        "profile_memory(model, (16, 3, 32, 32))  # Batch of 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0kZpdyj0sep"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 6: Data Pipeline Debugging\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-ibtomi0sep",
        "outputId": "623398c2-188a-4c0d-a416-153331a201ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoader Debug Info:\n",
            "==================================================\n",
            "Dataset size: 100\n",
            "Number of batches: 7\n",
            "Batch size: 16\n",
            "\n",
            "First batch:\n",
            "  X shape: torch.Size([16, 3, 32, 32])\n",
            "  y shape: torch.Size([16])\n",
            "  X dtype: torch.float32\n",
            "  y dtype: torch.int64\n",
            "  X range: [-4.26, 4.21]\n",
            "  y values: [0, 1, 2, 3, 4, 5, 6, 8, 9]\n"
          ]
        }
      ],
      "source": [
        "# Debugging DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DebuggableDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "# Create sample data\n",
        "X = torch.randn(100, 3, 32, 32)\n",
        "y = torch.randint(0, 10, (100,))\n",
        "\n",
        "dataset = DebuggableDataset(X, y)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Debug first batch\n",
        "print(\"DataLoader Debug Info:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Number of batches: {len(loader)}\")\n",
        "print(f\"Batch size: {loader.batch_size}\")\n",
        "\n",
        "# Inspect first batch\n",
        "batch_x, batch_y = next(iter(loader))\n",
        "print(f\"\\nFirst batch:\")\n",
        "print(f\"  X shape: {batch_x.shape}\")\n",
        "print(f\"  y shape: {batch_y.shape}\")\n",
        "print(f\"  X dtype: {batch_x.dtype}\")\n",
        "print(f\"  y dtype: {batch_y.dtype}\")\n",
        "print(f\"  X range: [{batch_x.min():.2f}, {batch_x.max():.2f}]\")\n",
        "print(f\"  y values: {batch_y.unique().tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vikovh-k0sep",
        "outputId": "baef6d0d-0bf2-4d5a-a3fd-42a003cdbaa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common Data Pipeline Issues:\n",
            "\n",
            "1. Data type mismatch:\n",
            "   FIX: Ensure tensors are float32 for inputs, long for class labels\n",
            "   x = x.float(), y = y.long()\n",
            "\n",
            "2. Data not normalized:\n",
            "   FIX: x = (x - mean) / std\n",
            "   Or use transforms.Normalize()\n",
            "\n",
            "3. Wrong label format:\n",
            "   CrossEntropyLoss expects class indices (0, 1, 2, ...), not one-hot\n",
            "   BCELoss expects float targets\n",
            "\n",
            "4. Image channels in wrong order:\n",
            "   PyTorch expects (C, H, W), not (H, W, C)\n",
            "   FIX: x = x.permute(2, 0, 1) or np.transpose(x, (2, 0, 1))\n"
          ]
        }
      ],
      "source": [
        "# Common data pipeline issues\n",
        "print(\"Common Data Pipeline Issues:\\n\")\n",
        "\n",
        "print(\"1. Data type mismatch:\")\n",
        "print(\"   FIX: Ensure tensors are float32 for inputs, long for class labels\")\n",
        "print(\"   x = x.float(), y = y.long()\\n\")\n",
        "\n",
        "print(\"2. Data not normalized:\")\n",
        "print(\"   FIX: x = (x - mean) / std\")\n",
        "print(\"   Or use transforms.Normalize()\\n\")\n",
        "\n",
        "print(\"3. Wrong label format:\")\n",
        "print(\"   CrossEntropyLoss expects class indices (0, 1, 2, ...), not one-hot\")\n",
        "print(\"   BCELoss expects float targets\\n\")\n",
        "\n",
        "print(\"4. Image channels in wrong order:\")\n",
        "print(\"   PyTorch expects (C, H, W), not (H, W, C)\")\n",
        "print(\"   FIX: x = x.permute(2, 0, 1) or np.transpose(x, (2, 0, 1))\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WDy-MzJ0sep"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 7: Training Loop Debugging\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFJB0TxA0sep",
        "outputId": "e1fa140a-73d3-43e5-c1b4-4c14977a0916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TRAINING STEP DEBUG\n",
            "============================================================\n",
            "\n",
            "1. INPUT CHECK\n",
            "   x.shape: torch.Size([4, 10]), dtype: torch.float32, device: cpu\n",
            "   y.shape: torch.Size([4]), dtype: torch.int64, device: cpu\n",
            "   x range: [-1.76, 2.57]\n",
            "\n",
            "2. FORWARD PASS\n",
            "   output.shape: torch.Size([4, 5])\n",
            "   output range: [-0.2530, 0.4381]\n",
            "   output has NaN: False\n",
            "\n",
            "3. LOSS COMPUTATION\n",
            "   loss value: 1.590008\n",
            "   loss is NaN: False\n",
            "   loss is Inf: False\n",
            "\n",
            "4. BACKWARD PASS\n",
            "   Top 3 gradient norms:\n",
            "      2.weight: 1.116569\n",
            "      0.weight: 0.578876\n",
            "      2.bias: 0.428131\n",
            "\n",
            "5. OPTIMIZER STEP\n",
            "   Step completed successfully\n"
          ]
        }
      ],
      "source": [
        "def debug_training_step(model, criterion, optimizer, x, y):\n",
        "    \"\"\"\n",
        "    A single training step with full debugging.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TRAINING STEP DEBUG\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 1. Input check\n",
        "    print(f\"\\n1. INPUT CHECK\")\n",
        "    print(f\"   x.shape: {x.shape}, dtype: {x.dtype}, device: {x.device}\")\n",
        "    print(f\"   y.shape: {y.shape}, dtype: {y.dtype}, device: {y.device}\")\n",
        "    print(f\"   x range: [{x.min():.2f}, {x.max():.2f}]\")\n",
        "\n",
        "    # 2. Forward pass\n",
        "    print(f\"\\n2. FORWARD PASS\")\n",
        "    output = model(x)\n",
        "    print(f\"   output.shape: {output.shape}\")\n",
        "    print(f\"   output range: [{output.min():.4f}, {output.max():.4f}]\")\n",
        "    print(f\"   output has NaN: {torch.isnan(output).any().item()}\")\n",
        "\n",
        "    # 3. Loss computation\n",
        "    print(f\"\\n3. LOSS COMPUTATION\")\n",
        "    loss = criterion(output, y)\n",
        "    print(f\"   loss value: {loss.item():.6f}\")\n",
        "    print(f\"   loss is NaN: {torch.isnan(loss).item()}\")\n",
        "    print(f\"   loss is Inf: {torch.isinf(loss).item()}\")\n",
        "\n",
        "    # 4. Backward pass\n",
        "    print(f\"\\n4. BACKWARD PASS\")\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Check gradients\n",
        "    grad_norms = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_norm = param.grad.norm().item()\n",
        "            grad_norms.append((name, grad_norm))\n",
        "            if torch.isnan(param.grad).any():\n",
        "                print(f\"   WARNING: NaN gradient in {name}\")\n",
        "\n",
        "    # Print largest gradients\n",
        "    grad_norms.sort(key=lambda x: x[1], reverse=True)\n",
        "    print(f\"   Top 3 gradient norms:\")\n",
        "    for name, norm in grad_norms[:3]:\n",
        "        print(f\"      {name}: {norm:.6f}\")\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    print(f\"\\n5. OPTIMIZER STEP\")\n",
        "    optimizer.step()\n",
        "    print(f\"   Step completed successfully\")\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# Test it\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 5)\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "x = torch.randn(4, 10)\n",
        "y = torch.randint(0, 5, (4,))\n",
        "\n",
        "loss = debug_training_step(model, criterion, optimizer, x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsmkprgc0sep"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 8: Debugging Checklist\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1axFaaZ70seq",
        "outputId": "fc9dced9-dc7b-41f5-a377-67e84db7362e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " RUNNING DEBUG CHECKLIST...\n",
            "\n",
            "1. Model Checks:\n",
            "    Total params: 55, Trainable: 55\n",
            "\n",
            "2. Data Checks:\n",
            "    Input shape: torch.Size([4, 10])\n",
            "    Label shape: torch.Size([4])\n",
            "    No NaN in input\n",
            "\n",
            "3. Forward Pass:\n",
            "    Output shape: torch.Size([4, 5])\n",
            "    No NaN in output\n",
            "\n",
            "4. Loss Check:\n",
            "    Loss value: 1.437672\n",
            "\n",
            "5. Backward Pass:\n",
            "    Backward pass successful\n",
            "    All gradients look healthy\n",
            "\n",
            "==================================================\n",
            " ALL CHECKS PASSED!\n"
          ]
        }
      ],
      "source": [
        "def run_debug_checklist(model, x, y, criterion):\n",
        "    \"\"\"\n",
        "    Run a comprehensive debug checklist.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "\n",
        "    print(\" RUNNING DEBUG CHECKLIST...\\n\")\n",
        "\n",
        "    # 1. Model checks\n",
        "    print(\"1. Model Checks:\")\n",
        "\n",
        "    # Check if model has parameters\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"    Total params: {num_params:,}, Trainable: {trainable:,}\")\n",
        "\n",
        "    if trainable == 0:\n",
        "        issues.append(\" No trainable parameters!\")\n",
        "\n",
        "    # 2. Data checks\n",
        "    print(\"\\n2. Data Checks:\")\n",
        "    print(f\"    Input shape: {x.shape}\")\n",
        "    print(f\"    Label shape: {y.shape}\")\n",
        "\n",
        "    if torch.isnan(x).any():\n",
        "        issues.append(\" NaN in input data!\")\n",
        "    else:\n",
        "        print(\"    No NaN in input\")\n",
        "\n",
        "    # 3. Forward pass\n",
        "    print(\"\\n3. Forward Pass:\")\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            out = model(x)\n",
        "        print(f\"    Output shape: {out.shape}\")\n",
        "\n",
        "        if torch.isnan(out).any():\n",
        "            issues.append(\" NaN in model output!\")\n",
        "        else:\n",
        "            print(\"    No NaN in output\")\n",
        "    except Exception as e:\n",
        "        issues.append(f\" Forward pass failed: {e}\")\n",
        "\n",
        "    # 4. Loss computation\n",
        "    print(\"\\n4. Loss Check:\")\n",
        "    try:\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        print(f\"    Loss value: {loss.item():.6f}\")\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            issues.append(\" Loss is NaN!\")\n",
        "        if torch.isinf(loss):\n",
        "            issues.append(\" Loss is Inf!\")\n",
        "    except Exception as e:\n",
        "        issues.append(f\" Loss computation failed: {e}\")\n",
        "\n",
        "    # 5. Backward pass\n",
        "    print(\"\\n5. Backward Pass:\")\n",
        "    try:\n",
        "        loss.backward()\n",
        "        print(\"    Backward pass successful\")\n",
        "\n",
        "        # Check for vanishing gradients\n",
        "        small_grads = 0\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                if param.grad.abs().mean() < 1e-7:\n",
        "                    small_grads += 1\n",
        "\n",
        "        if small_grads > 0:\n",
        "            issues.append(f\" {small_grads} layers have very small gradients (vanishing?)\")\n",
        "        else:\n",
        "            print(\"    All gradients look healthy\")\n",
        "    except Exception as e:\n",
        "        issues.append(f\" Backward pass failed: {e}\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    if issues:\n",
        "        print(\" ISSUES FOUND:\")\n",
        "        for issue in issues:\n",
        "            print(f\"   {issue}\")\n",
        "    else:\n",
        "        print(\" ALL CHECKS PASSED!\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "# Test it\n",
        "model = nn.Linear(10, 5)\n",
        "x = torch.randn(4, 10)\n",
        "y = torch.randint(0, 5, (4,))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "issues = run_debug_checklist(model, x, y, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgzPB87n0seq"
      },
      "source": [
        "---\n",
        "\n",
        "# Summary: PyTorch Debugging Toolkit\n",
        "\n",
        "---\n",
        "\n",
        "| Issue | Debugging Tool |\n",
        "|-------|---------------|\n",
        "| **Model architecture** | `print(model)`, `model.named_modules()` |\n",
        "| **Shape problems** | Shape debugging hooks, systematic tracing |\n",
        "| **Gradient issues** | `tensor.grad`, backward hooks |\n",
        "| **NaN/Inf values** | `torch.isnan()`, `torch.isinf()` |\n",
        "| **Memory leaks** | `torch.cuda.memory_allocated()` |\n",
        "| **Device errors** | Check `.device` attribute |\n",
        "| **Data pipeline** | Inspect first batch manually |\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Reference Functions\n",
        "\n",
        "```python\n",
        "# Print model summary\n",
        "print(model)\n",
        "\n",
        "# Count parameters\n",
        "sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# Check for NaN\n",
        "torch.isnan(tensor).any()\n",
        "\n",
        "# Get tensor device\n",
        "tensor.device\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Debug mode for autograd\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSAFCaKC0seq"
      },
      "source": [
        "---\n",
        "\n",
        "# Interview Tips\n",
        "\n",
        "---\n",
        "\n",
        "**Q: How would you debug a model that produces NaN loss?**\n",
        "\n",
        "A:\n",
        "1. Check input data for NaN/Inf\n",
        "2. Use `torch.autograd.set_detect_anomaly(True)` to find where NaN appears\n",
        "3. Check for numerical instability (log of zero, division by zero)\n",
        "4. Lower learning rate\n",
        "5. Add gradient clipping\n",
        "\n",
        "**Q: Model trains but accuracy stays at random chance. What's wrong?**\n",
        "\n",
        "A:\n",
        "1. Verify data-label alignment (labels match correct data)\n",
        "2. Check loss function matches task (CrossEntropy for classification)\n",
        "3. Check if gradients are flowing (no vanishing)\n",
        "4. Verify model is in train mode\n",
        "5. Check data normalization\n",
        "\n",
        "**Q: How do you find which layer is causing issues?**\n",
        "\n",
        "A:\n",
        "1. Register forward/backward hooks on all layers\n",
        "2. Track activation statistics (mean, std, NaN count)\n",
        "3. Visualize gradient magnitudes per layer\n",
        "4. Progressively test smaller parts of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FStqF7WI0ser"
      },
      "source": [
        "---\n",
        "\n",
        "## Back to: [README](../README.md)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}