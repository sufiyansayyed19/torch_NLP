{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq6gte8v9POQ"
      },
      "source": [
        "# Module 00: The Complete NLP Pipeline\n",
        "\n",
        "**A Bird's-Eye View of How NLP Projects Work From Problem to Production**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prsoEZsu9POS"
      },
      "source": [
        "## 1. Objectives\n",
        "\n",
        "By the end of this module, you will:\n",
        "\n",
        "- âœ… Understand all major NLP problem types and their use cases\n",
        "- âœ… Know the complete NLP project lifecycle from problem definition to deployment\n",
        "- âœ… Be able to choose the right model for your data size, latency, and budget constraints\n",
        "- âœ… Understand the 2024+ NLP landscape including LLMs and RAG\n",
        "- âœ… Know the production NLP stack used in industry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2NFKfy29POT"
      },
      "source": [
        "## 2. Prerequisites\n",
        "\n",
        "This is the **first module** - no prerequisites needed!\n",
        "\n",
        "However, you should have:\n",
        "- Basic Python knowledge\n",
        "- Familiarity with machine learning concepts (training, testing, overfitting)\n",
        "- PyTorch fundamentals (tensors, nn.Module) - see our PyTorch Fundamentals repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQSGDB5Q9POT"
      },
      "source": [
        "## 3. Intuition & Motivation\n",
        "\n",
        "### Why This Module Matters\n",
        "\n",
        "Before diving into specific NLP techniques, you need to understand:\n",
        "1. **What types of problems exist** in NLP\n",
        "2. **How to choose** the right approach for your problem\n",
        "3. **What the modern landscape** looks like (it's changed dramatically!)\n",
        "\n",
        "### The NLP Revolution Timeline\n",
        "\n",
        "```\n",
        "1950s-2000s: Rule-based systems, statistical methods\n",
        "2013: Word2Vec - words as vectors\n",
        "2014: Seq2Seq - neural machine translation\n",
        "2015: Attention mechanism\n",
        "2017: Transformer (\"Attention Is All You Need\")\n",
        "2018: BERT, GPT - pretrained language models\n",
        "2020: GPT-3 - in-context learning, few-shot\n",
        "2022: ChatGPT - instruction following\n",
        "2023-2024: GPT-4, Claude, open-source LLMs (LLaMA, Mistral)\n",
        "```\n",
        "\n",
        "### The Key Insight\n",
        "\n",
        "> **\"Most NLP projects now start with: Can GPT-4/Claude solve this?\"**\n",
        "\n",
        "This doesn't mean traditional techniques are dead - understanding them is crucial for:\n",
        "- Building intuition for how modern models work\n",
        "- Handling edge cases where LLMs fail\n",
        "- Cost optimization (LLM APIs are expensive at scale)\n",
        "- Privacy-sensitive applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMT1oBT39POU"
      },
      "source": [
        "## 4. Understanding NLP Problem Types\n",
        "\n",
        "### 4.1 Text Classification\n",
        "\n",
        "**Task**: Assign one or more labels to a piece of text\n",
        "\n",
        "| Sub-type | Example | Industry Use |\n",
        "|----------|---------|-------------|\n",
        "| Sentiment Analysis | \"I love this!\" â†’ Positive | Customer feedback |\n",
        "| Spam Detection | Email â†’ Spam/Not Spam | Email filtering |\n",
        "| Intent Classification | \"Book a flight\" â†’ BookFlight | Chatbots |\n",
        "| Topic Classification | News article â†’ Sports/Politics | Content organization |\n",
        "| Toxicity Detection | Comment â†’ Toxic/Safe | Content moderation |\n",
        "\n",
        "**Difficulty**: â­â­ (Easiest NLP task)\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2 Sequence Labeling / Token Classification\n",
        "\n",
        "**Task**: Assign a label to each token in the sequence\n",
        "\n",
        "| Sub-type | Example | Industry Use |\n",
        "|----------|---------|-------------|\n",
        "| Named Entity Recognition (NER) | \"John works at Google\" â†’ [PER, O, O, ORG] | Information extraction |\n",
        "| Part-of-Speech Tagging | \"The cat sat\" â†’ [DET, NOUN, VERB] | Grammar checking |\n",
        "| Chunking | Noun phrases, verb phrases | Parsing |\n",
        "\n",
        "**Difficulty**: â­â­â­\n",
        "\n",
        "---\n",
        "\n",
        "### 4.3 Sequence-to-Sequence (Seq2Seq)\n",
        "\n",
        "**Task**: Transform one sequence into another\n",
        "\n",
        "| Sub-type | Example | Industry Use |\n",
        "|----------|---------|-------------|\n",
        "| Machine Translation | \"Hello\" â†’ \"Bonjour\" | Google Translate |\n",
        "| Summarization | Long article â†’ Short summary | News apps |\n",
        "| Paraphrasing | Sentence â†’ Same meaning, different words | Writing assistants |\n",
        "| Grammar Correction | \"I goes home\" â†’ \"I go home\" | Grammarly |\n",
        "\n",
        "**Difficulty**: â­â­â­â­\n",
        "\n",
        "---\n",
        "\n",
        "### 4.4 Text Generation / Language Modeling\n",
        "\n",
        "**Task**: Generate coherent text given a prompt\n",
        "\n",
        "| Sub-type | Example | Industry Use |\n",
        "|----------|---------|-------------|\n",
        "| Open-ended Generation | Prompt â†’ Story/Article | ChatGPT |\n",
        "| Dialogue | User message â†’ Response | Chatbots |\n",
        "| Code Generation | Description â†’ Code | GitHub Copilot |\n",
        "| Creative Writing | Prompt â†’ Poem/Story | AI writing tools |\n",
        "\n",
        "**Difficulty**: â­â­â­â­â­ (Most complex)\n",
        "\n",
        "---\n",
        "\n",
        "### 4.5 Question Answering (QA)\n",
        "\n",
        "**Task**: Answer questions based on context or knowledge\n",
        "\n",
        "| Sub-type | Example | Industry Use |\n",
        "|----------|---------|-------------|\n",
        "| Extractive QA | Context + Question â†’ Span from context | Search engines |\n",
        "| Generative QA | Question â†’ Generated answer | AI assistants |\n",
        "| Open-domain QA | Question â†’ Answer (no context given) | Virtual assistants |\n",
        "\n",
        "**Difficulty**: â­â­â­â­\n",
        "\n",
        "---\n",
        "\n",
        "### 4.6 Information Extraction\n",
        "\n",
        "**Task**: Extract structured information from unstructured text\n",
        "\n",
        "| Sub-type | Example | Industry Use |\n",
        "|----------|---------|-------------|\n",
        "| Relation Extraction | \"Steve Jobs founded Apple\" â†’ (Steve Jobs, founded, Apple) | Knowledge graphs |\n",
        "| Event Extraction | News â†’ Who did what, when, where | News aggregation |\n",
        "| Slot Filling | \"Book flight to Paris\" â†’ {destination: Paris} | Chatbots |\n",
        "\n",
        "**Difficulty**: â­â­â­â­\n",
        "\n",
        "---\n",
        "\n",
        "### 4.7 Semantic Similarity & Search\n",
        "\n",
        "**Task**: Measure how similar two pieces of text are\n",
        "\n",
        "| Sub-type | Example | Industry Use |\n",
        "|----------|---------|-------------|\n",
        "| Semantic Similarity | \"Happy\" vs \"Joyful\" â†’ 0.95 | Duplicate detection |\n",
        "| Semantic Search | Query â†’ Relevant documents | Search engines |\n",
        "| Paraphrase Detection | Are these two sentences the same? | Plagiarism detection |\n",
        "\n",
        "**Difficulty**: â­â­â­"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bagbo1WW9POU"
      },
      "source": [
        "## 5. The NLP Project Lifecycle\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                           NLP PROJECT LIFECYCLE                              â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                             â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
        "â”‚  â”‚   Problem    â”‚â”€â”€â”€â–¶â”‚     Data     â”‚â”€â”€â”€â–¶â”‚     EDA      â”‚                   â”‚\n",
        "â”‚  â”‚  Definition  â”‚    â”‚  Collection  â”‚    â”‚  & Analysis  â”‚                   â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
        "â”‚         â”‚                                        â”‚                          â”‚\n",
        "â”‚         â”‚                                        â–¼                          â”‚\n",
        "â”‚         â”‚                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
        "â”‚         â”‚                                â”‚ Preprocessingâ”‚                   â”‚\n",
        "â”‚         â”‚                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
        "â”‚         â”‚                                        â”‚                          â”‚\n",
        "â”‚         â”‚                                        â–¼                          â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
        "â”‚  â”‚  Deployment  â”‚â—€â”€â”€â”€â”‚  Evaluation  â”‚â—€â”€â”€â”€â”‚   Training   â”‚                   â”‚\n",
        "â”‚  â”‚  & Serving   â”‚    â”‚ & Error Anal â”‚    â”‚              â”‚                   â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
        "â”‚         â”‚                   â”‚                                               â”‚\n",
        "â”‚         â”‚                   â”‚ (iterate)                                     â”‚\n",
        "â”‚         â–¼                   â”‚                                               â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                                               â”‚\n",
        "â”‚  â”‚  Monitoring  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚\n",
        "â”‚  â”‚  & Updates   â”‚                                                           â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                           â”‚\n",
        "â”‚                                                                             â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Stage 1: Problem Definition\n",
        "- What exactly are you trying to solve?\n",
        "- What's the success metric?\n",
        "- What's the latency requirement?\n",
        "- What's the accuracy requirement?\n",
        "\n",
        "### Stage 2: Data Collection\n",
        "- Existing datasets (HuggingFace, Kaggle)\n",
        "- Web scraping\n",
        "- Manual labeling\n",
        "- Synthetic data generation (LLM-based)\n",
        "\n",
        "### Stage 3: EDA & Analysis\n",
        "- Class distribution\n",
        "- Text length statistics\n",
        "- Vocabulary analysis\n",
        "- Data quality issues\n",
        "\n",
        "### Stage 4: Preprocessing\n",
        "- Cleaning (HTML, special chars, etc.)\n",
        "- Tokenization\n",
        "- Normalization\n",
        "\n",
        "### Stage 5: Model Selection & Training\n",
        "- Baseline first!\n",
        "- Progressive complexity\n",
        "- Hyperparameter tuning\n",
        "\n",
        "### Stage 6: Evaluation & Error Analysis\n",
        "- Beyond accuracy: look at failure cases\n",
        "- Slice-based analysis\n",
        "- Human evaluation for generation\n",
        "\n",
        "### Stage 7: Deployment\n",
        "- API design\n",
        "- Latency optimization\n",
        "- Scaling\n",
        "\n",
        "### Stage 8: Monitoring\n",
        "- Performance tracking\n",
        "- Data drift detection\n",
        "- User feedback collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roROESuv9POV"
      },
      "source": [
        "## 6. Decision Framework: What Model to Use?\n",
        "\n",
        "This is the **most important section** of this module. Use this decision tree for any NLP project:\n",
        "\n",
        "```\n",
        "START YOUR NLP PROJECT\n",
        "â”‚\n",
        "â”œâ”€â”€ Q1: Do you have LABELED data?\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ NO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   â”‚   â”‚                                                                    â”‚\n",
        "â”‚   â”‚   â”œâ”€â”€ Can you use Zero-Shot? â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ LLM API (GPT-4, Claude)       â”‚\n",
        "â”‚   â”‚   â”‚                                                                    â”‚\n",
        "â”‚   â”‚   â”œâ”€â”€ Can you provide few examples? â”€â”€â–¶ Few-Shot Prompting            â”‚\n",
        "â”‚   â”‚   â”‚                                                                    â”‚\n",
        "â”‚   â”‚   â”œâ”€â”€ Can you generate labels? â”€â”€â”€â”€â”€â”€â”€â–¶ Self-Training with LLM        â”‚\n",
        "â”‚   â”‚   â”‚                                                                    â”‚\n",
        "â”‚   â”‚   â””â”€â”€ Need to label manually? â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Active Learning               â”‚\n",
        "â”‚   â”‚                                                                        â”‚\n",
        "â”‚   â””â”€â”€ YES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚       â”‚                                                                    â”‚\n",
        "â”‚       â””â”€â”€ Q2: How much data do you have?                                   â”‚\n",
        "â”‚           â”‚                                                                â”‚\n",
        "â”‚           â”œâ”€â”€ < 1,000 samples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ TF-IDF + Traditional ML       â”‚\n",
        "â”‚           â”‚                                 or Few-Shot LLM                â”‚\n",
        "â”‚           â”‚                                                                â”‚\n",
        "â”‚           â”œâ”€â”€ 1,000 - 10,000 samples â”€â”€â”€â”€â”€â–¶ Fine-tune DistilBERT/         â”‚\n",
        "â”‚           â”‚                                 Small Transformer              â”‚\n",
        "â”‚           â”‚                                                                â”‚\n",
        "â”‚           â””â”€â”€ > 10,000 samples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Fine-tune BERT/RoBERTa        â”‚\n",
        "â”‚                                             or train custom model          â”‚\n",
        "â”‚\n",
        "â”œâ”€â”€ Q3: What's your LATENCY budget?\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ < 10ms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Rule-based / TF-IDF /         â”‚\n",
        "â”‚   â”‚                                         Quantized tiny model           â”‚\n",
        "â”‚   â”‚                                                                        â”‚\n",
        "â”‚   â”œâ”€â”€ 10-100ms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ DistilBERT / ONNX optimized   â”‚\n",
        "â”‚   â”‚                                                                        â”‚\n",
        "â”‚   â””â”€â”€ > 100ms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Full BERT / LLM API           â”‚\n",
        "â”‚\n",
        "â”œâ”€â”€ Q4: What's your ACCURACY requirement?\n",
        "â”‚   â”‚\n",
        "â”‚   â”œâ”€â”€ Must be explainable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Logistic Regression + TF-IDF  â”‚\n",
        "â”‚   â”‚                                                                        â”‚\n",
        "â”‚   â”œâ”€â”€ Best possible â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ GPT-4 / Claude API            â”‚\n",
        "â”‚   â”‚                                         or ensemble                    â”‚\n",
        "â”‚   â”‚                                                                        â”‚\n",
        "â”‚   â””â”€â”€ \"Good enough\" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Fine-tuned BERT               â”‚\n",
        "â”‚\n",
        "â””â”€â”€ Q5: What's your BUDGET?\n",
        "    â”‚\n",
        "    â”œâ”€â”€ $0 (open source only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ LLaMA, Mistral (self-hosted)  â”‚\n",
        "    â”‚                                                                        â”‚\n",
        "    â”œâ”€â”€ Low budget â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Fine-tune once,               â”‚\n",
        "    â”‚                                         inference on CPU               â”‚\n",
        "    â”‚                                                                        â”‚\n",
        "    â””â”€â”€ High budget â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ LLM APIs (pay per token)      â”‚\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGZeZ1AM9POW"
      },
      "source": [
        "## 7. ğŸ”¥ Real-World Usage: The 2024+ NLP Landscape\n",
        "\n",
        "### The Paradigm Shift\n",
        "\n",
        "**Before 2022:**\n",
        "```\n",
        "Problem â†’ Collect Data â†’ Label Data â†’ Train Model â†’ Deploy\n",
        "```\n",
        "\n",
        "**After 2022:**\n",
        "```\n",
        "Problem â†’ Try Zero-Shot LLM â†’ Works? â†’ Deploy API\n",
        "                            â†’ Doesn't work? â†’ Try Few-Shot\n",
        "                                            â†’ Still not enough? â†’ Fine-tune\n",
        "```\n",
        "\n",
        "### When to Use What (2024 Guide)\n",
        "\n",
        "| Approach | Best For | Cost | Latency | Accuracy |\n",
        "|----------|----------|------|---------|----------|\n",
        "| **Zero-Shot LLM** | Prototyping, low volume | High per request | High | Good-Excellent |\n",
        "| **Few-Shot LLM** | Limited labeled data | High per request | High | Good-Excellent |\n",
        "| **Fine-tuned BERT** | Classification, NER | Medium (one-time training) | Low-Medium | Excellent |\n",
        "| **Fine-tuned LLM** | Generation, complex reasoning | High (training + serving) | Medium | Excellent |\n",
        "| **RAG** | Knowledge-grounded tasks | Medium | Medium | Excellent |\n",
        "| **TF-IDF + ML** | Simple classification, baselines | Very Low | Very Low | Good |\n",
        "\n",
        "### The Rise of RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Most enterprise AI chatbots in 2024 use this pattern:\n",
        "\n",
        "```\n",
        "User Query\n",
        "    â”‚\n",
        "    â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Embed Query     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚\n",
        "         â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Vector Search  â”‚â—€â”€â”€â”€â”€â”‚  Vector DB with â”‚\n",
        "â”‚                 â”‚     â”‚  your documents â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚\n",
        "         â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Retrieved Docs  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚\n",
        "         â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ LLM + Context   â”‚\n",
        "â”‚ \"Answer based   â”‚\n",
        "â”‚  on these docs\" â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "         â”‚\n",
        "         â–¼\n",
        "    Final Answer\n",
        "```\n",
        "\n",
        "### Prompt Engineering is the New Feature Engineering\n",
        "\n",
        "In traditional ML:\n",
        "- Hours spent on feature engineering\n",
        "- Domain-specific feature extraction\n",
        "\n",
        "In LLM era:\n",
        "- Hours spent on prompt engineering\n",
        "- Domain-specific prompt templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zg9ONLU9POX"
      },
      "source": [
        "## 8. Production NLP Stack (2024)\n",
        "\n",
        "### Complete Production Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                         PRODUCTION NLP STACK                                 â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                             â”‚\n",
        "â”‚  DATA LAYER                                                                 â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
        "â”‚  â”‚  HuggingFace â”‚ â”‚ Label Studio â”‚ â”‚  Prodigy    â”‚ â”‚   Custom     â”‚       â”‚\n",
        "â”‚  â”‚   Datasets   â”‚ â”‚  (labeling)  â”‚ â”‚  (labeling) â”‚ â”‚   Scrapers   â”‚       â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                                                             â”‚\n",
        "â”‚  PREPROCESSING                                                              â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
        "â”‚  â”‚    SpaCy     â”‚ â”‚     NLTK     â”‚ â”‚  HuggingFace â”‚                        â”‚\n",
        "â”‚  â”‚  (prod-ready)â”‚ â”‚ (educational)â”‚ â”‚  Tokenizers  â”‚                        â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\n",
        "â”‚                                                                             â”‚\n",
        "â”‚  MODELING                                                                   â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
        "â”‚  â”‚  HuggingFace â”‚ â”‚   PyTorch    â”‚ â”‚   vLLM      â”‚ â”‚   OpenAI/    â”‚       â”‚\n",
        "â”‚  â”‚ Transformers â”‚ â”‚              â”‚ â”‚ (LLM serve) â”‚ â”‚   Anthropic  â”‚       â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                                                             â”‚\n",
        "â”‚  EXPERIMENT TRACKING                                                        â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
        "â”‚  â”‚  Weights &   â”‚ â”‚    MLflow    â”‚ â”‚  TensorBoard â”‚                        â”‚\n",
        "â”‚  â”‚   Biases     â”‚ â”‚              â”‚ â”‚              â”‚                        â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\n",
        "â”‚                                                                             â”‚\n",
        "â”‚  VECTOR DATABASE (for RAG)                                                  â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
        "â”‚  â”‚   Pinecone   â”‚ â”‚   Weaviate   â”‚ â”‚    Chroma    â”‚ â”‚    FAISS     â”‚       â”‚\n",
        "â”‚  â”‚   (managed)  â”‚ â”‚(open source) â”‚ â”‚ (lightweight)â”‚ â”‚   (local)    â”‚       â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                                                             â”‚\n",
        "â”‚  LLM ORCHESTRATION                                                          â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
        "â”‚  â”‚  LangChain   â”‚ â”‚  LlamaIndex  â”‚ â”‚   Haystack   â”‚                        â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\n",
        "â”‚                                                                             â”‚\n",
        "â”‚  SERVING                                                                    â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
        "â”‚  â”‚   FastAPI    â”‚ â”‚  TorchServe  â”‚ â”‚    Triton    â”‚ â”‚   Modal/     â”‚       â”‚\n",
        "â”‚  â”‚              â”‚ â”‚              â”‚ â”‚   (NVIDIA)   â”‚ â”‚   Replicate  â”‚       â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
        "â”‚                                                                             â”‚\n",
        "â”‚  MONITORING                                                                 â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
        "â”‚  â”‚  LangSmith   â”‚ â”‚   LangFuse   â”‚ â”‚  Prometheus  â”‚                        â”‚\n",
        "â”‚  â”‚  (LLM apps)  â”‚ â”‚  (LLM apps)  â”‚ â”‚  + Grafana   â”‚                        â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\n",
        "â”‚                                                                             â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXmYxcvc9POY"
      },
      "source": [
        "## 9. Quick Comparison: Traditional vs Modern Approaches\n",
        "\n",
        "### Text Classification Example\n",
        "\n",
        "**Traditional Approach (Pre-2018):**\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Preprocess\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "# Train\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(vectorizer.transform(test_texts))\n",
        "```\n",
        "\n",
        "**Modern Approach #1: Fine-tuned BERT (2018+):**\n",
        "```python\n",
        "from transformers import AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, train_dataset=train_dataset)\n",
        "trainer.train()\n",
        "```\n",
        "\n",
        "**Modern Approach #2: Zero-Shot LLM (2023+):**\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Classify the sentiment as positive or negative.\"},\n",
        "        {\"role\": \"user\", \"content\": text}\n",
        "    ]\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASz6TySr9POY",
        "outputId": "afba45f3-1b9b-47ea-a205-b64fdd712208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Classification Task:\n",
            "==================================================\n",
            "[Positive] I absolutely loved this movie! The acting was supe...\n",
            "[Negative] What a waste of time. Terrible plot and bad acting...\n",
            "[Positive] The product exceeded my expectations. Highly recom...\n",
            "[Negative] Never buying from this company again. Horrible exp...\n",
            "[Positive] Just okay. Nothing special but not bad either....\n"
          ]
        }
      ],
      "source": [
        "# Let's see a practical example of all three approaches\n",
        "# We'll create a simple sentiment classifier using each method\n",
        "\n",
        "# Sample data\n",
        "sample_texts = [\n",
        "    \"I absolutely loved this movie! The acting was superb.\",\n",
        "    \"What a waste of time. Terrible plot and bad acting.\",\n",
        "    \"The product exceeded my expectations. Highly recommend!\",\n",
        "    \"Never buying from this company again. Horrible experience.\",\n",
        "    \"Just okay. Nothing special but not bad either.\"\n",
        "]\n",
        "\n",
        "sample_labels = [1, 0, 1, 0, 1]  # 1 = positive, 0 = negative\n",
        "\n",
        "print(\"Sample Classification Task:\")\n",
        "print(\"=\"*50)\n",
        "for text, label in zip(sample_texts, sample_labels):\n",
        "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
        "    print(f\"[{sentiment}] {text[:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AAdR-_D9POY",
        "outputId": "8012203e-6083-4568-81c9-635854c1b08f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traditional Approach: TF-IDF + Logistic Regression\n",
            "==================================================\n",
            "Input: This is amazing! Best purchase ever!\n",
            "Prediction: Positive\n",
            "Confidence: 60.42%\n",
            "\n",
            "Pros: Fast, explainable, works with small data\n",
            "Cons: No semantic understanding, limited accuracy\n"
          ]
        }
      ],
      "source": [
        "# Approach 1: Traditional TF-IDF + Logistic Regression\n",
        "# This is your BASELINE - always start here!\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Create TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,\n",
        "    ngram_range=(1, 2),  # unigrams and bigrams\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(sample_texts)\n",
        "y = np.array(sample_labels)\n",
        "\n",
        "# Train simple model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Test on new text\n",
        "new_text = [\"This is amazing! Best purchase ever!\"]\n",
        "new_X = vectorizer.transform(new_text)\n",
        "prediction = model.predict(new_X)\n",
        "probability = model.predict_proba(new_X)\n",
        "\n",
        "print(\"Traditional Approach: TF-IDF + Logistic Regression\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Input: {new_text[0]}\")\n",
        "print(f\"Prediction: {'Positive' if prediction[0] == 1 else 'Negative'}\")\n",
        "print(f\"Confidence: {probability[0][prediction[0]]:.2%}\")\n",
        "print()\n",
        "print(\"Pros: Fast, explainable, works with small data\")\n",
        "print(\"Cons: No semantic understanding, limited accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWyZrs4y9POZ"
      },
      "source": [
        "## 10. Common Mistakes & Debugging\n",
        "\n",
        "### Mistake 1: Starting with the Most Complex Model\n",
        "âŒ \"I'll just use GPT-4 for everything!\"\n",
        "\n",
        "âœ… **Always start with a simple baseline:**\n",
        "- Classification: TF-IDF + Logistic Regression\n",
        "- NER: SpaCy's built-in NER\n",
        "- Generation: Zero-shot LLM prompting\n",
        "\n",
        "### Mistake 2: Ignoring Data Quality\n",
        "âŒ \"My model isn't working, I need a bigger model!\"\n",
        "\n",
        "âœ… **Check your data first:**\n",
        "- Label quality and consistency\n",
        "- Class imbalance\n",
        "- Data leakage\n",
        "- Duplicate examples\n",
        "\n",
        "### Mistake 3: Over-engineering Preprocessing\n",
        "âŒ \"Let me add stemming, lemmatization, and regex cleaning...\"\n",
        "\n",
        "âœ… **For transformer models:**\n",
        "- Minimal preprocessing is often better\n",
        "- The tokenizer handles most things\n",
        "- Over-cleaning can hurt performance\n",
        "\n",
        "### Mistake 4: Not Considering Latency\n",
        "âŒ \"BERT works great!\" (but takes 200ms per request)\n",
        "\n",
        "âœ… **Always profile latency:**\n",
        "- Know your latency budget\n",
        "- Consider distillation, quantization\n",
        "- Sometimes simpler models are better\n",
        "\n",
        "### Mistake 5: Ignoring Edge Cases\n",
        "âŒ \"Average accuracy is 95%!\"\n",
        "\n",
        "âœ… **Analyze failure modes:**\n",
        "- What types of inputs fail?\n",
        "- Are failures random or systematic?\n",
        "- Build targeted solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xMWtsvp9POc"
      },
      "source": [
        "## 11. Interview Questions\n",
        "\n",
        "### Conceptual Questions\n",
        "\n",
        "**Q1: What are the main types of NLP tasks? Give an example for each.**\n",
        "\n",
        "<details>\n",
        "<summary>Click for Answer</summary>\n",
        "\n",
        "1. **Text Classification**: Sentiment analysis (\"Great product!\" â†’ Positive)\n",
        "2. **Sequence Labeling**: NER (\"John works at Google\" â†’ PER, O, O, ORG)\n",
        "3. **Sequence-to-Sequence**: Machine translation (\"Hello\" â†’ \"Bonjour\")\n",
        "4. **Text Generation**: Story writing, chatbots\n",
        "5. **Question Answering**: \"What is the capital of France?\" â†’ \"Paris\"\n",
        "6. **Semantic Similarity**: Measuring if two sentences mean the same thing\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Q2: You have 500 labeled examples for a text classification task. What approach would you use?**\n",
        "\n",
        "<details>\n",
        "<summary>Click for Answer</summary>\n",
        "\n",
        "With only 500 samples, I would:\n",
        "\n",
        "1. **First try**: Zero-shot or few-shot LLM (GPT-4, Claude) - might work without training\n",
        "2. **Baseline**: TF-IDF + Logistic Regression - fast to train, surprisingly effective\n",
        "3. **If more accuracy needed**: Fine-tune a small model like DistilBERT\n",
        "4. **Consider**: Data augmentation to increase training data\n",
        "5. **Consider**: Active learning to label more valuable examples\n",
        "\n",
        "Key insight: With limited data, simpler models often generalize better.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Q3: When would you use TF-IDF over BERT embeddings?**\n",
        "\n",
        "<details>\n",
        "<summary>Click for Answer</summary>\n",
        "\n",
        "Use TF-IDF when:\n",
        "1. **Speed is critical**: TF-IDF is 100x faster\n",
        "2. **Explainability is required**: Can show which words matter\n",
        "3. **Limited training data**: BERT can overfit\n",
        "4. **As a baseline**: Always good to compare against\n",
        "5. **Keyword matching matters**: For search applications\n",
        "6. **Limited compute resources**: No GPU needed\n",
        "\n",
        "Use BERT when:\n",
        "1. Semantic understanding is important\n",
        "2. You have enough training data (1000+)\n",
        "3. Accuracy is the priority over speed\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Q4: Explain RAG. Why is it popular in 2024?**\n",
        "\n",
        "<details>\n",
        "<summary>Click for Answer</summary>\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation)**:\n",
        "1. User asks a question\n",
        "2. System retrieves relevant documents from a knowledge base\n",
        "3. LLM generates answer using retrieved context\n",
        "\n",
        "**Why it's popular**:\n",
        "- Reduces hallucination (LLM has factual grounding)\n",
        "- Keeps knowledge up-to-date (just update the database)\n",
        "- Works with private/proprietary data\n",
        "- Cheaper than fine-tuning (no training needed)\n",
        "- Provides citations/sources\n",
        "\n",
        "Most enterprise chatbots use RAG in 2024.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Q5: You're designing a customer support chatbot. Walk through your decision process.**\n",
        "\n",
        "<details>\n",
        "<summary>Click for Answer</summary>\n",
        "\n",
        "1. **Understand requirements**:\n",
        "   - What types of questions? (FAQ, technical, billing)\n",
        "   - Latency requirements?\n",
        "   - Accuracy requirements?\n",
        "   - Budget constraints?\n",
        "\n",
        "2. **Start simple**:\n",
        "   - FAQ matching with TF-IDF/embeddings\n",
        "   - Intent classification for routing\n",
        "\n",
        "3. **Add RAG**:\n",
        "   - Index company documentation\n",
        "   - Retrieve relevant articles for context\n",
        "   - Generate answers with LLM\n",
        "\n",
        "4. **Handle edge cases**:\n",
        "   - Escalation to human when confidence is low\n",
        "   - Guardrails against off-topic queries\n",
        "\n",
        "5. **Monitor and improve**:\n",
        "   - Track user satisfaction\n",
        "   - Log failure cases\n",
        "   - Continuously update knowledge base\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m8-pN5A9POc"
      },
      "source": [
        "## 12. Summary\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **NLP Problem Types**: Classification, Sequence Labeling, Seq2Seq, Generation, QA, Semantic Similarity\n",
        "\n",
        "2. **The 2024 Reality**:\n",
        "   - Start with \"Can an LLM solve this zero-shot?\"\n",
        "   - RAG is the dominant pattern for knowledge-grounded tasks\n",
        "   - Prompt engineering is the new feature engineering\n",
        "\n",
        "3. **Model Selection Framework**:\n",
        "   - Consider: data size, latency, accuracy, budget\n",
        "   - Always have a simple baseline\n",
        "\n",
        "4. **Production Stack**:\n",
        "   - Data: HuggingFace Datasets, Label Studio\n",
        "   - Models: HuggingFace Transformers\n",
        "   - Serving: FastAPI, vLLM\n",
        "   - Vector DB: Pinecone, Chroma\n",
        "   - Orchestration: LangChain, LlamaIndex\n",
        "\n",
        "5. **Common Mistakes**:\n",
        "   - Starting too complex\n",
        "   - Ignoring data quality\n",
        "   - Over-preprocessing for transformers\n",
        "   - Not considering latency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_rcH_5Y9POd"
      },
      "source": [
        "## 13. Exercises\n",
        "\n",
        "### Exercise 1: Problem Identification\n",
        "For each scenario, identify the NLP task type:\n",
        "\n",
        "1. Building a system to automatically tag customer support tickets by department\n",
        "2. Extracting product names and prices from e-commerce descriptions\n",
        "3. Creating a chatbot that answers questions about company policies\n",
        "4. Building a plagiarism detector for student essays\n",
        "5. Automatically generating product descriptions from specifications\n",
        "\n",
        "<details>\n",
        "<summary>Click for Answers</summary>\n",
        "\n",
        "1. **Text Classification** (Multi-label classification)\n",
        "2. **Named Entity Recognition / Information Extraction**\n",
        "3. **Question Answering** (likely with RAG)\n",
        "4. **Semantic Similarity** (Compare essay pairs)\n",
        "5. **Text Generation** (Seq2Seq or LLM-based)\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 2: Model Selection\n",
        "For each scenario, recommend an approach:\n",
        "\n",
        "1. 50 labeled examples, need results tomorrow, budget $0\n",
        "2. 100K labeled examples, need <10ms latency, high accuracy\n",
        "3. No labeled data, need to classify customer reviews\n",
        "4. Build a Q&A system over 10 years of company documents\n",
        "\n",
        "<details>\n",
        "<summary>Click for Answers</summary>\n",
        "\n",
        "1. **Few-shot prompting with open-source LLM** (LLaMA, Mistral)\n",
        "2. **Quantized DistilBERT or TF-IDF + XGBoost** for speed\n",
        "3. **Zero-shot LLM classification** (GPT-4, Claude) or self-training\n",
        "4. **RAG system**: Embed documents â†’ Vector DB â†’ LLM for answers\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 3: Critique This Approach\n",
        "Your colleague proposes:\n",
        "> \"Let's fine-tune GPT-3.5 on our 200 labeled examples for sentiment classification. We'll use the HuggingFace Trainer with default settings.\"\n",
        "\n",
        "What problems do you see?\n",
        "\n",
        "<details>\n",
        "<summary>Click for Answer</summary>\n",
        "\n",
        "Problems:\n",
        "1. **200 examples is too few** for fine-tuning a large model - likely to overfit\n",
        "2. **No baseline comparison** - should try simpler approaches first\n",
        "3. **GPT-3.5 can't be fine-tuned via HuggingFace** - it's OpenAI API only\n",
        "4. **Default settings** rarely optimal - need to tune learning rate, epochs\n",
        "5. **Sentiment is a simple task** - might not need a large model\n",
        "\n",
        "Better approach:\n",
        "1. Try zero-shot GPT-3.5\n",
        "2. Baseline with TF-IDF + LogReg\n",
        "3. If needed, fine-tune DistilBERT with careful regularization\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fep7CVd69POd"
      },
      "source": [
        "## 14. References & Further Reading\n",
        "\n",
        "### Foundational Papers\n",
        "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer architecture (2017)\n",
        "- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) (2018)\n",
        "- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - GPT-3 (2020)\n",
        "\n",
        "### Books\n",
        "- \"Speech and Language Processing\" - Jurafsky & Martin (Free online!)\n",
        "- \"Natural Language Processing with Transformers\" - HuggingFace Team\n",
        "\n",
        "### Online Resources\n",
        "- [HuggingFace Course](https://huggingface.co/course)\n",
        "- [Stanford CS224N: NLP with Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)\n",
        "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "### Stay Updated\n",
        "- [Papers with Code - NLP](https://paperswithcode.com/area/natural-language-processing)\n",
        "- [Sebastian Raschka's Newsletter](https://magazine.sebastianraschka.com/)\n",
        "- [The Batch by Andrew Ng](https://www.deeplearning.ai/the-batch/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RlNzzXt9POd"
      },
      "source": [
        "---\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "Now that you understand the NLP landscape, proceed to:\n",
        "\n",
        "**[Module 01: Text Preprocessing Pipeline](../01_text_preprocessing/01_text_preprocessing.ipynb)**\n",
        "\n",
        "We'll dive deep into cleaning and preparing text data - the foundation of any NLP project.\n",
        "\n",
        "---\n",
        "\n",
        "*Module 00 Complete | NLP with PyTorch*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}