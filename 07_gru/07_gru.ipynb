{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 07: GRU (Gated Recurrent Unit)\n",
                "\n",
                "**A Simplified Alternative to LSTM**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand GRU architecture\n",
                "- âœ… Compare GRU vs LSTM\n",
                "- âœ… Implement GRU from scratch\n",
                "- âœ… Know when to use which"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 06: LSTM](../06_lstm/06_lstm.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Intuition & Motivation\n",
                "\n",
                "### GRU: LSTM Simplified\n",
                "\n",
                "| Aspect | LSTM | GRU |\n",
                "|--------|------|-----|\n",
                "| Gates | 3 (forget, input, output) | 2 (update, reset) |\n",
                "| States | 2 (hidden, cell) | 1 (hidden only) |\n",
                "| Parameters | More | ~25% fewer |\n",
                "| Performance | Often similar | Often similar |\n",
                "\n",
                "**Key insight**: GRU merges forget and input gates into one \"update\" gate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(\"Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Mathematical Foundation\n",
                "\n",
                "### GRU Equations\n",
                "\n",
                "**1. Update Gate** - How much of new state to use:\n",
                "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$$\n",
                "\n",
                "**2. Reset Gate** - How much of past to forget:\n",
                "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$$\n",
                "\n",
                "**3. Candidate State** - Proposed new state:\n",
                "$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)$$\n",
                "\n",
                "**4. Final State** - Interpolate old and new:\n",
                "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
                "\n",
                "### Key Insight\n",
                "\n",
                "- When $z_t \\approx 0$: Keep old state ($h_t \\approx h_{t-1}$)\n",
                "- When $z_t \\approx 1$: Use new state ($h_t \\approx \\tilde{h}_t$)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. GRU from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GRUCell:\n",
                "    \"\"\"GRU cell from scratch (NumPy).\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size: int, hidden_size: int):\n",
                "        self.hidden_size = hidden_size\n",
                "        combined_size = input_size + hidden_size\n",
                "        scale = np.sqrt(2.0 / combined_size)\n",
                "        \n",
                "        # Update gate\n",
                "        self.W_z = np.random.randn(hidden_size, combined_size) * scale\n",
                "        self.b_z = np.zeros((hidden_size, 1))\n",
                "        \n",
                "        # Reset gate\n",
                "        self.W_r = np.random.randn(hidden_size, combined_size) * scale\n",
                "        self.b_r = np.zeros((hidden_size, 1))\n",
                "        \n",
                "        # Candidate\n",
                "        self.W_h = np.random.randn(hidden_size, combined_size) * scale\n",
                "        self.b_h = np.zeros((hidden_size, 1))\n",
                "    \n",
                "    def sigmoid(self, x):\n",
                "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
                "    \n",
                "    def forward(self, x, h_prev):\n",
                "        \"\"\"\n",
                "        Single GRU step.\n",
                "        \n",
                "        Args:\n",
                "            x: (input_size, 1)\n",
                "            h_prev: (hidden_size, 1)\n",
                "        \"\"\"\n",
                "        combined = np.vstack([h_prev, x])\n",
                "        \n",
                "        # Update gate\n",
                "        z = self.sigmoid(self.W_z @ combined + self.b_z)\n",
                "        \n",
                "        # Reset gate\n",
                "        r = self.sigmoid(self.W_r @ combined + self.b_r)\n",
                "        \n",
                "        # Candidate (reset applied to h_prev)\n",
                "        combined_reset = np.vstack([r * h_prev, x])\n",
                "        h_tilde = np.tanh(self.W_h @ combined_reset + self.b_h)\n",
                "        \n",
                "        # Final state: interpolate\n",
                "        h = (1 - z) * h_prev + z * h_tilde\n",
                "        \n",
                "        return h, {'z': z, 'r': r, 'h_tilde': h_tilde}\n",
                "\n",
                "# Test\n",
                "cell = GRUCell(input_size=10, hidden_size=20)\n",
                "x = np.random.randn(10, 1)\n",
                "h = np.zeros((20, 1))\n",
                "\n",
                "h_new, gates = cell.forward(x, h)\n",
                "print(f\"Input: {x.shape}\")\n",
                "print(f\"Hidden: {h_new.shape}\")\n",
                "print(f\"Gates: z={gates['z'].mean():.3f}, r={gates['r'].mean():.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GRU:\n",
                "    \"\"\"Full GRU layer from scratch.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size: int, hidden_size: int):\n",
                "        self.cell = GRUCell(input_size, hidden_size)\n",
                "        self.hidden_size = hidden_size\n",
                "    \n",
                "    def forward(self, inputs, h0=None):\n",
                "        if h0 is None:\n",
                "            h0 = np.zeros((self.hidden_size, 1))\n",
                "        \n",
                "        h = h0\n",
                "        outputs = []\n",
                "        \n",
                "        for x in inputs:\n",
                "            h, _ = self.cell.forward(x, h)\n",
                "            outputs.append(h)\n",
                "        \n",
                "        return outputs, h\n",
                "\n",
                "# Test\n",
                "gru = GRU(input_size=10, hidden_size=20)\n",
                "seq = [np.random.randn(10, 1) for _ in range(15)]\n",
                "outputs, h_n = gru.forward(seq)\n",
                "print(f\"Outputs: {len(outputs)}, Final hidden: {h_n.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. PyTorch Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PyTorch GRU\n",
                "gru_pt = nn.GRU(\n",
                "    input_size=10,\n",
                "    hidden_size=20,\n",
                "    num_layers=2,\n",
                "    batch_first=True,\n",
                "    dropout=0.1\n",
                ")\n",
                "\n",
                "x = torch.randn(32, 15, 10)  # (batch, seq, features)\n",
                "h0 = torch.zeros(2, 32, 20)  # (layers, batch, hidden)\n",
                "\n",
                "output, h_n = gru_pt(x, h0)\n",
                "\n",
                "print(f\"Input: {x.shape}\")\n",
                "print(f\"Output: {output.shape}\")\n",
                "print(f\"h_n: {h_n.shape}\")\n",
                "\n",
                "# Compare parameter counts\n",
                "lstm = nn.LSTM(10, 20, 2, batch_first=True)\n",
                "print(f\"\\nLSTM params: {sum(p.numel() for p in lstm.parameters()):,}\")\n",
                "print(f\"GRU params: {sum(p.numel() for p in gru_pt.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. GRU vs LSTM Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Speed comparison\n",
                "import time\n",
                "\n",
                "def benchmark(model, x, h0, n_runs=100):\n",
                "    # Warmup\n",
                "    for _ in range(10):\n",
                "        _ = model(x, h0) if isinstance(h0, tuple) else model(x, h0)\n",
                "    \n",
                "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
                "    start = time.time()\n",
                "    for _ in range(n_runs):\n",
                "        _ = model(x, h0) if isinstance(h0, tuple) else model(x, h0)\n",
                "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
                "    return (time.time() - start) / n_runs * 1000\n",
                "\n",
                "x = torch.randn(32, 100, 128)\n",
                "lstm = nn.LSTM(128, 256, 2, batch_first=True)\n",
                "gru = nn.GRU(128, 256, 2, batch_first=True)\n",
                "\n",
                "h0_lstm = (torch.zeros(2, 32, 256), torch.zeros(2, 32, 256))\n",
                "h0_gru = torch.zeros(2, 32, 256)\n",
                "\n",
                "lstm_time = benchmark(lstm, x, h0_lstm)\n",
                "gru_time = benchmark(gru, x, h0_gru)\n",
                "\n",
                "print(f\"LSTM: {lstm_time:.2f} ms\")\n",
                "print(f\"GRU: {gru_time:.2f} ms\")\n",
                "print(f\"GRU is {lstm_time/gru_time:.2f}x faster\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. ðŸ”¥ Real-World Usage\n",
                "\n",
                "### When to Use GRU vs LSTM\n",
                "\n",
                "| Factor | Choose GRU | Choose LSTM |\n",
                "|--------|-----------|-------------|\n",
                "| Model size matters | âœ… Fewer params | |\n",
                "| Training speed | âœ… Faster | |\n",
                "| Very long sequences | | âœ… Better memory |\n",
                "| Default choice | Try both! | Try both! |\n",
                "\n",
                "### In Practice\n",
                "\n",
                "- Performance is usually **similar**\n",
                "- LSTM is **slightly more common** (historical reasons)\n",
                "- **Try both**, pick what works for your task"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Interview Questions\n",
                "\n",
                "**Q1: What's the difference between GRU and LSTM?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- GRU has 2 gates (update, reset), LSTM has 3 (forget, input, output)\n",
                "- GRU has 1 state, LSTM has 2 (hidden + cell)\n",
                "- GRU is ~25% fewer parameters\n",
                "- Performance is usually similar\n",
                "</details>\n",
                "\n",
                "**Q2: How does GRU handle long-term dependencies?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "The update gate z can be close to 0, making h_t â‰ˆ h_{t-1}. This allows information to flow unchanged through time, similar to LSTM's cell state.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Summary\n",
                "\n",
                "- **GRU**: Simplified LSTM with 2 gates (update, reset)\n",
                "- **Update gate (z)**: Interpolate old and new state\n",
                "- **Reset gate (r)**: Control how much past to use\n",
                "- **Equation**: $h_t = (1-z) \\odot h_{t-1} + z \\odot \\tilde{h}_t$\n",
                "- **Practice**: ~25% fewer params, often similar performance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Exercises\n",
                "\n",
                "1. Compare GRU vs LSTM on sentiment classification\n",
                "2. Visualize gate activations for both models\n",
                "3. Implement backward pass for GRU"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. References\n",
                "\n",
                "- [GRU Paper (2014)](https://arxiv.org/abs/1406.1078)\n",
                "- [Empirical Evaluation of Gated RNNs](https://arxiv.org/abs/1412.3555)\n",
                "- [PyTorch GRU Docs](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
                "\n",
                "---\n",
                "**Next:** [Module 08: Bidirectional & Deep RNNs](../08_bidirectional_deep_rnns/08_bidirectional_deep_rnns.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}