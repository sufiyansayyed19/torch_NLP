{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-FV1r1L6kAf"
      },
      "source": [
        "# Module 07: GRU (Gated Recurrent Unit)\n",
        "\n",
        "**A Simplified Alternative to LSTM**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVFO8z0-6kAg"
      },
      "source": [
        "## 1. Objectives\n",
        "\n",
        "- âœ… Understand GRU architecture\n",
        "- âœ… Compare GRU vs LSTM\n",
        "- âœ… Implement GRU from scratch\n",
        "- âœ… Know when to use which"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saOw-Bgn6kAh"
      },
      "source": [
        "## 2. Prerequisites\n",
        "\n",
        "- [Module 06: LSTM](../06_lstm/06_lstm.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlUJe1ag6kAi"
      },
      "source": [
        "## 3. Intuition & Motivation\n",
        "\n",
        "### GRU: LSTM Simplified\n",
        "\n",
        "| Aspect | LSTM | GRU |\n",
        "|--------|------|-----|\n",
        "| Gates | 3 (forget, input, output) | 2 (update, reset) |\n",
        "| States | 2 (hidden, cell) | 1 (hidden only) |\n",
        "| Parameters | More | ~25% fewer |\n",
        "| Performance | Often similar | Often similar |\n",
        "\n",
        "**Key insight**: GRU merges forget and input gates into one \"update\" gate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IT1Gs7E6kAi",
        "outputId": "ce824a11-6bce-4e66-e4e5-06f329ce12fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EjjRtkj6kAj"
      },
      "source": [
        "## 4. Mathematical Foundation\n",
        "\n",
        "### GRU Equations\n",
        "\n",
        "**1. Update Gate** - How much of new state to use:\n",
        "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$$\n",
        "\n",
        "**2. Reset Gate** - How much of past to forget:\n",
        "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$$\n",
        "\n",
        "**3. Candidate State** - Proposed new state:\n",
        "$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)$$\n",
        "\n",
        "**4. Final State** - Interpolate old and new:\n",
        "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "- When $z_t \\approx 0$: Keep old state ($h_t \\approx h_{t-1}$)\n",
        "- When $z_t \\approx 1$: Use new state ($h_t \\approx \\tilde{h}_t$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke_9TBqK6kAj"
      },
      "source": [
        "## 5. GRU from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLYH7qL-6kAk",
        "outputId": "5b3bc332-cc03-469e-c447-edde1b807ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: (10, 1)\n",
            "Hidden: (20, 1)\n",
            "Gates: z=0.537, r=0.525\n"
          ]
        }
      ],
      "source": [
        "class GRUCell:\n",
        "    \"\"\"GRU cell from scratch (NumPy).\"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int):\n",
        "        self.hidden_size = hidden_size\n",
        "        combined_size = input_size + hidden_size\n",
        "        scale = np.sqrt(2.0 / combined_size)\n",
        "\n",
        "        # Update gate\n",
        "        self.W_z = np.random.randn(hidden_size, combined_size) * scale\n",
        "        self.b_z = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Reset gate\n",
        "        self.W_r = np.random.randn(hidden_size, combined_size) * scale\n",
        "        self.b_r = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Candidate\n",
        "        self.W_h = np.random.randn(hidden_size, combined_size) * scale\n",
        "        self.b_h = np.zeros((hidden_size, 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        Single GRU step.\n",
        "\n",
        "        Args:\n",
        "            x: (input_size, 1)\n",
        "            h_prev: (hidden_size, 1)\n",
        "        \"\"\"\n",
        "        combined = np.vstack([h_prev, x])\n",
        "\n",
        "        # Update gate\n",
        "        z = self.sigmoid(self.W_z @ combined + self.b_z)\n",
        "\n",
        "        # Reset gate\n",
        "        r = self.sigmoid(self.W_r @ combined + self.b_r)\n",
        "\n",
        "        # Candidate (reset applied to h_prev)\n",
        "        combined_reset = np.vstack([r * h_prev, x])\n",
        "        h_tilde = np.tanh(self.W_h @ combined_reset + self.b_h)\n",
        "\n",
        "        # Final state: interpolate\n",
        "        h = (1 - z) * h_prev + z * h_tilde\n",
        "\n",
        "        return h, {'z': z, 'r': r, 'h_tilde': h_tilde}\n",
        "\n",
        "# Test\n",
        "cell = GRUCell(input_size=10, hidden_size=20)\n",
        "x = np.random.randn(10, 1)\n",
        "h = np.zeros((20, 1))\n",
        "\n",
        "h_new, gates = cell.forward(x, h)\n",
        "print(f\"Input: {x.shape}\")\n",
        "print(f\"Hidden: {h_new.shape}\")\n",
        "print(f\"Gates: z={gates['z'].mean():.3f}, r={gates['r'].mean():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COZ8lSh66kAk",
        "outputId": "2cabfac0-e460-4928-e940-5b2b2bed27b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs: 15, Final hidden: (20, 1)\n"
          ]
        }
      ],
      "source": [
        "class GRU:\n",
        "    \"\"\"Full GRU layer from scratch.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int):\n",
        "        self.cell = GRUCell(input_size, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, inputs, h0=None):\n",
        "        if h0 is None:\n",
        "            h0 = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "        h = h0\n",
        "        outputs = []\n",
        "\n",
        "        for x in inputs:\n",
        "            h, _ = self.cell.forward(x, h)\n",
        "            outputs.append(h)\n",
        "\n",
        "        return outputs, h\n",
        "\n",
        "# Test\n",
        "gru = GRU(input_size=10, hidden_size=20)\n",
        "seq = [np.random.randn(10, 1) for _ in range(15)]\n",
        "outputs, h_n = gru.forward(seq)\n",
        "print(f\"Outputs: {len(outputs)}, Final hidden: {h_n.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jjg31L46kAl"
      },
      "source": [
        "## 6. PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4CiL56R6kAl",
        "outputId": "f10ca65a-fb39-47e7-814f-ae9670e5fd28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: torch.Size([32, 15, 10])\n",
            "Output: torch.Size([32, 15, 20])\n",
            "h_n: torch.Size([2, 32, 20])\n",
            "\n",
            "LSTM params: 5,920\n",
            "GRU params: 4,440\n"
          ]
        }
      ],
      "source": [
        "# PyTorch GRU\n",
        "gru_pt = nn.GRU(\n",
        "    input_size=10,\n",
        "    hidden_size=20,\n",
        "    num_layers=2,\n",
        "    batch_first=True,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "x = torch.randn(32, 15, 10)  # (batch, seq, features)\n",
        "h0 = torch.zeros(2, 32, 20)  # (layers, batch, hidden)\n",
        "\n",
        "output, h_n = gru_pt(x, h0)\n",
        "\n",
        "print(f\"Input: {x.shape}\")\n",
        "print(f\"Output: {output.shape}\")\n",
        "print(f\"h_n: {h_n.shape}\")\n",
        "\n",
        "# Compare parameter counts\n",
        "lstm = nn.LSTM(10, 20, 2, batch_first=True)\n",
        "print(f\"\\nLSTM params: {sum(p.numel() for p in lstm.parameters()):,}\")\n",
        "print(f\"GRU params: {sum(p.numel() for p in gru_pt.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okV4EEnG6kAl"
      },
      "source": [
        "## 7. GRU vs LSTM Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86TOx6VK6kAl",
        "outputId": "ed3c64f9-34d7-4528-c1ff-28eb5289f2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM: 196.63 ms\n",
            "GRU: 124.71 ms\n",
            "GRU is 1.58x faster\n"
          ]
        }
      ],
      "source": [
        "# Speed comparison\n",
        "import time\n",
        "\n",
        "def benchmark(model, x, h0, n_runs=100):\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = model(x, h0) if isinstance(h0, tuple) else model(x, h0)\n",
        "\n",
        "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "    start = time.time()\n",
        "    for _ in range(n_runs):\n",
        "        _ = model(x, h0) if isinstance(h0, tuple) else model(x, h0)\n",
        "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "    return (time.time() - start) / n_runs * 1000\n",
        "\n",
        "x = torch.randn(32, 100, 128)\n",
        "lstm = nn.LSTM(128, 256, 2, batch_first=True)\n",
        "gru = nn.GRU(128, 256, 2, batch_first=True)\n",
        "\n",
        "h0_lstm = (torch.zeros(2, 32, 256), torch.zeros(2, 32, 256))\n",
        "h0_gru = torch.zeros(2, 32, 256)\n",
        "\n",
        "lstm_time = benchmark(lstm, x, h0_lstm)\n",
        "gru_time = benchmark(gru, x, h0_gru)\n",
        "\n",
        "print(f\"LSTM: {lstm_time:.2f} ms\")\n",
        "print(f\"GRU: {gru_time:.2f} ms\")\n",
        "print(f\"GRU is {lstm_time/gru_time:.2f}x faster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z53mMlqH6kAl"
      },
      "source": [
        "## 8. ðŸ”¥ Real-World Usage\n",
        "\n",
        "### When to Use GRU vs LSTM\n",
        "\n",
        "| Factor | Choose GRU | Choose LSTM |\n",
        "|--------|-----------|-------------|\n",
        "| Model size matters | âœ… Fewer params | |\n",
        "| Training speed | âœ… Faster | |\n",
        "| Very long sequences | | âœ… Better memory |\n",
        "| Default choice | Try both! | Try both! |\n",
        "\n",
        "### In Practice\n",
        "\n",
        "- Performance is usually **similar**\n",
        "- LSTM is **slightly more common** (historical reasons)\n",
        "- **Try both**, pick what works for your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0q8Qbla6kAm"
      },
      "source": [
        "## 9. Interview Questions\n",
        "\n",
        "**Q1: What's the difference between GRU and LSTM?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "- GRU has 2 gates (update, reset), LSTM has 3 (forget, input, output)\n",
        "- GRU has 1 state, LSTM has 2 (hidden + cell)\n",
        "- GRU is ~25% fewer parameters\n",
        "- Performance is usually similar\n",
        "</details>\n",
        "\n",
        "**Q2: How does GRU handle long-term dependencies?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "The update gate z can be close to 0, making h_t â‰ˆ h_{t-1}. This allows information to flow unchanged through time, similar to LSTM's cell state.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNZl_L0l6kAm"
      },
      "source": [
        "## 10. Summary\n",
        "\n",
        "- **GRU**: Simplified LSTM with 2 gates (update, reset)\n",
        "- **Update gate (z)**: Interpolate old and new state\n",
        "- **Reset gate (r)**: Control how much past to use\n",
        "- **Equation**: $h_t = (1-z) \\odot h_{t-1} + z \\odot \\tilde{h}_t$\n",
        "- **Practice**: ~25% fewer params, often similar performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBHe42dM6kAm"
      },
      "source": [
        "## 11. Exercises\n",
        "\n",
        "1. Compare GRU vs LSTM on sentiment classification\n",
        "2. Visualize gate activations for both models\n",
        "3. Implement backward pass for GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iel6DwOR6kAm"
      },
      "source": [
        "## 12. References\n",
        "\n",
        "- [GRU Paper (2014)](https://arxiv.org/abs/1406.1078)\n",
        "- [Empirical Evaluation of Gated RNNs](https://arxiv.org/abs/1412.3555)\n",
        "- [PyTorch GRU Docs](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
        "\n",
        "---\n",
        "**Next:** [Module 08: Bidirectional & Deep RNNs](../08_bidirectional_deep_rnns/08_bidirectional_deep_rnns.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}