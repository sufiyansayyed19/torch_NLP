{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 02: Text Representation (Classical)\n",
                "\n",
                "**Bag of Words, TF-IDF, and N-grams - Your Essential Baselines**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand Bag of Words representation and its limitations\n",
                "- âœ… Master TF-IDF from scratch with mathematical derivation\n",
                "- âœ… Implement N-grams for capturing local context\n",
                "- âœ… Build a complete text classification baseline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 01: Text Preprocessing](../01_text_preprocessing/01_text_preprocessing.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Intuition & Motivation\n",
                "\n",
                "### The Problem\n",
                "Machine learning models need **numbers**, not text. How do we convert:\n",
                "\n",
                "`\"I love this movie\"` â†’ `[0.2, 0.5, 0.1, ...]`\n",
                "\n",
                "### Classical Approaches\n",
                "\n",
                "| Method | Idea | Captures Meaning? |\n",
                "|--------|------|------------------|\n",
                "| Bag of Words | Count words | âŒ No semantics |\n",
                "| TF-IDF | Weight by importance | âŒ No semantics |\n",
                "| N-grams | Local word patterns | ðŸ”¶ Some context |\n",
                "\n",
                "> **These are your BASELINES** - always try them first!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import numpy as np\n",
                "from collections import Counter\n",
                "from typing import List, Dict\n",
                "import math\n",
                "\n",
                "# Sample corpus for examples\n",
                "corpus = [\n",
                "    \"I love this movie it is amazing\",\n",
                "    \"This movie is terrible I hate it\",\n",
                "    \"Great film with amazing acting\",\n",
                "    \"Worst movie ever seen terrible acting\"\n",
                "]\n",
                "labels = [1, 0, 1, 0]  # 1=positive, 0=negative"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Mathematical Foundation\n",
                "\n",
                "### 4.1 Bag of Words (BoW)\n",
                "\n",
                "**Idea**: Represent text as word counts, ignoring order.\n",
                "\n",
                "For vocabulary $V = \\{w_1, w_2, ..., w_n\\}$ and document $d$:\n",
                "\n",
                "$$\\text{BoW}(d) = [count(w_1, d), count(w_2, d), ..., count(w_n, d)]$$\n",
                "\n",
                "### 4.2 TF-IDF\n",
                "\n",
                "**Idea**: Words that appear frequently in a document but rarely across documents are more informative.\n",
                "\n",
                "**Term Frequency (TF)**:\n",
                "$$TF(t, d) = \\frac{\\text{count of } t \\text{ in } d}{\\text{total words in } d}$$\n",
                "\n",
                "**Inverse Document Frequency (IDF)**:\n",
                "$$IDF(t) = \\log\\left(\\frac{N}{df(t)}\\right)$$\n",
                "\n",
                "Where:\n",
                "- $N$ = total number of documents\n",
                "- $df(t)$ = number of documents containing term $t$\n",
                "\n",
                "**TF-IDF**:\n",
                "$$\\text{TF-IDF}(t, d) = TF(t, d) \\times IDF(t)$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Implementation from Scratch\n",
                "\n",
                "### 5.1 Bag of Words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BagOfWords:\n",
                "    \"\"\"Bag of Words vectorizer from scratch.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.vocab = {}  # word -> index\n",
                "        self.vocab_size = 0\n",
                "    \n",
                "    def fit(self, documents: List[str]):\n",
                "        \"\"\"Build vocabulary from documents.\"\"\"\n",
                "        all_words = set()\n",
                "        for doc in documents:\n",
                "            all_words.update(doc.lower().split())\n",
                "        \n",
                "        self.vocab = {word: idx for idx, word in enumerate(sorted(all_words))}\n",
                "        self.vocab_size = len(self.vocab)\n",
                "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
                "        return self\n",
                "    \n",
                "    def transform(self, documents: List[str]) -> np.ndarray:\n",
                "        \"\"\"Convert documents to BoW vectors.\"\"\"\n",
                "        vectors = np.zeros((len(documents), self.vocab_size))\n",
                "        \n",
                "        for i, doc in enumerate(documents):\n",
                "            word_counts = Counter(doc.lower().split())\n",
                "            for word, count in word_counts.items():\n",
                "                if word in self.vocab:\n",
                "                    vectors[i, self.vocab[word]] = count\n",
                "        return vectors\n",
                "    \n",
                "    def fit_transform(self, documents: List[str]) -> np.ndarray:\n",
                "        return self.fit(documents).transform(documents)\n",
                "\n",
                "# Test BoW\n",
                "bow = BagOfWords()\n",
                "X_bow = bow.fit_transform(corpus)\n",
                "\n",
                "print(f\"\\nShape: {X_bow.shape}\")\n",
                "print(f\"Vocabulary: {list(bow.vocab.keys())[:10]}...\")\n",
                "print(f\"\\nFirst document vector (non-zero): {[(list(bow.vocab.keys())[i], int(X_bow[0,i])) for i in np.nonzero(X_bow[0])[0]]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 TF-IDF from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TfidfVectorizer:\n",
                "    \"\"\"TF-IDF vectorizer from scratch.\"\"\"\n",
                "    \n",
                "    def __init__(self, max_features: int = None):\n",
                "        self.vocab = {}\n",
                "        self.idf = {}\n",
                "        self.max_features = max_features\n",
                "    \n",
                "    def fit(self, documents: List[str]):\n",
                "        \"\"\"Compute vocabulary and IDF values.\"\"\"\n",
                "        N = len(documents)\n",
                "        doc_freq = Counter()  # How many docs contain each word\n",
                "        \n",
                "        # Count document frequencies\n",
                "        for doc in documents:\n",
                "            unique_words = set(doc.lower().split())\n",
                "            for word in unique_words:\n",
                "                doc_freq[word] += 1\n",
                "        \n",
                "        # Build vocabulary (optionally limit by frequency)\n",
                "        if self.max_features:\n",
                "            most_common = doc_freq.most_common(self.max_features)\n",
                "            self.vocab = {word: idx for idx, (word, _) in enumerate(most_common)}\n",
                "        else:\n",
                "            self.vocab = {word: idx for idx, word in enumerate(sorted(doc_freq.keys()))}\n",
                "        \n",
                "        # Compute IDF: log(N / df(t)) with smoothing\n",
                "        for word in self.vocab:\n",
                "            self.idf[word] = math.log((N + 1) / (doc_freq[word] + 1)) + 1\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def transform(self, documents: List[str]) -> np.ndarray:\n",
                "        \"\"\"Convert documents to TF-IDF vectors.\"\"\"\n",
                "        vectors = np.zeros((len(documents), len(self.vocab)))\n",
                "        \n",
                "        for i, doc in enumerate(documents):\n",
                "            words = doc.lower().split()\n",
                "            word_counts = Counter(words)\n",
                "            doc_len = len(words)\n",
                "            \n",
                "            for word, count in word_counts.items():\n",
                "                if word in self.vocab:\n",
                "                    # TF = count / doc_length\n",
                "                    tf = count / doc_len\n",
                "                    # TF-IDF = TF * IDF\n",
                "                    vectors[i, self.vocab[word]] = tf * self.idf[word]\n",
                "            \n",
                "            # L2 normalize\n",
                "            norm = np.linalg.norm(vectors[i])\n",
                "            if norm > 0:\n",
                "                vectors[i] /= norm\n",
                "        \n",
                "        return vectors\n",
                "    \n",
                "    def fit_transform(self, documents: List[str]) -> np.ndarray:\n",
                "        return self.fit(documents).transform(documents)\n",
                "\n",
                "# Test TF-IDF\n",
                "tfidf = TfidfVectorizer()\n",
                "X_tfidf = tfidf.fit_transform(corpus)\n",
                "\n",
                "print(f\"Shape: {X_tfidf.shape}\")\n",
                "print(f\"\\nIDF values (sample):\")\n",
                "for word in ['movie', 'amazing', 'terrible']:\n",
                "    if word in tfidf.idf:\n",
                "        print(f\"  {word}: {tfidf.idf[word]:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 N-grams"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_ngrams(text: str, n: int) -> List[str]:\n",
                "    \"\"\"Extract n-grams from text.\"\"\"\n",
                "    words = text.lower().split()\n",
                "    return ['_'.join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
                "\n",
                "# Examples\n",
                "text = \"I love this movie\"\n",
                "print(f\"Text: {text}\")\n",
                "print(f\"Unigrams: {get_ngrams(text, 1)}\")\n",
                "print(f\"Bigrams: {get_ngrams(text, 2)}\")\n",
                "print(f\"Trigrams: {get_ngrams(text, 3)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. PyTorch/Sklearn Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer as SklearnTfidf\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import cross_val_score\n",
                "\n",
                "# Sklearn TF-IDF\n",
                "sklearn_tfidf = SklearnTfidf(ngram_range=(1, 2), max_features=100)\n",
                "X_sklearn = sklearn_tfidf.fit_transform(corpus)\n",
                "\n",
                "print(f\"Sklearn TF-IDF shape: {X_sklearn.shape}\")\n",
                "print(f\"Sample features: {sklearn_tfidf.get_feature_names_out()[:10]}\")\n",
                "\n",
                "# Quick classifier\n",
                "clf = LogisticRegression()\n",
                "clf.fit(X_sklearn, labels)\n",
                "\n",
                "# Predict on new text\n",
                "new_text = [\"This film is absolutely amazing!\"]\n",
                "new_vec = sklearn_tfidf.transform(new_text)\n",
                "pred = clf.predict(new_vec)\n",
                "prob = clf.predict_proba(new_vec)\n",
                "\n",
                "print(f\"\\nPrediction for '{new_text[0]}': {'Positive' if pred[0] == 1 else 'Negative'}\")\n",
                "print(f\"Confidence: {prob[0][pred[0]]:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. ðŸ”¥ Real-World Usage\n",
                "\n",
                "### Why TF-IDF is Still Relevant (2024)\n",
                "\n",
                "| Use Case | Why TF-IDF |\n",
                "|----------|------------|\n",
                "| **Fast baseline** | Run it first! |\n",
                "| **Interpretable** | Show which words matter |\n",
                "| **Small data** | <1000 samples |\n",
                "| **Search/Retrieval** | BM25 is TF-IDF variant |\n",
                "| **Hybrid systems** | Combine with embeddings |\n",
                "\n",
                "### Industry Example: Elasticsearch\n",
                "```\n",
                "Elasticsearch uses BM25 (TF-IDF variant) for text search.\n",
                "Modern search: BM25 + Semantic Embeddings (hybrid)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Visualize TF-IDF for a document\n",
                "doc_idx = 0\n",
                "features = sklearn_tfidf.get_feature_names_out()\n",
                "tfidf_scores = X_sklearn[doc_idx].toarray().flatten()\n",
                "\n",
                "# Get top features\n",
                "top_indices = tfidf_scores.argsort()[-10:][::-1]\n",
                "top_features = [features[i] for i in top_indices]\n",
                "top_scores = [tfidf_scores[i] for i in top_indices]\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.barh(top_features[::-1], top_scores[::-1])\n",
                "plt.xlabel('TF-IDF Score')\n",
                "plt.title(f'Top TF-IDF Features for: \"{corpus[doc_idx][:30]}...\"')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Common Mistakes\n",
                "\n",
                "| Mistake | Fix |\n",
                "|---------|-----|\n",
                "| Not limiting vocab size | Use `max_features` |\n",
                "| Ignoring OOV words | Track unknown word rate |\n",
                "| Using on transformers | Transformers have their own tokenization |\n",
                "| Forgetting normalization | Always L2 normalize |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Questions\n",
                "\n",
                "**Q1: What does IDF do? Why is it useful?**\n",
                "<details><summary>Answer</summary>\n",
                "IDF down-weights common words (\"the\", \"is\") and up-weights rare, informative words. A word in every document has low IDF; a word in one document has high IDF.\n",
                "</details>\n",
                "\n",
                "**Q2: How would you handle OOV words in TF-IDF?**\n",
                "<details><summary>Answer</summary>\n",
                "Options: (1) Ignore them, (2) Use a special UNK token, (3) Use subword/character n-grams, (4) Use embeddings for OOV\n",
                "</details>\n",
                "\n",
                "**Q3: When would you use TF-IDF over BERT?**\n",
                "<details><summary>Answer</summary>\n",
                "Small data (<1000), need explainability, strict latency (<10ms), baseline comparison, hybrid with semantic search\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "- **Bag of Words**: Simple word counts, sparse vectors\n",
                "- **TF-IDF**: Weights words by importance (TF Ã— IDF)\n",
                "- **N-grams**: Capture local word patterns\n",
                "- **Use as baseline** before trying deep learning\n",
                "- **Still relevant** for search, small data, explainability"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Exercises\n",
                "\n",
                "1. Implement character-level TF-IDF\n",
                "2. Build a spam classifier using TF-IDF + Logistic Regression\n",
                "3. Compare unigrams, bigrams, and trigrams on sentiment task\n",
                "4. Implement BM25 (TF-IDF variant used in search)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. References\n",
                "\n",
                "- [TF-IDF Wikipedia](https://en.wikipedia.org/wiki/Tfâ€“idf)\n",
                "- [BM25: The Next Generation of TF-IDF](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables)\n",
                "- [Scikit-learn Text Features](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
                "\n",
                "---\n",
                "**Next:** [Module 03: Word Embeddings](../03_word_embeddings/03_word_embeddings.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}