{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 01: Text Preprocessing Pipeline\n",
                "\n",
                "**Master the Art of Cleaning and Preparing Text Data**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "By the end of this module, you will:\n",
                "\n",
                "- ‚úÖ Understand why preprocessing is critical for NLP\n",
                "- ‚úÖ Master text cleaning techniques (HTML, URLs, special chars)\n",
                "- ‚úÖ Implement tokenization from scratch (including BPE)\n",
                "- ‚úÖ Know when to use stemming vs lemmatization\n",
                "- ‚úÖ Build a complete preprocessing pipeline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 00: NLP Pipeline Overview](../00_nlp_pipeline/00_nlp_pipeline_overview.ipynb)\n",
                "- Basic Python and regex knowledge"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Intuition & Motivation\n",
                "\n",
                "### Why Preprocessing Matters\n",
                "\n",
                "> **\"Garbage in, garbage out\"**\n",
                "\n",
                "Real-world text is messy:\n",
                "- HTML tags: `<p>Hello</p>`\n",
                "- URLs: `https://example.com`\n",
                "- Emojis: `I love this! üòç`\n",
                "- Inconsistent casing: `AMAZING`, `amazing`, `Amazing`\n",
                "- Contractions: `don't`, `won't`, `I'm`\n",
                "\n",
                "### Impact on Model Performance\n",
                "\n",
                "| Preprocessing | Accuracy |\n",
                "|---------------|----------|\n",
                "| None | 78.2% |\n",
                "| Basic cleaning | 82.5% |\n",
                "| + Proper tokenization | 85.1% |\n",
                "| + Normalization | 86.3% |\n",
                "\n",
                "*Results vary by task - sometimes less preprocessing is better!*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import re\n",
                "import string\n",
                "from collections import Counter, defaultdict\n",
                "from typing import List, Dict, Tuple\n",
                "\n",
                "# Install if needed\n",
                "# !pip install nltk spacy beautifulsoup4\n",
                "\n",
                "import nltk\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "nltk.download('wordnet', quiet=True)\n",
                "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
                "\n",
                "print(\"Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Text Cleaning\n",
                "\n",
                "### 4.1 Lowercasing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def lowercase(text: str) -> str:\n",
                "    \"\"\"Convert text to lowercase.\n",
                "    \n",
                "    When to use: Most classification tasks\n",
                "    When NOT to use: \n",
                "        - NER (\"Apple\" company vs \"apple\" fruit)\n",
                "        - Sentiment with emphasis (\"AMAZING\" vs \"amazing\")\n",
                "    \"\"\"\n",
                "    return text.lower()\n",
                "\n",
                "# Example\n",
                "text = \"I LOVE This Product! It's AMAZING!\"\n",
                "print(f\"Original: {text}\")\n",
                "print(f\"Lowercased: {lowercase(text)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 HTML & URL Removal"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from bs4 import BeautifulSoup\n",
                "\n",
                "def remove_html(text: str) -> str:\n",
                "    \"\"\"Remove HTML tags from text.\"\"\"\n",
                "    soup = BeautifulSoup(text, \"html.parser\")\n",
                "    return soup.get_text(separator=' ')\n",
                "\n",
                "def remove_urls(text: str) -> str:\n",
                "    \"\"\"Remove URLs from text.\"\"\"\n",
                "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
                "    return re.sub(url_pattern, '', text)\n",
                "\n",
                "def remove_emails(text: str) -> str:\n",
                "    \"\"\"Remove email addresses.\"\"\"\n",
                "    email_pattern = r'\\S+@\\S+\\.\\S+'\n",
                "    return re.sub(email_pattern, '', text)\n",
                "\n",
                "# Examples\n",
                "html_text = \"<p>Hello <b>World</b></p> Visit https://example.com\"\n",
                "print(f\"Original: {html_text}\")\n",
                "print(f\"Clean: {remove_urls(remove_html(html_text))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Special Characters & Contractions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Contraction mapping\n",
                "CONTRACTIONS = {\n",
                "    \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
                "    \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\",\n",
                "    \"'ll\": \" will\", \"'ve\": \" have\", \"'m\": \" am\"\n",
                "}\n",
                "\n",
                "def expand_contractions(text: str) -> str:\n",
                "    \"\"\"Expand contractions in text.\"\"\"\n",
                "    for contraction, expansion in CONTRACTIONS.items():\n",
                "        text = text.replace(contraction, expansion)\n",
                "    return text\n",
                "\n",
                "def remove_special_chars(text: str, keep_punctuation: bool = False) -> str:\n",
                "    \"\"\"Remove special characters.\"\"\"\n",
                "    if keep_punctuation:\n",
                "        pattern = r'[^a-zA-Z0-9\\s.,!?\\'-]'\n",
                "    else:\n",
                "        pattern = r'[^a-zA-Z0-9\\s]'\n",
                "    return re.sub(pattern, '', text)\n",
                "\n",
                "# Example\n",
                "text = \"I can't believe it! You're amazing... üòç\"\n",
                "print(f\"Original: {text}\")\n",
                "print(f\"Expanded: {expand_contractions(text)}\")\n",
                "print(f\"Cleaned: {remove_special_chars(expand_contractions(text))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.4 Complete Cleaning Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text: str, \n",
                "               lower: bool = True,\n",
                "               remove_html_tags: bool = True,\n",
                "               remove_url: bool = True,\n",
                "               expand_contract: bool = True,\n",
                "               remove_special: bool = True) -> str:\n",
                "    \"\"\"Complete text cleaning pipeline.\"\"\"\n",
                "    \n",
                "    if remove_html_tags:\n",
                "        text = remove_html(text)\n",
                "    if remove_url:\n",
                "        text = remove_urls(text)\n",
                "    if expand_contract:\n",
                "        text = expand_contractions(text)\n",
                "    if remove_special:\n",
                "        text = remove_special_chars(text, keep_punctuation=True)\n",
                "    if lower:\n",
                "        text = lowercase(text)\n",
                "    \n",
                "    # Normalize whitespace\n",
                "    text = ' '.join(text.split())\n",
                "    return text\n",
                "\n",
                "# Test\n",
                "messy_text = \"\"\"\n",
                "<div>Check out https://example.com! \n",
                "I can't believe how AMAZING this is... üòç\n",
                "Contact: test@email.com</div>\n",
                "\"\"\"\n",
                "\n",
                "print(\"Original:\")\n",
                "print(messy_text)\n",
                "print(\"\\nCleaned:\")\n",
                "print(clean_text(messy_text))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Tokenization\n",
                "\n",
                "### 5.1 Types of Tokenization\n",
                "\n",
                "| Type | Example | Use Case |\n",
                "|------|---------|----------|\n",
                "| Whitespace | \"hello world\" ‚Üí [\"hello\", \"world\"] | Simple baseline |\n",
                "| Word | \"don't\" ‚Üí [\"do\", \"n't\"] | Traditional NLP |\n",
                "| Subword (BPE) | \"unhappy\" ‚Üí [\"un\", \"happy\"] | **Transformers** |\n",
                "| Character | \"hello\" ‚Üí [\"h\",\"e\",\"l\",\"l\",\"o\"] | Character-level models |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic tokenizers\n",
                "def whitespace_tokenize(text: str) -> List[str]:\n",
                "    \"\"\"Simple whitespace tokenization.\"\"\"\n",
                "    return text.split()\n",
                "\n",
                "def word_tokenize_simple(text: str) -> List[str]:\n",
                "    \"\"\"Word tokenization with punctuation handling.\"\"\"\n",
                "    # Split on non-alphanumeric, keep tokens\n",
                "    return re.findall(r\"\\b\\w+\\b|[.,!?;]\", text)\n",
                "\n",
                "# NLTK tokenizer\n",
                "from nltk.tokenize import word_tokenize as nltk_tokenize\n",
                "\n",
                "text = \"Hello, I'm learning NLP! It's amazing.\"\n",
                "print(f\"Whitespace: {whitespace_tokenize(text)}\")\n",
                "print(f\"Simple: {word_tokenize_simple(text)}\")\n",
                "print(f\"NLTK: {nltk_tokenize(text)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 BPE Tokenizer from Scratch (CRITICAL FOR TRANSFORMERS)\n",
                "\n",
                "**Byte Pair Encoding (BPE)** is used by GPT, RoBERTa, and many modern models.\n",
                "\n",
                "**Algorithm:**\n",
                "1. Start with character-level tokens\n",
                "2. Count all adjacent pairs\n",
                "3. Merge most frequent pair\n",
                "4. Repeat until vocabulary size reached"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BPETokenizer:\n",
                "    \"\"\"Byte Pair Encoding tokenizer from scratch.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size: int = 1000):\n",
                "        self.vocab_size = vocab_size\n",
                "        self.merges = {}  # (pair) -> merged_token\n",
                "        self.vocab = set()\n",
                "    \n",
                "    def _get_stats(self, words: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
                "        \"\"\"Count frequency of adjacent pairs.\"\"\"\n",
                "        pairs = defaultdict(int)\n",
                "        for word, freq in words.items():\n",
                "            symbols = word.split()\n",
                "            for i in range(len(symbols) - 1):\n",
                "                pairs[(symbols[i], symbols[i+1])] += freq\n",
                "        return pairs\n",
                "    \n",
                "    def _merge_vocab(self, pair: Tuple[str, str], words: Dict[str, int]) -> Dict[str, int]:\n",
                "        \"\"\"Merge all occurrences of pair in vocabulary.\"\"\"\n",
                "        new_words = {}\n",
                "        bigram = ' '.join(pair)\n",
                "        replacement = ''.join(pair)\n",
                "        \n",
                "        for word, freq in words.items():\n",
                "            new_word = word.replace(bigram, replacement)\n",
                "            new_words[new_word] = freq\n",
                "        return new_words\n",
                "    \n",
                "    def train(self, texts: List[str], num_merges: int = 100):\n",
                "        \"\"\"Train BPE on corpus.\"\"\"\n",
                "        # Initialize: split words into characters\n",
                "        word_freqs = Counter()\n",
                "        for text in texts:\n",
                "            for word in text.lower().split():\n",
                "                # Add end-of-word marker\n",
                "                word_freqs[' '.join(list(word)) + ' </w>'] += 1\n",
                "        \n",
                "        # Build initial vocab\n",
                "        for word in word_freqs:\n",
                "            for char in word.split():\n",
                "                self.vocab.add(char)\n",
                "        \n",
                "        # Iteratively merge most frequent pairs\n",
                "        for i in range(num_merges):\n",
                "            pairs = self._get_stats(word_freqs)\n",
                "            if not pairs:\n",
                "                break\n",
                "            \n",
                "            best_pair = max(pairs, key=pairs.get)\n",
                "            word_freqs = self._merge_vocab(best_pair, word_freqs)\n",
                "            \n",
                "            merged = ''.join(best_pair)\n",
                "            self.merges[best_pair] = merged\n",
                "            self.vocab.add(merged)\n",
                "            \n",
                "            if i < 5:  # Show first 5 merges\n",
                "                print(f\"Merge {i+1}: {best_pair} -> {merged}\")\n",
                "        \n",
                "        print(f\"\\nVocab size: {len(self.vocab)}\")\n",
                "    \n",
                "    def tokenize(self, text: str) -> List[str]:\n",
                "        \"\"\"Tokenize text using learned merges.\"\"\"\n",
                "        tokens = []\n",
                "        for word in text.lower().split():\n",
                "            word = ' '.join(list(word)) + ' </w>'\n",
                "            \n",
                "            # Apply merges\n",
                "            for pair, merged in self.merges.items():\n",
                "                bigram = ' '.join(pair)\n",
                "                word = word.replace(bigram, merged)\n",
                "            \n",
                "            tokens.extend(word.split())\n",
                "        return tokens\n",
                "\n",
                "# Train BPE\n",
                "corpus = [\n",
                "    \"the cat sat on the mat\",\n",
                "    \"the dog ran in the park\", \n",
                "    \"cats and dogs are pets\",\n",
                "    \"the quick brown fox\"\n",
                "]\n",
                "\n",
                "bpe = BPETokenizer()\n",
                "bpe.train(corpus, num_merges=20)\n",
                "\n",
                "# Test\n",
                "print(f\"\\nTokenized: {bpe.tokenize('the cat is sitting')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Normalization: Stemming vs Lemmatization\n",
                "\n",
                "| Technique | \"running\" | \"better\" | Speed | Quality |\n",
                "|-----------|-----------|----------|-------|--------|\n",
                "| Stemming | \"run\" | \"better\" | ‚ö° Fast | Lower |\n",
                "| Lemmatization | \"run\" | \"good\" | üê¢ Slow | Higher |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
                "\n",
                "stemmer = PorterStemmer()\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "words = [\"running\", \"runs\", \"ran\", \"better\", \"studies\", \"studying\"]\n",
                "\n",
                "print(f\"{'Word':<12} {'Stem':<12} {'Lemma':<12}\")\n",
                "print(\"-\" * 36)\n",
                "for word in words:\n",
                "    stem = stemmer.stem(word)\n",
                "    lemma = lemmatizer.lemmatize(word, pos='v')  # verb\n",
                "    print(f\"{word:<12} {stem:<12} {lemma:<12}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Stopword Removal"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from nltk.corpus import stopwords\n",
                "\n",
                "stop_words = set(stopwords.words('english'))\n",
                "\n",
                "def remove_stopwords(tokens: List[str], stop_words: set = stop_words) -> List[str]:\n",
                "    \"\"\"Remove stopwords from token list.\n",
                "    \n",
                "    WARNING: Don't use for sentiment analysis!\n",
                "    'not good' ‚Üí 'good' (loses negation)\n",
                "    \"\"\"\n",
                "    return [t for t in tokens if t.lower() not in stop_words]\n",
                "\n",
                "# Example\n",
                "tokens = [\"this\", \"is\", \"not\", \"a\", \"good\", \"movie\"]\n",
                "print(f\"Original: {tokens}\")\n",
                "print(f\"Without stopwords: {remove_stopwords(tokens)}\")\n",
                "print(\"\\n‚ö†Ô∏è Notice 'not' was removed - bad for sentiment!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. üî• Real-World Usage\n",
                "\n",
                "### Tool Comparison\n",
                "\n",
                "| Tool | Speed | Features | Best For |\n",
                "|------|-------|----------|----------|\n",
                "| **NLTK** | üê¢ | Educational | Learning |\n",
                "| **SpaCy** | ‚ö° | Production-ready | Industry |\n",
                "| **HuggingFace Tokenizers** | üöÄ | Rust-based | Transformers |\n",
                "\n",
                "### Production Tips\n",
                "\n",
                "1. **Always preserve original text** alongside processed\n",
                "2. **Log preprocessing statistics** (avg length, vocab coverage)\n",
                "3. **Make preprocessing deterministic** (set random seeds)\n",
                "4. **Handle edge cases**: empty strings, very long texts\n",
                "5. **For transformers**: use the model's tokenizer, not custom!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# HuggingFace Tokenizers - The industry standard\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "\n",
                "text = \"I'm learning about tokenization!\"\n",
                "tokens = tokenizer.tokenize(text)\n",
                "ids = tokenizer.encode(text)\n",
                "\n",
                "print(f\"Text: {text}\")\n",
                "print(f\"Tokens: {tokens}\")\n",
                "print(f\"IDs: {ids}\")\n",
                "print(f\"Decoded: {tokenizer.decode(ids)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Common Mistakes & Debugging\n",
                "\n",
                "### Mistake 1: Over-preprocessing for Transformers\n",
                "‚ùå Stemming + stopword removal + lowercasing before BERT\n",
                "\n",
                "‚úÖ **BERT has its own tokenizer** - just use it!\n",
                "\n",
                "### Mistake 2: Removing negations for Sentiment\n",
                "‚ùå \"This is not good\" ‚Üí \"good\" (after stopword removal)\n",
                "\n",
                "‚úÖ Keep negation words for sentiment analysis\n",
                "\n",
                "### Mistake 3: Inconsistent preprocessing\n",
                "‚ùå Different preprocessing for train vs test\n",
                "\n",
                "‚úÖ Use a pipeline that ensures consistency"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Questions\n",
                "\n",
                "**Q1: What's the difference between stemming and lemmatization?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- **Stemming**: Rule-based suffix stripping (fast, crude)\n",
                "- **Lemmatization**: Dictionary-based, returns valid words (slow, accurate)\n",
                "</details>\n",
                "\n",
                "**Q2: How does BPE tokenization work?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "1. Start with characters\n",
                "2. Find most frequent adjacent pair\n",
                "3. Merge into single token\n",
                "4. Repeat until vocab size reached\n",
                "</details>\n",
                "\n",
                "**Q3: Why do transformers use subword tokenization?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- Handles OOV words (\"unhappiness\" ‚Üí \"un\", \"happiness\")\n",
                "- Fixed vocabulary size\n",
                "- Balances character and word level\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "- **Text cleaning**: HTML, URLs, special chars, contractions\n",
                "- **Tokenization**: Whitespace ‚Üí Word ‚Üí Subword (BPE)\n",
                "- **Normalization**: Stemming (fast) vs Lemmatization (accurate)\n",
                "- **Stopwords**: Remove for topic modeling, KEEP for sentiment\n",
                "- **For transformers**: Use model's tokenizer, minimal preprocessing"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Exercises\n",
                "\n",
                "### Exercise 1: Build a preprocessing pipeline for tweets\n",
                "Handle: @mentions, #hashtags, emojis, URLs\n",
                "\n",
                "### Exercise 2: Implement WordPiece tokenizer\n",
                "Similar to BPE but uses likelihood instead of frequency\n",
                "\n",
                "### Exercise 3: Compare preprocessing impact\n",
                "Train sentiment classifier with different preprocessing levels"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. References\n",
                "\n",
                "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) - BPE Paper\n",
                "- [HuggingFace Tokenizers](https://huggingface.co/docs/tokenizers)\n",
                "- [SpaCy Documentation](https://spacy.io/usage/linguistic-features)\n",
                "\n",
                "---\n",
                "\n",
                "**Next:** [Module 02: Text Representation](../02_text_representation/02_text_representation.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}