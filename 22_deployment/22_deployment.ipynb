{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQv9z84HRABH"
      },
      "source": [
        "# Module 22: NLP Model Deployment\n",
        "\n",
        "**From Prototype to Production**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP5qlXkgRABI"
      },
      "source": [
        "## 1. Objectives\n",
        "\n",
        "- ✅ Build REST APIs with FastAPI\n",
        "- ✅ Optimize models for inference\n",
        "- ✅ Containerize with Docker\n",
        "- ✅ Understand deployment patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErRqC4FKRABJ"
      },
      "source": [
        "## 2. Prerequisites\n",
        "\n",
        "- [Module 21: RAG](../21_rag/21_rag.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juMZeIG_RABK"
      },
      "source": [
        "## 3. Deployment Overview\n",
        "\n",
        "### Deployment Stack\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────┐\n",
        "│           Load Balancer             │\n",
        "├─────────────────────────────────────┤\n",
        "│    FastAPI / Flask / Gradio         │\n",
        "├─────────────────────────────────────┤\n",
        "│    Model (PyTorch / ONNX)           │\n",
        "├─────────────────────────────────────┤\n",
        "│    Docker Container                 │\n",
        "├─────────────────────────────────────┤\n",
        "│    Cloud (AWS/GCP/Azure)            │\n",
        "└─────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### Framework Comparison\n",
        "\n",
        "| Framework | Best For | Latency |\n",
        "|-----------|----------|--------|\n",
        "| FastAPI | Production APIs | Low |\n",
        "| Flask | Simple apps | Medium |\n",
        "| Gradio | Demos/Prototypes | Medium |\n",
        "| Streamlit | Dashboards | Higher |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BVLuCsQRABL",
        "outputId": "1147c6cd-2379-404f-e22b-eb7e214d5a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "# Install: pip install fastapi uvicorn python-multipart\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhWVf0JWRABL"
      },
      "source": [
        "## 4. FastAPI Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5UZh6b3RABM",
        "outputId": "42a95877-b766-4ab7-c6ab-3766296d2512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "from fastapi import FastAPI\n",
            "from pydantic import BaseModel\n",
            "from transformers import pipeline\n",
            "\n",
            "app = FastAPI(title=\"NLP API\", version=\"1.0\")\n",
            "\n",
            "# Load model at startup\n",
            "classifier = None\n",
            "\n",
            "@app.on_event(\"startup\")\n",
            "async def load_model():\n",
            "    global classifier\n",
            "    classifier = pipeline(\"sentiment-analysis\")\n",
            "\n",
            "class TextRequest(BaseModel):\n",
            "    text: str\n",
            "\n",
            "class PredictionResponse(BaseModel):\n",
            "    label: str\n",
            "    score: float\n",
            "\n",
            "@app.post(\"/predict\", response_model=PredictionResponse)\n",
            "async def predict(request: TextRequest):\n",
            "    result = classifier(request.text)[0]\n",
            "    return PredictionResponse(\n",
            "        label=result[\"label\"],\n",
            "        score=result[\"score\"]\n",
            "    )\n",
            "\n",
            "@app.get(\"/health\")\n",
            "async def health():\n",
            "    return {\"status\": \"healthy\"}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# app.py - Basic FastAPI structure\n",
        "\n",
        "fastapi_code = '''\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline\n",
        "\n",
        "app = FastAPI(title=\"NLP API\", version=\"1.0\")\n",
        "\n",
        "# Load model at startup\n",
        "classifier = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def load_model():\n",
        "    global classifier\n",
        "    classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    label: str\n",
        "    score: float\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "async def predict(request: TextRequest):\n",
        "    result = classifier(request.text)[0]\n",
        "    return PredictionResponse(\n",
        "        label=result[\"label\"],\n",
        "        score=result[\"score\"]\n",
        "    )\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\"status\": \"healthy\"}\n",
        "'''\n",
        "\n",
        "print(fastapi_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQCq76HaRABM",
        "outputId": "154c3abd-be1e-47b7-d81d-07fb4c20b97b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API endpoints:\n",
            "  POST /predict - Classify text\n",
            "  GET /health - Health check\n",
            "  GET /docs - Swagger UI\n"
          ]
        }
      ],
      "source": [
        "# Run with: uvicorn app:app --reload\n",
        "# Access docs at: http://localhost:8000/docs\n",
        "\n",
        "# Test with curl:\n",
        "# curl -X POST \"http://localhost:8000/predict\" \\\n",
        "#      -H \"Content-Type: application/json\" \\\n",
        "#      -d '{\"text\": \"This is amazing!\"}'\n",
        "\n",
        "print(\"API endpoints:\")\n",
        "print(\"  POST /predict - Classify text\")\n",
        "print(\"  GET /health - Health check\")\n",
        "print(\"  GET /docs - Swagger UI\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSHuksCbRABM"
      },
      "source": [
        "## 5. Model Optimization\n",
        "\n",
        "### Optimization Techniques\n",
        "\n",
        "| Technique | Speedup | Memory | Quality |\n",
        "|-----------|---------|--------|--------|\n",
        "| FP16 | 2x | 50% | ~Same |\n",
        "| Dynamic Quant | 2-4x | 75% | Slight ↓ |\n",
        "| ONNX Runtime | 2-3x | Same | Same |\n",
        "| TorchScript | 1.5x | Same | Same |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGIJmZ8PRABN",
        "outputId": "fae40bb8-2554-4e48-e3c5-063afb04de3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: 525.2 KB\n",
            "Quantized: 135.3 KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2506707605.py:17: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = quantize_dynamic(\n"
          ]
        }
      ],
      "source": [
        "# Dynamic Quantization\n",
        "from torch.quantization import quantize_dynamic\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(512, 256)\n",
        "        self.linear2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(torch.relu(self.linear1(x)))\n",
        "\n",
        "model = SimpleModel()\n",
        "\n",
        "# Quantize\n",
        "quantized_model = quantize_dynamic(\n",
        "    model,\n",
        "    {nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Size comparison\n",
        "import os\n",
        "torch.save(model.state_dict(), \"model.pt\")\n",
        "torch.save(quantized_model.state_dict(), \"model_quant.pt\")\n",
        "print(f\"Original: {os.path.getsize('model.pt') / 1024:.1f} KB\")\n",
        "print(f\"Quantized: {os.path.getsize('model_quant.pt') / 1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWLgbMuORABN",
        "outputId": "24c018f2-267e-4d57-88cf-8a706e2398ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TorchScript output shape: torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "# TorchScript Export\n",
        "\n",
        "model.eval()\n",
        "example_input = torch.randn(1, 512)\n",
        "\n",
        "# Trace the model\n",
        "traced_model = torch.jit.trace(model, example_input)\n",
        "traced_model.save(\"model_traced.pt\")\n",
        "\n",
        "# Load and use\n",
        "loaded = torch.jit.load(\"model_traced.pt\")\n",
        "output = loaded(example_input)\n",
        "print(f\"TorchScript output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsaf3-NBRABN"
      },
      "source": [
        "## 6. ONNX Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R14ZnhJMRABN",
        "outputId": "575dd84e-1e83-45cc-9c91-722440bc6ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.20.1)\n",
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
            "Requirement already satisfied: onnx_ir<2,>=0.1.15 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.1.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (25.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-565014769.py:7: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
            "  torch.onnx.export(\n",
            "W0202 16:57:21.309000 136 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0202 16:57:21.311000 136 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0202 16:57:21.313000 136 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
            "W0202 16:57:21.315000 136 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `SimpleModel([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `SimpleModel([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ✅\n",
            "Model exported to ONNX!\n"
          ]
        }
      ],
      "source": [
        "# Install missing dependencies\n",
        "!pip install onnx onnxscript\n",
        "\n",
        "import torch\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    example_input,\n",
        "    \"model.onnx\",\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\n",
        "        \"input\": {0: \"batch_size\"},\n",
        "        \"output\": {0: \"batch_size\"}\n",
        "    }\n",
        ")\n",
        "print(\"Model exported to ONNX!\")\n",
        "\n",
        "# For HuggingFace models:\n",
        "# from transformers.onnx import export\n",
        "# export(tokenizer, model, config, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvmp0-wDRABO",
        "outputId": "2989d5f7-dfb6-4f75-da50-4d4d9d346f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.12.19)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2\n",
            "ONNX output shape: (1, 10)\n"
          ]
        }
      ],
      "source": [
        "# ONNX Runtime Inference\n",
        "!pip install onnxruntime\n",
        "\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "session = ort.InferenceSession(\"model.onnx\")\n",
        "input_name = session.get_inputs()[0].name\n",
        "\n",
        "# Run inference\n",
        "result = session.run(\n",
        "    None,\n",
        "    {input_name: example_input.numpy()}\n",
        ")\n",
        "print(f\"ONNX output shape: {result[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62epKBbRABO"
      },
      "source": [
        "## 7. Docker Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL_5cIrGRABO",
        "outputId": "4ec27433-fe51-4812-81d4-bb8ca5d0af01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FROM python:3.10-slim\n",
            "\n",
            "WORKDIR /app\n",
            "\n",
            "# Install dependencies\n",
            "COPY requirements.txt .\n",
            "RUN pip install --no-cache-dir -r requirements.txt\n",
            "\n",
            "# Copy application\n",
            "COPY app.py .\n",
            "COPY model/ ./model/\n",
            "\n",
            "# Expose port\n",
            "EXPOSE 8000\n",
            "\n",
            "# Run\n",
            "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Dockerfile\n",
        "\n",
        "dockerfile = '''\n",
        "FROM python:3.10-slim\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "# Install dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy application\n",
        "COPY app.py .\n",
        "COPY model/ ./model/\n",
        "\n",
        "# Expose port\n",
        "EXPOSE 8000\n",
        "\n",
        "# Run\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "'''\n",
        "\n",
        "print(dockerfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxSmcpMgRABO",
        "outputId": "3baf069c-c6e3-4595-b53b-d72c73b121a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "fastapi==0.104.0\n",
            "uvicorn==0.24.0\n",
            "transformers==4.35.0\n",
            "torch==2.1.0\n",
            "pydantic==2.5.0\n",
            "\n",
            "\n",
            "# Build: docker build -t nlp-api .\n",
            "# Run: docker run -p 8000:8000 nlp-api\n"
          ]
        }
      ],
      "source": [
        "# requirements.txt\n",
        "\n",
        "requirements = '''\n",
        "fastapi==0.104.0\n",
        "uvicorn==0.24.0\n",
        "transformers==4.35.0\n",
        "torch==2.1.0\n",
        "pydantic==2.5.0\n",
        "'''\n",
        "\n",
        "print(requirements)\n",
        "print(\"\\n# Build: docker build -t nlp-api .\")\n",
        "print(\"# Run: docker run -p 8000:8000 nlp-api\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgTCcE26RABO"
      },
      "source": [
        "## 8. Deployment Patterns\n",
        "\n",
        "### Common Patterns\n",
        "\n",
        "| Pattern | Use Case |\n",
        "|---------|----------|\n",
        "| Sync API | Low latency, real-time |\n",
        "| Async Queue | Batch processing |\n",
        "| Serverless | Variable load |\n",
        "| Model Server | Multi-model serving |\n",
        "\n",
        "### Scaling Considerations\n",
        "\n",
        "1. **Horizontal**: Multiple replicas behind load balancer\n",
        "2. **Batching**: Combine requests for GPU efficiency\n",
        "3. **Caching**: Cache embeddings/predictions\n",
        "4. **GPU Sharing**: Use NVIDIA Triton for multi-model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYgDdqC9RABO"
      },
      "source": [
        "## 9. Interview Questions\n",
        "\n",
        "**Q1: How do you reduce model latency?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "1. Quantization (INT8/FP16)\n",
        "2. ONNX Runtime / TensorRT\n",
        "3. Distillation to smaller model\n",
        "4. Batching requests\n",
        "5. Caching frequent predictions\n",
        "</details>\n",
        "\n",
        "**Q2: What's the difference between TorchScript trace vs script?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "- `trace`: Records operations from example input. Doesn't capture control flow.\n",
        "- `script`: Analyzes Python code directly. Handles control flow but more restrictive.\n",
        "- Use trace for simple forward passes, script for complex logic.\n",
        "</details>\n",
        "\n",
        "**Q3: How do you handle model updates in production?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "1. Blue-green deployment: Run old/new in parallel\n",
        "2. Canary: Route % of traffic to new model\n",
        "3. A/B testing: Compare metrics\n",
        "4. Shadow mode: Run new model without serving\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBkVvlIqRABO"
      },
      "source": [
        "## 10. Summary\n",
        "\n",
        "- **FastAPI**: Production-ready REST APIs\n",
        "- **Optimization**: Quantization, ONNX, TorchScript\n",
        "- **Docker**: Containerized deployment\n",
        "- **Patterns**: Sync, async, serverless, batched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duzhI67IRABP"
      },
      "source": [
        "## 11. References\n",
        "\n",
        "- [FastAPI Docs](https://fastapi.tiangolo.com/)\n",
        "- [ONNX Runtime](https://onnxruntime.ai/)\n",
        "- [TorchServe](https://pytorch.org/serve/)\n",
        "- [Docker Best Practices](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)\n",
        "\n",
        "---\n",
        "**Next:** [Module 23: NLP Evaluation & Monitoring](../23_evaluation/23_evaluation.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}