{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 24: Multimodal Learning\n",
                "\n",
                "**Bridging Vision and Language**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- ✅ Understand multimodal architectures\n",
                "- ✅ Implement Vision Transformer (ViT) from scratch\n",
                "- ✅ Master CLIP for vision-language understanding\n",
                "- ✅ Build image captioning pipeline\n",
                "- ✅ Use HuggingFace multimodal models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 14: Transformer Architecture](../14_transformer_architecture/14_transformer_architecture.ipynb)\n",
                "- [Module 17: HuggingFace Ecosystem](../17_huggingface/17_huggingface.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. What is Multimodal Learning?\n",
                "\n",
                "### Definition\n",
                "\n",
                "Multimodal learning combines **multiple data types** (modalities) to create richer representations:\n",
                "\n",
                "```\n",
                "┌─────────────────────────────────────────────────────────┐\n",
                "│                    Multimodal AI                         │\n",
                "├─────────────────────────────────────────────────────────┤\n",
                "│                                                          │\n",
                "│   [Image]  ──┐                                          │\n",
                "│              ├──→  [Joint Embedding]  ──→  [Output]     │\n",
                "│   [Text]   ──┘         Space                            │\n",
                "│                                                          │\n",
                "└─────────────────────────────────────────────────────────┘\n",
                "```\n",
                "\n",
                "### Key Applications\n",
                "\n",
                "| Task | Input | Output |\n",
                "|------|-------|--------|\n",
                "| Image Captioning | Image | Text description |\n",
                "| Visual QA | Image + Question | Answer |\n",
                "| Text-to-Image | Text prompt | Generated image |\n",
                "| Image-Text Matching | Image + Text | Similarity score |\n",
                "| Document Understanding | Document image | Structured data |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "# !pip install torch torchvision transformers pillow timm\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "from PIL import Image\n",
                "import requests\n",
                "from io import BytesIO\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Vision Transformer (ViT) - Theory\n",
                "\n",
                "### The Key Insight\n",
                "\n",
                "**\"An image is worth 16x16 words\"** - Split image into patches and treat them like tokens!\n",
                "\n",
                "### Architecture Overview\n",
                "\n",
                "```\n",
                "┌─────────────────────────────────────────────────────────┐\n",
                "│                  Vision Transformer                      │\n",
                "├─────────────────────────────────────────────────────────┤\n",
                "│                                                          │\n",
                "│  Input Image (224×224×3)                                 │\n",
                "│       ↓                                                  │\n",
                "│  Split into Patches (14×14 = 196 patches of 16×16)      │\n",
                "│       ↓                                                  │\n",
                "│  Linear Projection (flatten 16×16×3 → 768)              │\n",
                "│       ↓                                                  │\n",
                "│  Add [CLS] Token + Positional Embeddings                │\n",
                "│       ↓                                                  │\n",
                "│  Transformer Encoder (L layers)                          │\n",
                "│       ↓                                                  │\n",
                "│  [CLS] Token → Classification Head                       │\n",
                "│                                                          │\n",
                "└─────────────────────────────────────────────────────────┘\n",
                "```\n",
                "\n",
                "### Patch Embedding Math\n",
                "\n",
                "For an image of size $H \\times W \\times C$:\n",
                "- Patch size: $P \\times P$\n",
                "- Number of patches: $N = \\frac{H \\times W}{P^2}$\n",
                "- Each patch: $P^2 \\cdot C$ values → projected to $D$ dimensions\n",
                "\n",
                "Example: 224×224 image with 16×16 patches = 196 patches"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PatchEmbedding(nn.Module):\n",
                "    \"\"\"Split image into patches and embed them.\n",
                "    \n",
                "    Args:\n",
                "        img_size: Input image size (assumes square)\n",
                "        patch_size: Size of each patch\n",
                "        in_channels: Number of input channels (3 for RGB)\n",
                "        embed_dim: Embedding dimension\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
                "        super().__init__()\n",
                "        self.img_size = img_size\n",
                "        self.patch_size = patch_size\n",
                "        self.n_patches = (img_size // patch_size) ** 2\n",
                "        \n",
                "        # Linear projection of flattened patches\n",
                "        # Conv2d with kernel=patch_size, stride=patch_size does the same\n",
                "        self.proj = nn.Conv2d(\n",
                "            in_channels, \n",
                "            embed_dim, \n",
                "            kernel_size=patch_size, \n",
                "            stride=patch_size\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"x: [batch, channels, height, width]\"\"\"\n",
                "        x = self.proj(x)  # [batch, embed_dim, n_patches_h, n_patches_w]\n",
                "        x = x.flatten(2)  # [batch, embed_dim, n_patches]\n",
                "        x = x.transpose(1, 2)  # [batch, n_patches, embed_dim]\n",
                "        return x\n",
                "\n",
                "# Test\n",
                "patch_embed = PatchEmbedding()\n",
                "img = torch.randn(1, 3, 224, 224)\n",
                "patches = patch_embed(img)\n",
                "print(f\"Input: {img.shape}\")\n",
                "print(f\"Patches: {patches.shape}  # [batch, 196 patches, 768 dim]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VisionTransformer(nn.Module):\n",
                "    \"\"\"Complete Vision Transformer implementation.\n",
                "    \n",
                "    This follows the original ViT paper architecture.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        img_size=224,\n",
                "        patch_size=16,\n",
                "        in_channels=3,\n",
                "        n_classes=1000,\n",
                "        embed_dim=768,\n",
                "        depth=12,\n",
                "        n_heads=12,\n",
                "        mlp_ratio=4.0,\n",
                "        dropout=0.1\n",
                "    ):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Patch embedding\n",
                "        self.patch_embed = PatchEmbedding(\n",
                "            img_size, patch_size, in_channels, embed_dim\n",
                "        )\n",
                "        n_patches = self.patch_embed.n_patches\n",
                "        \n",
                "        # Learnable [CLS] token\n",
                "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
                "        \n",
                "        # Positional embeddings (learnable)\n",
                "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
                "        self.pos_drop = nn.Dropout(dropout)\n",
                "        \n",
                "        # Transformer encoder blocks\n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=embed_dim,\n",
                "            nhead=n_heads,\n",
                "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
                "            dropout=dropout,\n",
                "            activation='gelu',\n",
                "            batch_first=True\n",
                "        )\n",
                "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
                "        \n",
                "        # Classification head\n",
                "        self.norm = nn.LayerNorm(embed_dim)\n",
                "        self.head = nn.Linear(embed_dim, n_classes)\n",
                "        \n",
                "        # Initialize weights\n",
                "        nn.init.normal_(self.pos_embed, std=0.02)\n",
                "        nn.init.normal_(self.cls_token, std=0.02)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"x: [batch, channels, height, width]\"\"\"\n",
                "        batch_size = x.shape[0]\n",
                "        \n",
                "        # Create patch embeddings\n",
                "        x = self.patch_embed(x)  # [batch, n_patches, embed_dim]\n",
                "        \n",
                "        # Prepend [CLS] token\n",
                "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
                "        x = torch.cat([cls_tokens, x], dim=1)  # [batch, n_patches+1, embed_dim]\n",
                "        \n",
                "        # Add positional embeddings\n",
                "        x = x + self.pos_embed\n",
                "        x = self.pos_drop(x)\n",
                "        \n",
                "        # Transformer encoder\n",
                "        x = self.transformer(x)\n",
                "        \n",
                "        # Classification: use [CLS] token output\n",
                "        x = self.norm(x[:, 0])  # Take first token\n",
                "        x = self.head(x)\n",
                "        \n",
                "        return x\n",
                "\n",
                "# Create ViT-Base\n",
                "vit = VisionTransformer(n_classes=10)  # 10 classes for demo\n",
                "print(f\"ViT parameters: {sum(p.numel() for p in vit.parameters()):,}\")\n",
                "\n",
                "# Test forward pass\n",
                "img = torch.randn(2, 3, 224, 224)\n",
                "output = vit(img)\n",
                "print(f\"Output: {output.shape}  # [batch, n_classes]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. CLIP - Contrastive Language-Image Pre-training\n",
                "\n",
                "### Theory\n",
                "\n",
                "CLIP learns a **joint embedding space** for images and text through contrastive learning:\n",
                "\n",
                "```\n",
                "┌────────────────────────────────────────────────────────────┐\n",
                "│                      CLIP Architecture                      │\n",
                "├────────────────────────────────────────────────────────────┤\n",
                "│                                                             │\n",
                "│   Image ──→ [Image Encoder (ViT/ResNet)] ──→ Image Embed   │\n",
                "│                                                    ↓        │\n",
                "│                                              Cosine Sim     │\n",
                "│                                                    ↑        │\n",
                "│   Text  ──→ [Text Encoder (Transformer)]  ──→ Text Embed   │\n",
                "│                                                             │\n",
                "└────────────────────────────────────────────────────────────┘\n",
                "\n",
                "Training: Match N images with their N correct captions\n",
                "          Minimize distance for matches, maximize for non-matches\n",
                "```\n",
                "\n",
                "### Contrastive Loss\n",
                "\n",
                "For a batch of N (image, text) pairs:\n",
                "\n",
                "$$\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log \\frac{\\exp(sim(I_i, T_i)/\\tau)}{\\sum_{j=1}^{N}\\exp(sim(I_i, T_j)/\\tau)}$$\n",
                "\n",
                "Where:\n",
                "- $sim(I, T)$ = cosine similarity between image and text embeddings\n",
                "- $\\tau$ = temperature parameter (typically 0.07)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleCLIP(nn.Module):\n",
                "    \"\"\"Simplified CLIP implementation for understanding.\n",
                "    \n",
                "    In practice, you'd use the pretrained CLIP from OpenAI.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, embed_dim=512, vocab_size=10000, max_seq_len=77):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Image encoder (simplified ViT)\n",
                "        self.image_encoder = nn.Sequential(\n",
                "            PatchEmbedding(embed_dim=embed_dim),\n",
                "            nn.TransformerEncoder(\n",
                "                nn.TransformerEncoderLayer(\n",
                "                    d_model=embed_dim, nhead=8, batch_first=True\n",
                "                ),\n",
                "                num_layers=6\n",
                "            )\n",
                "        )\n",
                "        self.image_proj = nn.Linear(embed_dim, embed_dim)\n",
                "        \n",
                "        # Text encoder\n",
                "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
                "        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
                "        self.text_encoder = nn.TransformerEncoder(\n",
                "            nn.TransformerEncoderLayer(\n",
                "                d_model=embed_dim, nhead=8, batch_first=True\n",
                "            ),\n",
                "            num_layers=6\n",
                "        )\n",
                "        self.text_proj = nn.Linear(embed_dim, embed_dim)\n",
                "        \n",
                "        # Learnable temperature\n",
                "        self.logit_scale = nn.Parameter(torch.ones([]) * math.log(1 / 0.07))\n",
                "    \n",
                "    def encode_image(self, image):\n",
                "        \"\"\"Encode images to embeddings.\"\"\"\n",
                "        x = self.image_encoder(image)\n",
                "        x = x.mean(dim=1)  # Global average pooling\n",
                "        x = self.image_proj(x)\n",
                "        return F.normalize(x, dim=-1)  # L2 normalize\n",
                "    \n",
                "    def encode_text(self, text_ids):\n",
                "        \"\"\"Encode text to embeddings.\"\"\"\n",
                "        x = self.token_embed(text_ids) + self.pos_embed[:, :text_ids.shape[1]]\n",
                "        x = self.text_encoder(x)\n",
                "        x = x[:, 0]  # Take [CLS] or first token\n",
                "        x = self.text_proj(x)\n",
                "        return F.normalize(x, dim=-1)  # L2 normalize\n",
                "    \n",
                "    def forward(self, image, text_ids):\n",
                "        \"\"\"Compute similarity matrix.\"\"\"\n",
                "        image_embeds = self.encode_image(image)\n",
                "        text_embeds = self.encode_text(text_ids)\n",
                "        \n",
                "        # Cosine similarity with temperature scaling\n",
                "        logit_scale = self.logit_scale.exp()\n",
                "        logits = logit_scale * image_embeds @ text_embeds.T\n",
                "        \n",
                "        return logits  # [batch_images, batch_texts]\n",
                "\n",
                "print(\"SimpleCLIP architecture ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clip_loss(logits):\n",
                "    \"\"\"Symmetric contrastive loss for CLIP.\n",
                "    \n",
                "    Args:\n",
                "        logits: Similarity matrix [batch, batch]\n",
                "    \n",
                "    Returns:\n",
                "        Average of image-to-text and text-to-image losses\n",
                "    \"\"\"\n",
                "    batch_size = logits.shape[0]\n",
                "    labels = torch.arange(batch_size, device=logits.device)\n",
                "    \n",
                "    # Image-to-text loss (which text matches each image?)\n",
                "    loss_i2t = F.cross_entropy(logits, labels)\n",
                "    \n",
                "    # Text-to-image loss (which image matches each text?)\n",
                "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
                "    \n",
                "    return (loss_i2t + loss_t2i) / 2\n",
                "\n",
                "# Example\n",
                "logits = torch.randn(4, 4)  # 4 images, 4 texts\n",
                "loss = clip_loss(logits)\n",
                "print(f\"CLIP Loss: {loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Using Pretrained CLIP (HuggingFace)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import CLIPProcessor, CLIPModel\n",
                "\n",
                "# Load pretrained CLIP\n",
                "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
                "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
                "\n",
                "print(f\"CLIP loaded! Image encoder: ViT-B/32\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download a sample image\n",
                "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\"\n",
                "response = requests.get(url)\n",
                "image = Image.open(BytesIO(response.content))\n",
                "\n",
                "# Candidate texts\n",
                "texts = [\n",
                "    \"a photo of a dog\",\n",
                "    \"a photo of a cat\",\n",
                "    \"a photo of a bird\",\n",
                "    \"a photo of a car\"\n",
                "]\n",
                "\n",
                "# Process inputs\n",
                "inputs = processor(\n",
                "    text=texts,\n",
                "    images=image,\n",
                "    return_tensors=\"pt\",\n",
                "    padding=True\n",
                ")\n",
                "\n",
                "# Get similarity scores\n",
                "with torch.no_grad():\n",
                "    outputs = model(**inputs)\n",
                "    logits_per_image = outputs.logits_per_image\n",
                "    probs = logits_per_image.softmax(dim=1)\n",
                "\n",
                "print(\"Zero-shot classification results:\")\n",
                "for text, prob in zip(texts, probs[0]):\n",
                "    print(f\"  '{text}': {prob:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Image Captioning\n",
                "\n",
                "### Architecture\n",
                "\n",
                "```\n",
                "Image ──→ [ViT Encoder] ──→ Image Features\n",
                "                                   ↓\n",
                "              [Cross-Attention in Decoder]\n",
                "                                   ↓\n",
                "                         [GPT-like Decoder] ──→ Caption\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
                "\n",
                "# Load image captioning model\n",
                "caption_model = VisionEncoderDecoderModel.from_pretrained(\n",
                "    \"nlpconnect/vit-gpt2-image-captioning\"\n",
                ")\n",
                "feature_extractor = ViTImageProcessor.from_pretrained(\n",
                "    \"nlpconnect/vit-gpt2-image-captioning\"\n",
                ")\n",
                "tokenizer = AutoTokenizer.from_pretrained(\n",
                "    \"nlpconnect/vit-gpt2-image-captioning\"\n",
                ")\n",
                "\n",
                "print(\"Image captioning model loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_caption(image, max_length=16):\n",
                "    \"\"\"Generate caption for an image.\"\"\"\n",
                "    pixel_values = feature_extractor(\n",
                "        images=image, return_tensors=\"pt\"\n",
                "    ).pixel_values\n",
                "    \n",
                "    output_ids = caption_model.generate(\n",
                "        pixel_values,\n",
                "        max_length=max_length,\n",
                "        num_beams=4\n",
                "    )\n",
                "    \n",
                "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
                "    return caption\n",
                "\n",
                "# Test with our image\n",
                "caption = generate_caption(image)\n",
                "print(f\"Generated caption: '{caption}'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visual Question Answering (VQA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
                "\n",
                "# Load VQA model (ViLT - Vision-and-Language Transformer)\n",
                "vqa_processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
                "vqa_model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
                "\n",
                "def answer_question(image, question):\n",
                "    \"\"\"Answer a question about an image.\"\"\"\n",
                "    inputs = vqa_processor(image, question, return_tensors=\"pt\")\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = vqa_model(**inputs)\n",
                "    \n",
                "    # Get top answer\n",
                "    idx = outputs.logits.argmax(-1).item()\n",
                "    return vqa_model.config.id2label[idx]\n",
                "\n",
                "# Test\n",
                "questions = [\n",
                "    \"What animal is in the image?\",\n",
                "    \"What color is the animal?\",\n",
                "    \"Is the animal sitting or standing?\"\n",
                "]\n",
                "\n",
                "print(\"Visual Question Answering:\")\n",
                "for q in questions:\n",
                "    answer = answer_question(image, q)\n",
                "    print(f\"  Q: {q}\")\n",
                "    print(f\"  A: {answer}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. BLIP-2 - State of the Art\n",
                "\n",
                "### Architecture\n",
                "\n",
                "BLIP-2 bridges frozen image encoders and LLMs with a lightweight Q-Former:\n",
                "\n",
                "```\n",
                "Image ──→ [Frozen ViT] ──→ [Q-Former] ──→ [Frozen LLM] ──→ Response\n",
                "                              ↑\n",
                "                        Learnable queries\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
                "\n",
                "# Load BLIP-2 (requires significant memory)\n",
                "# blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
                "# blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
                "#     \"Salesforce/blip2-opt-2.7b\", \n",
                "#     torch_dtype=torch.float16\n",
                "# )\n",
                "\n",
                "# Usage (if loaded):\n",
                "# inputs = blip_processor(images=image, text=\"Question: What is in the image?\", return_tensors=\"pt\")\n",
                "# output = blip_model.generate(**inputs)\n",
                "# print(blip_processor.decode(output[0], skip_special_tokens=True))\n",
                "\n",
                "print(\"BLIP-2 example (commented out due to memory requirements)\")\n",
                "print(\"Use Salesforce/blip2-opt-2.7b for state-of-the-art results\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Questions\n",
                "\n",
                "**Q1: How does ViT differ from CNNs for image processing?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- ViT: Splits image into patches, treats them as tokens, uses self-attention\n",
                "- CNN: Uses convolutional kernels that slide across image\n",
                "- ViT captures global context from start; CNN builds it hierarchically\n",
                "- ViT needs more data to train from scratch (no inductive bias for locality)\n",
                "</details>\n",
                "\n",
                "**Q2: Explain CLIP's contrastive learning objective.**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "CLIP maximizes cosine similarity between matching (image, text) pairs while minimizing similarity for non-matching pairs. This creates a shared embedding space where similar concepts (regardless of modality) are close together.\n",
                "</details>\n",
                "\n",
                "**Q3: What is zero-shot classification with CLIP?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Using CLIP to classify images without task-specific training:\n",
                "1. Encode image with image encoder\n",
                "2. Encode class names as text (e.g., \"a photo of a cat\")\n",
                "3. Compute cosine similarity\n",
                "4. Highest similarity = predicted class\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "| Model | Task | Key Insight |\n",
                "|-------|------|-------------|\n",
                "| ViT | Image classification | Images as sequences of patches |\n",
                "| CLIP | Vision-language | Contrastive learning for joint space |\n",
                "| VQA (ViLT) | Question answering | Fused image-text transformer |\n",
                "| BLIP-2 | Multi-task | Q-Former bridges vision & LLM |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. References\n",
                "\n",
                "- [ViT Paper](https://arxiv.org/abs/2010.11929)\n",
                "- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n",
                "- [BLIP-2 Paper](https://arxiv.org/abs/2301.12597)\n",
                "- [ViLT Paper](https://arxiv.org/abs/2102.03334)\n",
                "\n",
                "---\n",
                "**Next:** [Module 25: Stable Diffusion](../25_stable_diffusion/25_stable_diffusion.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}