{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFFn1ciP_1TJ"
      },
      "source": [
        "# Module 20: Fine-Tuning LLMs\n",
        "\n",
        "**LoRA, QLoRA, and Efficient Adaptation**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPnPekPr_1TK"
      },
      "source": [
        "## 1. Objectives\n",
        "\n",
        "- âœ… Understand full fine-tuning vs PEFT\n",
        "- âœ… Master LoRA (Low-Rank Adaptation)\n",
        "- âœ… Implement QLoRA for memory efficiency\n",
        "- âœ… Know when and how to fine-tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0MO8WUU_1TL"
      },
      "source": [
        "## 2. Prerequisites\n",
        "\n",
        "- [Module 19: Prompt Engineering](../19_prompt_engineering/19_prompt_engineering.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJeq7_gC_1TL"
      },
      "source": [
        "## 3. Fine-Tuning Landscape\n",
        "\n",
        "### Types of Fine-Tuning\n",
        "\n",
        "| Method | What Changes | Memory | Quality |\n",
        "|--------|-------------|--------|--------|\n",
        "| Full | All weights | Very High | Best |\n",
        "| LoRA | Low-rank adapters | Low | Great |\n",
        "| QLoRA | Quantized + LoRA | Very Low | Great |\n",
        "| Prefix Tuning | Soft prompts | Low | Good |\n",
        "\n",
        "### Decision Framework\n",
        "\n",
        "```\n",
        "Have 100+ GPU hours? â†’ Full fine-tuning\n",
        "Have 16GB+ VRAM?    â†’ LoRA\n",
        "Have 8GB VRAM?      â†’ QLoRA\n",
        "Just prototyping?   â†’ Prompt engineering first!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiYwM3sC_1TM",
        "outputId": "9b716bc8-d43c-4e3d-dc70-e96de7e7e1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# !pip install peft bitsandbytes accelerate transformers datasets trl\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiWB5Uyp_1TM"
      },
      "source": [
        "## 4. LoRA (Low-Rank Adaptation)\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "Instead of updating full weight matrix W, learn low-rank update:\n",
        "\n",
        "$$W' = W + BA$$\n",
        "\n",
        "Where:\n",
        "- W: Original weights (frozen)\n",
        "- B: (d Ã— r) matrix\n",
        "- A: (r Ã— k) matrix\n",
        "- r << min(d, k) (typically 8-64)\n",
        "\n",
        "### Memory Savings\n",
        "\n",
        "```\n",
        "Original: d Ã— k parameters\n",
        "LoRA: r Ã— (d + k) parameters\n",
        "\n",
        "Example: 4096 Ã— 4096 = 16M params\n",
        "LoRA (r=16): 16 Ã— 8192 = 131K params (0.8%!)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz-ED08o_1TN",
        "outputId": "e89ede4e-a98b-4019-cb3a-909bbb4067f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Config: r=16, alpha=32\n",
            "Target modules: {'q_proj', 'v_proj'}\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                         # Rank\n",
        "    lora_alpha=32,                # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers to adapt\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "print(f\"LoRA Config: r={lora_config.r}, alpha={lora_config.lora_alpha}\")\n",
        "print(f\"Target modules: {lora_config.target_modules}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_iikJqo_1TN"
      },
      "source": [
        "## 5. QLoRA Setup\n",
        "\n",
        "QLoRA = 4-bit Quantization + LoRA\n",
        "\n",
        "Enables fine-tuning 65B models on single GPU!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPlcMRYv_1TN",
        "outputId": "f25fa19c-9135-4064-ad53-e4fb15ebd7ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QLoRA config ready!\n",
            "This reduces 7B model from 28GB to ~4GB\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes accelerate peft\n",
        "\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Normalized Float 4\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True  # Nested quantization\n",
        ")\n",
        "\n",
        "print(\"QLoRA config ready!\")\n",
        "print(\"This reduces 7B model from 28GB to ~4GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMA05vVc_1TO"
      },
      "source": [
        "## 6. Complete Fine-Tuning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4NaW6-r_1TO",
        "outputId": "26df8bd4-14cc-44e0-8bb4-cc94dde14903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QLoRA training setup function ready!\n"
          ]
        }
      ],
      "source": [
        "# Full QLoRA fine-tuning example (pseudocode - needs GPU)\n",
        "\n",
        "def setup_qlora_training(model_name, dataset):\n",
        "    \"\"\"Complete QLoRA setup.\"\"\"\n",
        "\n",
        "    # 1. Load quantized model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # 2. Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # 3. Print trainable parameters\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"QLoRA training setup function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUWEKB-Z_1TO",
        "outputId": "9330d0ff-08de-48e7-cf83-b41b2c72dee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments configured!\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Training arguments for LoRA\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Effective batch = 16\n",
        "    learning_rate=2e-4,  # Higher LR for LoRA\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,  # Mixed precision\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3RdsIK_1TO"
      },
      "source": [
        "## 7. Data Formatting\n",
        "\n",
        "### Instruction Format (Alpaca Style)\n",
        "\n",
        "```\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "{output}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bey150uP_1TP",
        "outputId": "ea2a83be-bccf-492e-bb95-ca14d7011f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Summarize the following text.\n",
            "\n",
            "### Input:\n",
            "Machine learning is a subset of AI that enables systems to learn from data.\n",
            "\n",
            "### Response:\n",
            "ML is AI that learns from data.\n"
          ]
        }
      ],
      "source": [
        "def format_instruction(sample):\n",
        "    \"\"\"Format sample for instruction tuning.\"\"\"\n",
        "\n",
        "    if sample.get('input'):\n",
        "        return f\"\"\"### Instruction:\n",
        "{sample['instruction']}\n",
        "\n",
        "### Input:\n",
        "{sample['input']}\n",
        "\n",
        "### Response:\n",
        "{sample['output']}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"### Instruction:\n",
        "{sample['instruction']}\n",
        "\n",
        "### Response:\n",
        "{sample['output']}\"\"\"\n",
        "\n",
        "# Example\n",
        "sample = {\n",
        "    'instruction': 'Summarize the following text.',\n",
        "    'input': 'Machine learning is a subset of AI that enables systems to learn from data.',\n",
        "    'output': 'ML is AI that learns from data.'\n",
        "}\n",
        "print(format_instruction(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxK4qinz_1TP"
      },
      "source": [
        "## 8. ðŸ”¥ Real-World Usage\n",
        "\n",
        "### When to Fine-Tune\n",
        "\n",
        "| Scenario | Approach |\n",
        "|----------|----------|\n",
        "| Need specific format | LoRA |\n",
        "| Domain adaptation | QLoRA |\n",
        "| Better at task | Prompt first, then LoRA |\n",
        "| New capabilities | Full fine-tune |\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Start with prompting** - often sufficient\n",
        "2. **Use quality data** - 1000 good examples > 10000 bad\n",
        "3. **Validate on held-out set**\n",
        "4. **Monitor for overfitting**\n",
        "5. **Merge weights for deployment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axN8PuV0_1TP"
      },
      "source": [
        "## 9. Interview Questions\n",
        "\n",
        "**Q1: What is LoRA and why is it memory efficient?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "LoRA learns low-rank decomposition (BA) instead of full weight updates. With r=16, it uses <1% of parameters while achieving similar quality to full fine-tuning.\n",
        "</details>\n",
        "\n",
        "**Q2: What is QLoRA?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "QLoRA combines 4-bit quantization (NF4) with LoRA. Frozen weights are 4-bit, LoRA adapters are trained in FP16/BF16. Enables 65B fine-tuning on 48GB VRAM.\n",
        "</details>\n",
        "\n",
        "**Q3: When NOT to fine-tune?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "- Task solvable by prompting\n",
        "- Very small datasets (<100 examples)\n",
        "- No evaluation data\n",
        "- Time-sensitive deployment\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2ldYHrD_1TR"
      },
      "source": [
        "## 10. Summary\n",
        "\n",
        "- **Full Fine-Tuning**: All weights, best results, high cost\n",
        "- **LoRA**: Low-rank adapters, great quality, low memory\n",
        "- **QLoRA**: 4-bit + LoRA, very low memory\n",
        "- **Best Practice**: Prompt â†’ LoRA â†’ Full fine-tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2T1tmXT_1TR"
      },
      "source": [
        "## 11. References\n",
        "\n",
        "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
        "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
        "- [PEFT Library](https://github.com/huggingface/peft)\n",
        "- [TRL Library](https://github.com/huggingface/trl)\n",
        "\n",
        "---\n",
        "**Next:** [Module 21: RAG (Retrieval-Augmented Generation)](../21_rag/21_rag.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}