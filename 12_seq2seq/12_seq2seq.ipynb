{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC628M6H7gdC"
      },
      "source": [
        "# Module 12: Sequence-to-Sequence\n",
        "\n",
        "**Encoder-Decoder Architecture for Translation**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEVZHoKC7gdD"
      },
      "source": [
        "## 1. Objectives\n",
        "\n",
        "- âœ… Understand encoder-decoder architecture\n",
        "- âœ… Implement Seq2Seq from scratch\n",
        "- âœ… Use teacher forcing\n",
        "- âœ… Implement greedy and beam search decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNPFxUqO7gdE"
      },
      "source": [
        "## 2. Prerequisites\n",
        "\n",
        "- [Module 11: Language Modeling](../11_language_modeling/11_language_modeling.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV_BwTrl7gdE"
      },
      "source": [
        "## 3. Seq2Seq Architecture\n",
        "\n",
        "```\n",
        "        Encoder                      Decoder\n",
        "        \n",
        "  \"hello world\"               \"<SOS> bonjour monde <EOS>\"\n",
        "        â†“                              â†“\n",
        "  [Embed] â†’ [LSTM] â†’ context â†’ [LSTM] â†’ [Output]\n",
        "                      vector\n",
        "```\n",
        "\n",
        "### Key Insight\n",
        "- **Encoder**: Compress input into context vector\n",
        "- **Decoder**: Generate output conditioned on context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEw6sqih7gdE",
        "outputId": "190314bc-416d-4f67-f195-ceac3d205cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDriK2Ys7gdF"
      },
      "source": [
        "## 4. Sample Data (Toy Translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkllZhom7gdF",
        "outputId": "827e78e1-b86a-42d4-d11a-131b0538e41d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source vocab: 9, Target vocab: 7\n"
          ]
        }
      ],
      "source": [
        "# Toy English-French pairs\n",
        "pairs = [\n",
        "    (\"hello\", \"bonjour\"),\n",
        "    (\"world\", \"monde\"),\n",
        "    (\"cat\", \"chat\"),\n",
        "    (\"dog\", \"chien\"),\n",
        "    (\"good morning\", \"bonjour\"),\n",
        "]\n",
        "\n",
        "# Build vocabularies\n",
        "SOS_TOKEN, EOS_TOKEN, PAD_TOKEN = 0, 1, 2\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    vocab = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2}\n",
        "    for sent in sentences:\n",
        "        for word in sent.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "src_vocab = build_vocab([p[0] for p in pairs])\n",
        "tgt_vocab = build_vocab([p[1] for p in pairs])\n",
        "tgt_idx2word = {v: k for k, v in tgt_vocab.items()}\n",
        "\n",
        "print(f\"Source vocab: {len(src_vocab)}, Target vocab: {len(tgt_vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c05UMNn87gdF"
      },
      "source": [
        "## 5. Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcSHRwBw7gdF",
        "outputId": "c8a3dfcd-c364-434f-e486-17d947009573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context shapes - h: torch.Size([1, 1, 128]), c: torch.Size([1, 1, 128])\n"
          ]
        }
      ],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"LSTM Encoder.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_TOKEN)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq)\n",
        "        embedded = self.dropout(self.embedding(x))  # (batch, seq, embed)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        # hidden, cell: (layers, batch, hidden)\n",
        "        return hidden, cell\n",
        "\n",
        "encoder = Encoder(len(src_vocab), embed_dim=64, hidden_dim=128)\n",
        "x = torch.tensor([[src_vocab.get(w, 2) for w in \"hello\".split()]])\n",
        "h, c = encoder(x)\n",
        "print(f\"Context shapes - h: {h.shape}, c: {c.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIdCyh4S7gdF"
      },
      "source": [
        "## 6. Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b9d55nt7gdF",
        "outputId": "7632a1a9-4777-489b-8dc7-fa5473c4fcbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output: torch.Size([1, 7])\n"
          ]
        }
      ],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"LSTM Decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_TOKEN)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        # x: (batch, 1) single token\n",
        "        embedded = self.dropout(self.embedding(x))  # (batch, 1, embed)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        logits = self.fc(output.squeeze(1))  # (batch, vocab)\n",
        "        return logits, hidden, cell\n",
        "\n",
        "decoder = Decoder(len(tgt_vocab), embed_dim=64, hidden_dim=128)\n",
        "start = torch.tensor([[SOS_TOKEN]])\n",
        "logits, h, c = decoder(start, h, c)\n",
        "print(f\"Decoder output: {logits.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7KGpv6n7gdF"
      },
      "source": [
        "## 7. Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAAWV3sM7gdG",
        "outputId": "288a5d88-f034-4092-dd32-928fbd3d277d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 200,583\n"
          ]
        }
      ],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"Complete Seq2Seq model.\"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Training forward pass with teacher forcing.\n",
        "\n",
        "        Args:\n",
        "            src: (batch, src_len)\n",
        "            tgt: (batch, tgt_len) including <SOS>\n",
        "            teacher_forcing_ratio: Probability of using true target\n",
        "        \"\"\"\n",
        "        batch_size = src.size(0)\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        # Encode\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # Store outputs\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(src.device)\n",
        "\n",
        "        # First input is <SOS>\n",
        "        decoder_input = tgt[:, 0:1]  # (batch, 1)\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            logits, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            # Teacher forcing\n",
        "            use_teacher = random.random() < teacher_forcing_ratio\n",
        "            top1 = logits.argmax(dim=1, keepdim=True)\n",
        "            decoder_input = tgt[:, t:t+1] if use_teacher else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def translate(self, src, max_len=20):\n",
        "        \"\"\"Inference: greedy decoding.\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            hidden, cell = self.encoder(src)\n",
        "            decoder_input = torch.tensor([[SOS_TOKEN]]).to(src.device)\n",
        "\n",
        "            output_tokens = []\n",
        "            for _ in range(max_len):\n",
        "                logits, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "                top1 = logits.argmax(dim=1)\n",
        "\n",
        "                if top1.item() == EOS_TOKEN:\n",
        "                    break\n",
        "\n",
        "                output_tokens.append(top1.item())\n",
        "                decoder_input = top1.unsqueeze(1)\n",
        "\n",
        "            return output_tokens\n",
        "\n",
        "# Create model\n",
        "encoder = Encoder(len(src_vocab), 64, 128)\n",
        "decoder = Decoder(len(tgt_vocab), 64, 128)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi9NzxLb7gdG"
      },
      "source": [
        "## 8. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBT1aelU7gdG",
        "outputId": "bb2adf56-d9f6-4f88-f142-fd749c254de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50: Loss=0.0316\n",
            "Epoch 100: Loss=0.0053\n",
            "Epoch 150: Loss=0.0028\n",
            "Epoch 200: Loss=0.0019\n"
          ]
        }
      ],
      "source": [
        "def prepare_batch(pairs, src_vocab, tgt_vocab):\n",
        "    \"\"\"Prepare training batch.\"\"\"\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src, tgt in pairs:\n",
        "        src_tokens = [src_vocab.get(w, PAD_TOKEN) for w in src.split()]\n",
        "        tgt_tokens = [SOS_TOKEN] + [tgt_vocab.get(w, PAD_TOKEN) for w in tgt.split()] + [EOS_TOKEN]\n",
        "        src_batch.append(src_tokens)\n",
        "        tgt_batch.append(tgt_tokens)\n",
        "\n",
        "    # Pad\n",
        "    max_src = max(len(s) for s in src_batch)\n",
        "    max_tgt = max(len(t) for t in tgt_batch)\n",
        "\n",
        "    src_batch = [s + [PAD_TOKEN] * (max_src - len(s)) for s in src_batch]\n",
        "    tgt_batch = [t + [PAD_TOKEN] * (max_tgt - len(t)) for t in tgt_batch]\n",
        "\n",
        "    return torch.tensor(src_batch), torch.tensor(tgt_batch)\n",
        "\n",
        "# Training\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "\n",
        "src, tgt = prepare_batch(pairs, src_vocab, tgt_vocab)\n",
        "src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(src, tgt, teacher_forcing_ratio=0.5)\n",
        "\n",
        "    # outputs: (batch, tgt_len, vocab), tgt: (batch, tgt_len)\n",
        "    # Skip first target (<SOS>)\n",
        "    loss = criterion(outputs[:, 1:].reshape(-1, len(tgt_vocab)), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jL9Egfz7gdG"
      },
      "source": [
        "## 9. Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARsbXd_47gdG",
        "outputId": "bfb762d2-38bd-4b88-94d2-9022d4cb26a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello â†’ bonjour (expected: bonjour)\n",
            "world â†’ monde (expected: monde)\n",
            "cat â†’ chat (expected: chat)\n",
            "dog â†’ chien (expected: chien)\n",
            "good morning â†’ bonjour (expected: bonjour)\n"
          ]
        }
      ],
      "source": [
        "# Test translation\n",
        "def translate(model, sentence, src_vocab, tgt_idx2word):\n",
        "    tokens = [src_vocab.get(w, PAD_TOKEN) for w in sentence.split()]\n",
        "    src = torch.tensor([tokens]).to(device)\n",
        "    output_tokens = model.translate(src)\n",
        "    return ' '.join([tgt_idx2word.get(t, '<UNK>') for t in output_tokens])\n",
        "\n",
        "for src, expected in pairs:\n",
        "    result = translate(model, src, src_vocab, tgt_idx2word)\n",
        "    print(f\"{src} â†’ {result} (expected: {expected})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDj1FJ2d7gdG"
      },
      "source": [
        "## 10. ðŸ”¥ Real-World Usage\n",
        "\n",
        "### Evolution\n",
        "\n",
        "| Year | Model | Innovation |\n",
        "|------|-------|------------|\n",
        "| 2014 | Seq2Seq | Encoder-Decoder |\n",
        "| 2015 | Attention | Look at relevant parts |\n",
        "| 2017 | Transformer | Self-attention only |\n",
        "| 2020+ | BART, T5, mBART | Pretrained Seq2Seq |\n",
        "\n",
        "### Modern Practice\n",
        "- Use pretrained: `transformers.AutoModelForSeq2SeqLM`\n",
        "- Fine-tune on your task\n",
        "- Examples: T5, BART, mT5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttZcuoId7gdG"
      },
      "source": [
        "## 11. Interview Questions\n",
        "\n",
        "**Q1: What is teacher forcing?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "During training, use ground truth previous token as decoder input instead of predicted token. Speeds up training but can cause exposure bias (train-test mismatch).\n",
        "</details>\n",
        "\n",
        "**Q2: What is the bottleneck problem in Seq2Seq?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "All source information must be compressed into a single fixed-size context vector. Long sequences lose information. Solution: Attention mechanism.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDmzqQML7gdG"
      },
      "source": [
        "## 12. Summary\n",
        "\n",
        "- **Encoder**: Compress input into context vector\n",
        "- **Decoder**: Generate output token by token\n",
        "- **Teacher forcing**: Use true targets during training\n",
        "- **Limitation**: Bottleneck in context vector â†’ fixed by Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZarJswwc7gdG"
      },
      "source": [
        "## 13. References\n",
        "\n",
        "- [Seq2Seq Paper (2014)](https://arxiv.org/abs/1409.3215)\n",
        "- [PyTorch Seq2Seq Tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
        "\n",
        "---\n",
        "**Next:** [Module 13: Attention Mechanism](../13_attention/13_attention.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}