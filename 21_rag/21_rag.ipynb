{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyfSnUdV_3gD"
      },
      "source": [
        "# Module 21: Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "**Grounding LLMs with External Knowledge**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg9N1ia8_3gF"
      },
      "source": [
        "## 1. Objectives\n",
        "\n",
        "- ✅ Understand RAG architecture\n",
        "- ✅ Implement document chunking\n",
        "- ✅ Create embeddings and vector stores\n",
        "- ✅ Build retrieval pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gltsFJjz_3gF"
      },
      "source": [
        "## 2. Prerequisites\n",
        "\n",
        "- [Module 20: Fine-Tuning LLMs](../20_finetuning/20_finetuning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjfWncND_3gF"
      },
      "source": [
        "## 3. Why RAG?\n",
        "\n",
        "### LLM Limitations\n",
        "\n",
        "| Problem | RAG Solution |\n",
        "|---------|-------------|\n",
        "| Knowledge cutoff | Retrieve current docs |\n",
        "| Hallucinations | Ground in sources |\n",
        "| No private data | Index your documents |\n",
        "| Token limits | Retrieve relevant chunks |\n",
        "\n",
        "### RAG Architecture\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────┐\n",
        "│                      RAG Pipeline                    │\n",
        "├─────────────────────────────────────────────────────┤\n",
        "│  [Documents] → [Chunk] → [Embed] → [Vector Store]   │\n",
        "│                                          ↓           │\n",
        "│  [Query] → [Embed] → [Similarity Search]            │\n",
        "│                              ↓                       │\n",
        "│  [Retrieved Chunks] + [Query] → [LLM] → [Answer]    │\n",
        "└─────────────────────────────────────────────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8wSll8zo_3gG"
      },
      "outputs": [],
      "source": [
        "# Install: pip install sentence-transformers chromadb langchain\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKp8ysfW_3gG"
      },
      "source": [
        "## 4. Document Chunking\n",
        "\n",
        "### Chunking Strategies\n",
        "\n",
        "| Strategy | Pros | Cons |\n",
        "|----------|------|------|\n",
        "| Fixed size | Simple | Breaks context |\n",
        "| Sentence | Semantic | Variable size |\n",
        "| Recursive | Best quality | Complex |\n",
        "| Semantic | Preserves meaning | Slow |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UcoiHjM_3gG",
        "outputId": "87b1ca21-a4e4-4968-ba96-8da348be2c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 10 chunks\n",
            "First chunk: 'RAG combines retrieval and generation. RAG combine...'\n"
          ]
        }
      ],
      "source": [
        "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "    \"\"\"Simple chunking with overlap.\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap  # Overlap for context continuity\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Example\n",
        "sample_text = \"RAG combines retrieval and generation. \" * 20\n",
        "chunks = chunk_text(sample_text, chunk_size=100, overlap=20)\n",
        "print(f\"Created {len(chunks)} chunks\")\n",
        "print(f\"First chunk: '{chunks[0][:50]}...'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHfTVWYV_3gH",
        "outputId": "123c4ecb-a59b-4dec-b96b-a6a7d858954f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recursive chunking ready!\n"
          ]
        }
      ],
      "source": [
        "def recursive_chunk(text: str, max_size: int = 500, separators: List[str] = None) -> List[str]:\n",
        "    \"\"\"Recursive chunking - tries to split on semantic boundaries.\"\"\"\n",
        "    if separators is None:\n",
        "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
        "\n",
        "    if len(text) <= max_size:\n",
        "        return [text]\n",
        "\n",
        "    for sep in separators:\n",
        "        if sep in text:\n",
        "            parts = text.split(sep)\n",
        "            chunks = []\n",
        "            current = \"\"\n",
        "\n",
        "            for part in parts:\n",
        "                if len(current) + len(part) + len(sep) <= max_size:\n",
        "                    current += part + sep\n",
        "                else:\n",
        "                    if current:\n",
        "                        chunks.append(current.strip())\n",
        "                    current = part + sep\n",
        "\n",
        "            if current:\n",
        "                chunks.append(current.strip())\n",
        "            return chunks\n",
        "\n",
        "    # Fallback: hard split\n",
        "    return [text[i:i+max_size] for i in range(0, len(text), max_size)]\n",
        "\n",
        "print(\"Recursive chunking ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snVqsfAR_3gH"
      },
      "source": [
        "## 5. Embeddings\n",
        "\n",
        "### Popular Embedding Models\n",
        "\n",
        "| Model | Dimensions | Quality | Speed |\n",
        "|-------|-----------|---------|-------|\n",
        "| all-MiniLM-L6-v2 | 384 | Good | Fast |\n",
        "| all-mpnet-base-v2 | 768 | Better | Medium |\n",
        "| bge-large-en-v1.5 | 1024 | Best | Slow |\n",
        "| OpenAI ada-002 | 1536 | Great | API |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNxAF87j_3gH",
        "outputId": "d09ab628-2306-4666-ba8d-02ecb48d4dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (3, 384)\n",
            "Each embedding: (384,)\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load embedding model\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings\n",
        "texts = [\n",
        "    \"RAG combines retrieval with generation\",\n",
        "    \"Vector databases store embeddings\",\n",
        "    \"The weather is nice today\"\n",
        "]\n",
        "\n",
        "embeddings = embed_model.encode(texts)\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Each embedding: {embeddings[0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0JTIcVy_3gH",
        "outputId": "be5299e4-f846-43a3-d627-17c0dde3f9c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: How does retrieval augmented generation work?\n",
            "\n",
            "Similarities:\n",
            "  'RAG combines retrieval with generation...': 0.608\n",
            "  'Vector databases store embeddings...': 0.185\n",
            "  'The weather is nice today...': -0.102\n"
          ]
        }
      ],
      "source": [
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"Compute cosine similarity.\"\"\"\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Compare similarities\n",
        "query = \"How does retrieval augmented generation work?\"\n",
        "query_emb = embed_model.encode(query)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nSimilarities:\")\n",
        "for i, text in enumerate(texts):\n",
        "    sim = cosine_similarity(query_emb, embeddings[i])\n",
        "    print(f\"  '{text[:40]}...': {sim:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qey4Xtqs_3gH"
      },
      "source": [
        "## 6. Vector Store with ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oJbEGBz_3gH",
        "outputId": "8780a04c-ef0e-40f5-8d10-4836b9175dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.4.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.12.3)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.3)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.51.0)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (35.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.26.0)\n",
            "Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=2.6.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.39.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Added 4 documents\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb\n",
        "import chromadb\n",
        "\n",
        "# Create in-memory client\n",
        "client = chromadb.Client()\n",
        "\n",
        "# Create collection\n",
        "collection = client.create_collection(\n",
        "    name=\"my_documents\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# Add documents\n",
        "documents = [\n",
        "    \"RAG helps LLMs access external knowledge\",\n",
        "    \"Vector databases enable fast similarity search\",\n",
        "    \"Chunking splits documents into manageable pieces\",\n",
        "    \"Embeddings represent text as dense vectors\"\n",
        "]\n",
        "\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "print(f\"Added {collection.count()} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvqUPFFU_3gI",
        "outputId": "d7ddd4b0-07a6-4d77-b234-766d89c73745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top results:\n",
            "  [0.516] Vector databases enable fast similarity search\n",
            "  [0.683] Chunking splits documents into manageable pieces\n"
          ]
        }
      ],
      "source": [
        "# Query the collection\n",
        "results = collection.query(\n",
        "    query_texts=[\"How do I search for similar documents?\"],\n",
        "    n_results=2\n",
        ")\n",
        "\n",
        "print(\"Top results:\")\n",
        "for doc, dist in zip(results['documents'][0], results['distances'][0]):\n",
        "    print(f\"  [{dist:.3f}] {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxbdB4N9_3gI"
      },
      "source": [
        "## 7. Complete RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC2kPFxj_3gI",
        "outputId": "8a8596d0-a4b4-4ed9-d9f2-17ae3c291410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleRAG initialized!\n"
          ]
        }
      ],
      "source": [
        "class SimpleRAG:\n",
        "    \"\"\"Minimal RAG implementation.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        self.embed_model = SentenceTransformer(embed_model_name)\n",
        "        self.client = chromadb.Client()\n",
        "        self.collection = self.client.create_collection(\"rag_docs\")\n",
        "        self.documents = []\n",
        "\n",
        "    def add_documents(self, docs: List[str]):\n",
        "        \"\"\"Add documents to index.\"\"\"\n",
        "        embeddings = self.embed_model.encode(docs).tolist()\n",
        "        ids = [f\"doc_{len(self.documents)+i}\" for i in range(len(docs))]\n",
        "\n",
        "        self.collection.add(\n",
        "            documents=docs,\n",
        "            embeddings=embeddings,\n",
        "            ids=ids\n",
        "        )\n",
        "        self.documents.extend(docs)\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
        "        \"\"\"Retrieve top-k relevant chunks.\"\"\"\n",
        "        query_emb = self.embed_model.encode([query]).tolist()\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=query_emb,\n",
        "            n_results=k\n",
        "        )\n",
        "        return results['documents'][0]\n",
        "\n",
        "    def generate_prompt(self, query: str, context: List[str]) -> str:\n",
        "        \"\"\"Create prompt with retrieved context.\"\"\"\n",
        "        context_str = \"\\n\\n\".join(context)\n",
        "        return f\"\"\"Answer the question based on the context below.\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Usage\n",
        "rag = SimpleRAG()\n",
        "print(\"SimpleRAG initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIyx-UnU_3gI",
        "outputId": "51b20208-8322-47da-99f2-8f816c5cf27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Prompt:\n",
            "Answer the question based on the context below.\n",
            "\n",
            "Context:\n",
            "BERT is an encoder-only transformer for understanding.\n",
            "\n",
            "PyTorch is a deep learning framework developed by Meta.\n",
            "\n",
            "Question: What is BERT used for?\n",
            "\n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "# Add knowledge base\n",
        "knowledge = [\n",
        "    \"PyTorch is a deep learning framework developed by Meta.\",\n",
        "    \"Transformers use self-attention to process sequences.\",\n",
        "    \"BERT is an encoder-only transformer for understanding.\",\n",
        "    \"GPT models use decoder-only architecture for generation.\",\n",
        "    \"RAG retrieves relevant documents to augment LLM responses.\"\n",
        "]\n",
        "\n",
        "rag.add_documents(knowledge)\n",
        "\n",
        "# Query\n",
        "query = \"What is BERT used for?\"\n",
        "context = rag.retrieve(query, k=2)\n",
        "prompt = rag.generate_prompt(query, context)\n",
        "\n",
        "print(\"Generated Prompt:\")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD5bfMsL_3gI"
      },
      "source": [
        "## 8. Advanced RAG Techniques\n",
        "\n",
        "### Improvements\n",
        "\n",
        "| Technique | Purpose |\n",
        "|-----------|--------|\n",
        "| HyDE | Generate hypothetical answer to embed |\n",
        "| Re-ranking | Reorder results with cross-encoder |\n",
        "| Multi-query | Generate variations of query |\n",
        "| Parent retrieval | Return larger context around chunks |\n",
        "\n",
        "### Production Considerations\n",
        "\n",
        "1. **Persistent storage**: Use ChromaDB persist or Pinecone/Weaviate\n",
        "2. **Metadata filtering**: Filter by date, source, category\n",
        "3. **Hybrid search**: Combine dense + sparse (BM25)\n",
        "4. **Evaluation**: Hit rate, MRR, answer faithfulness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIV4iKOn_3gJ"
      },
      "source": [
        "## 9. Interview Questions\n",
        "\n",
        "**Q1: What is RAG and why use it?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "RAG retrieves relevant documents and includes them in the LLM prompt. Benefits: reduces hallucinations, adds current/private knowledge, enables source citation.\n",
        "</details>\n",
        "\n",
        "**Q2: How do you choose chunk size?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "Balance between: too small (loses context) vs too large (dilutes relevance). Typical: 200-500 tokens. Add 10-20% overlap to maintain context across chunks.\n",
        "</details>\n",
        "\n",
        "**Q3: What's the difference between dense and sparse retrieval?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "- Dense: Uses embedding vectors, captures semantic similarity\n",
        "- Sparse: Keyword matching (BM25), exact term matches\n",
        "- Hybrid: Combines both for best results\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS9OimhP_3gJ"
      },
      "source": [
        "## 10. Summary\n",
        "\n",
        "- **RAG**: Retrieval + Generation to ground LLMs\n",
        "- **Chunking**: Split documents with overlap\n",
        "- **Embeddings**: Dense vector representations\n",
        "- **Vector Store**: Fast similarity search\n",
        "- **Pipeline**: Retrieve → Augment prompt → Generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipa9itjB_3gJ"
      },
      "source": [
        "## 11. References\n",
        "\n",
        "- [RAG Paper](https://arxiv.org/abs/2005.11401)\n",
        "- [ChromaDB](https://www.trychroma.com/)\n",
        "- [LangChain RAG](https://python.langchain.com/docs/use_cases/question_answering/)\n",
        "- [Sentence Transformers](https://www.sbert.net/)\n",
        "\n",
        "---\n",
        "**Next:** [Module 22: NLP Model Deployment](../22_deployment/22_deployment.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}