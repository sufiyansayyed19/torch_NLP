{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 10: Named Entity Recognition (NER)\n",
                "\n",
                "**Sequence Labeling with BiLSTM-CRF**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand NER and BIO tagging\n",
                "- âœ… Build BiLSTM for token classification\n",
                "- âœ… Implement CRF layer from scratch\n",
                "- âœ… Create complete BiLSTM-CRF model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 09: Text Classification](../09_text_classification_rnns/09_text_classification_rnns.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. NER Task Definition\n",
                "\n",
                "### Input/Output\n",
                "```\n",
                "Input:  \"John works at Google in California\"\n",
                "Output: [B-PER, O, O, B-ORG, O, B-LOC]\n",
                "```\n",
                "\n",
                "### BIO Tagging Scheme\n",
                "\n",
                "| Tag | Meaning |\n",
                "|-----|--------|\n",
                "| B-XXX | Beginning of entity XXX |\n",
                "| I-XXX | Inside entity XXX |\n",
                "| O | Outside any entity |\n",
                "\n",
                "**Example:**\n",
                "```\n",
                "\"New York is a city\"\n",
                "[B-LOC, I-LOC, O, O, O]\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from typing import List, Tuple\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Sample Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample NER data\n",
                "train_data = [\n",
                "    ([\"John\", \"works\", \"at\", \"Google\"], [\"B-PER\", \"O\", \"O\", \"B-ORG\"]),\n",
                "    ([\"Mary\", \"lives\", \"in\", \"New\", \"York\"], [\"B-PER\", \"O\", \"O\", \"B-LOC\", \"I-LOC\"]),\n",
                "    ([\"Apple\", \"is\", \"in\", \"California\"], [\"B-ORG\", \"O\", \"O\", \"B-LOC\"]),\n",
                "]\n",
                "\n",
                "# Build vocabularies\n",
                "all_words = set(w for sent, _ in train_data for w in sent)\n",
                "all_tags = set(t for _, tags in train_data for t in tags)\n",
                "\n",
                "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
                "word2idx.update({w: i+2 for i, w in enumerate(all_words)})\n",
                "\n",
                "tag2idx = {'<PAD>': 0}\n",
                "tag2idx.update({t: i+1 for i, t in enumerate(all_tags)})\n",
                "idx2tag = {v: k for k, v in tag2idx.items()}\n",
                "\n",
                "print(f\"Words: {len(word2idx)}, Tags: {tag2idx}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. BiLSTM for NER (Without CRF)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BiLSTMNER(nn.Module):\n",
                "    \"\"\"BiLSTM for token classification.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size, tag_size, embed_dim=100, hidden_dim=128):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
                "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
                "        self.fc = nn.Linear(hidden_dim * 2, tag_size)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (batch, seq)\n",
                "        embedded = self.embedding(x)  # (batch, seq, embed)\n",
                "        lstm_out, _ = self.lstm(embedded)  # (batch, seq, hidden*2)\n",
                "        logits = self.fc(lstm_out)  # (batch, seq, tag_size)\n",
                "        return logits\n",
                "\n",
                "# Test\n",
                "model = BiLSTMNER(len(word2idx), len(tag2idx))\n",
                "x = torch.randint(0, len(word2idx), (2, 5))\n",
                "out = model(x)\n",
                "print(f\"Output shape: {out.shape}  # (batch, seq, tags)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. CRF Layer (Key Concept!)\n",
                "\n",
                "### Why CRF?\n",
                "\n",
                "Independent softmax per token ignores **tag dependencies**:\n",
                "- `I-PER` can't follow `B-ORG`\n",
                "- `I-LOC` should follow `B-LOC` or `I-LOC`\n",
                "\n",
                "CRF learns **transition scores** between tags."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CRF(nn.Module):\n",
                "    \"\"\"Linear-chain CRF layer.\"\"\"\n",
                "    \n",
                "    def __init__(self, num_tags: int):\n",
                "        super().__init__()\n",
                "        self.num_tags = num_tags\n",
                "        \n",
                "        # Transition scores: transitions[i,j] = score of j -> i\n",
                "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n",
                "        \n",
                "        # Start and end transitions\n",
                "        self.start_transitions = nn.Parameter(torch.randn(num_tags))\n",
                "        self.end_transitions = nn.Parameter(torch.randn(num_tags))\n",
                "    \n",
                "    def forward(self, emissions, tags, mask=None):\n",
                "        \"\"\"\n",
                "        Compute negative log likelihood.\n",
                "        \n",
                "        Args:\n",
                "            emissions: (batch, seq, num_tags)\n",
                "            tags: (batch, seq)\n",
                "            mask: (batch, seq)\n",
                "        \"\"\"\n",
                "        if mask is None:\n",
                "            mask = torch.ones_like(tags, dtype=torch.bool)\n",
                "        \n",
                "        log_numerator = self._score_sentence(emissions, tags, mask)\n",
                "        log_denominator = self._compute_log_partition(emissions, mask)\n",
                "        \n",
                "        return (log_denominator - log_numerator).mean()\n",
                "    \n",
                "    def _score_sentence(self, emissions, tags, mask):\n",
                "        \"\"\"Score of a specific tag sequence.\"\"\"\n",
                "        batch_size, seq_len, _ = emissions.shape\n",
                "        \n",
                "        # Start transition\n",
                "        score = self.start_transitions[tags[:, 0]]\n",
                "        \n",
                "        # Emission for first tag\n",
                "        score += emissions[:, 0].gather(1, tags[:, 0].unsqueeze(1)).squeeze(1)\n",
                "        \n",
                "        for i in range(1, seq_len):\n",
                "            # Transition score\n",
                "            score += self.transitions[tags[:, i], tags[:, i-1]] * mask[:, i]\n",
                "            # Emission score\n",
                "            score += emissions[:, i].gather(1, tags[:, i].unsqueeze(1)).squeeze(1) * mask[:, i]\n",
                "        \n",
                "        # End transition\n",
                "        last_tag_idx = mask.sum(dim=1) - 1\n",
                "        last_tags = tags.gather(1, last_tag_idx.unsqueeze(1)).squeeze(1)\n",
                "        score += self.end_transitions[last_tags]\n",
                "        \n",
                "        return score\n",
                "    \n",
                "    def _compute_log_partition(self, emissions, mask):\n",
                "        \"\"\"Compute log partition function (forward algorithm).\"\"\"\n",
                "        batch_size, seq_len, num_tags = emissions.shape\n",
                "        \n",
                "        # Initialize with start transitions + first emissions\n",
                "        score = self.start_transitions + emissions[:, 0]  # (batch, tags)\n",
                "        \n",
                "        for i in range(1, seq_len):\n",
                "            # score: (batch, tags) -> (batch, tags, 1)\n",
                "            # transitions: (tags, tags)\n",
                "            # emissions: (batch, tags)\n",
                "            broadcast_score = score.unsqueeze(2)  # (batch, tags, 1)\n",
                "            broadcast_emissions = emissions[:, i].unsqueeze(1)  # (batch, 1, tags)\n",
                "            \n",
                "            next_score = broadcast_score + self.transitions + broadcast_emissions\n",
                "            next_score = torch.logsumexp(next_score, dim=1)  # (batch, tags)\n",
                "            \n",
                "            score = torch.where(mask[:, i].unsqueeze(1), next_score, score)\n",
                "        \n",
                "        score += self.end_transitions\n",
                "        return torch.logsumexp(score, dim=1)\n",
                "    \n",
                "    def decode(self, emissions, mask=None):\n",
                "        \"\"\"Viterbi decoding.\"\"\"\n",
                "        if mask is None:\n",
                "            mask = torch.ones(emissions.shape[:2], dtype=torch.bool, device=emissions.device)\n",
                "        \n",
                "        batch_size, seq_len, num_tags = emissions.shape\n",
                "        \n",
                "        score = self.start_transitions + emissions[:, 0]\n",
                "        history = []\n",
                "        \n",
                "        for i in range(1, seq_len):\n",
                "            broadcast_score = score.unsqueeze(2)\n",
                "            broadcast_emissions = emissions[:, i].unsqueeze(1)\n",
                "            \n",
                "            next_score = broadcast_score + self.transitions + broadcast_emissions\n",
                "            next_score, indices = next_score.max(dim=1)\n",
                "            \n",
                "            score = torch.where(mask[:, i].unsqueeze(1), next_score, score)\n",
                "            history.append(indices)\n",
                "        \n",
                "        score += self.end_transitions\n",
                "        _, best_last_tags = score.max(dim=1)\n",
                "        \n",
                "        # Backtrack\n",
                "        best_tags = [best_last_tags]\n",
                "        for hist in reversed(history):\n",
                "            best_last_tags = hist.gather(1, best_last_tags.unsqueeze(1)).squeeze(1)\n",
                "            best_tags.append(best_last_tags)\n",
                "        \n",
                "        best_tags.reverse()\n",
                "        return torch.stack(best_tags, dim=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. BiLSTM-CRF Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BiLSTMCRF(nn.Module):\n",
                "    \"\"\"BiLSTM-CRF for NER.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size, tag_size, embed_dim=100, hidden_dim=128):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
                "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
                "        self.fc = nn.Linear(hidden_dim * 2, tag_size)\n",
                "        self.crf = CRF(tag_size)\n",
                "    \n",
                "    def forward(self, x, tags, mask=None):\n",
                "        \"\"\"Compute loss.\"\"\"\n",
                "        emissions = self._get_emissions(x)\n",
                "        return self.crf(emissions, tags, mask)\n",
                "    \n",
                "    def decode(self, x, mask=None):\n",
                "        \"\"\"Predict tags.\"\"\"\n",
                "        emissions = self._get_emissions(x)\n",
                "        return self.crf.decode(emissions, mask)\n",
                "    \n",
                "    def _get_emissions(self, x):\n",
                "        embedded = self.embedding(x)\n",
                "        lstm_out, _ = self.lstm(embedded)\n",
                "        return self.fc(lstm_out)\n",
                "\n",
                "# Test\n",
                "model = BiLSTMCRF(len(word2idx), len(tag2idx))\n",
                "x = torch.tensor([[word2idx.get(w, 1) for w in [\"John\", \"works\", \"at\", \"Google\"]]])\n",
                "tags = torch.tensor([[tag2idx[t] for t in [\"B-PER\", \"O\", \"O\", \"B-ORG\"]]])\n",
                "\n",
                "loss = model(x, tags)\n",
                "pred = model.decode(x)\n",
                "\n",
                "print(f\"Loss: {loss.item():.4f}\")\n",
                "print(f\"Predicted tags: {[idx2tag[i.item()] for i in pred[0]]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. ðŸ”¥ Real-World Usage\n",
                "\n",
                "### NER Solutions (2024)\n",
                "\n",
                "| Priority | Solution |\n",
                "|----------|----------|\n",
                "| **Speed** | SpaCy (rule-based + small models) |\n",
                "| **Accuracy** | Fine-tuned BERT for token classification |\n",
                "| **Domain-specific** | BiLSTM-CRF with domain embeddings |\n",
                "\n",
                "### Production Pattern\n",
                "```\n",
                "1. Start with SpaCy + rules\n",
                "2. Add ML for entities SpaCy misses\n",
                "3. Ensemble for best results\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Interview Questions\n",
                "\n",
                "**Q1: Why use CRF instead of softmax per token?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "CRF models dependencies between adjacent tags. It learns that I-PER should follow B-PER, not B-ORG. Softmax treats each position independently.\n",
                "</details>\n",
                "\n",
                "**Q2: What is BIO tagging?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- B-XXX: Beginning of entity type XXX\n",
                "- I-XXX: Inside/continuation of entity\n",
                "- O: Outside any entity\n",
                "This distinguishes multi-word entities (\"New York\" â†’ B-LOC, I-LOC)\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Summary\n",
                "\n",
                "- **NER**: Token-level classification\n",
                "- **BIO scheme**: B-/I-/O tags for multi-word entities\n",
                "- **BiLSTM-CRF**: Classic architecture, still competitive\n",
                "- **CRF**: Models tag transitions, better than independent softmax"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. References\n",
                "\n",
                "- [Neural Architectures for NER](https://arxiv.org/abs/1603.01360)\n",
                "- [pytorch-crf library](https://pytorch-crf.readthedocs.io/)\n",
                "- [SpaCy NER](https://spacy.io/usage/linguistic-features#named-entities)\n",
                "\n",
                "---\n",
                "**Next:** [Module 11: Language Modeling](../11_language_modeling/11_language_modeling.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}