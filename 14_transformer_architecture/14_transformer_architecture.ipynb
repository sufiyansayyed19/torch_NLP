{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 14: Transformer Architecture\n",
                "\n",
                "**Attention Is All You Need**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand Transformer architecture\n",
                "- âœ… Implement multi-head attention\n",
                "- âœ… Implement positional encoding\n",
                "- âœ… Build complete Transformer from scratch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 13: Attention Mechanism](../13_attention/13_attention.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Transformer Overview\n",
                "\n",
                "```\n",
                "         Encoder                     Decoder\n",
                "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "    â”‚   Self-     â”‚            â”‚   Masked    â”‚\n",
                "    â”‚  Attention  â”‚            â”‚ Self-Attn   â”‚\n",
                "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "    â”‚  Add & Norm â”‚            â”‚ Add & Norm  â”‚\n",
                "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "    â”‚    FFN      â”‚            â”‚Cross-Attn   â”‚â†â”€â”€\n",
                "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "    â”‚  Add & Norm â”‚            â”‚ Add & Norm  â”‚\n",
                "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "           â†“                   â”‚    FFN      â”‚\n",
                "      (to decoder)             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "                               â”‚ Add & Norm  â”‚\n",
                "                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```\n",
                "\n",
                "### Key Innovations\n",
                "- No recurrence â†’ full parallelization\n",
                "- Self-attention captures all pairwise relationships\n",
                "- Positional encoding adds sequence order"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Positional Encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PositionalEncoding(nn.Module):\n",
                "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        \n",
                "        # Create positional encoding matrix\n",
                "        pe = torch.zeros(max_len, d_model)\n",
                "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
                "        \n",
                "        # div_term = 10000^(2i/d_model)\n",
                "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
                "        \n",
                "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
                "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
                "        \n",
                "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
                "        self.register_buffer('pe', pe)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (batch, seq, d_model)\n",
                "        x = x + self.pe[:, :x.size(1), :]\n",
                "        return self.dropout(x)\n",
                "\n",
                "# Visualize\n",
                "pe = PositionalEncoding(d_model=64)\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.imshow(pe.pe[0, :50, :].numpy(), aspect='auto', cmap='RdBu')\n",
                "plt.xlabel('Dimension')\n",
                "plt.ylabel('Position')\n",
                "plt.title('Positional Encoding')\n",
                "plt.colorbar()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Multi-Head Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention(nn.Module):\n",
                "    \"\"\"Multi-head attention from scratch.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
                "        super().__init__()\n",
                "        assert d_model % num_heads == 0\n",
                "        \n",
                "        self.d_model = d_model\n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        \n",
                "        self.W_q = nn.Linear(d_model, d_model)\n",
                "        self.W_k = nn.Linear(d_model, d_model)\n",
                "        self.W_v = nn.Linear(d_model, d_model)\n",
                "        self.W_o = nn.Linear(d_model, d_model)\n",
                "        \n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(self, query, key, value, mask=None):\n",
                "        \"\"\"\n",
                "        query, key, value: (batch, seq, d_model)\n",
                "        mask: (batch, 1, seq) or (batch, seq, seq)\n",
                "        \"\"\"\n",
                "        batch_size = query.size(0)\n",
                "        \n",
                "        # Linear projections\n",
                "        Q = self.W_q(query)  # (batch, seq, d_model)\n",
                "        K = self.W_k(key)\n",
                "        V = self.W_v(value)\n",
                "        \n",
                "        # Split into heads: (batch, seq, d_model) -> (batch, heads, seq, d_k)\n",
                "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        \n",
                "        # Scaled dot-product attention\n",
                "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
                "        \n",
                "        if mask is not None:\n",
                "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
                "        \n",
                "        attn_weights = F.softmax(scores, dim=-1)\n",
                "        attn_weights = self.dropout(attn_weights)\n",
                "        \n",
                "        # Apply to values\n",
                "        context = torch.matmul(attn_weights, V)  # (batch, heads, seq, d_k)\n",
                "        \n",
                "        # Concatenate heads: (batch, seq, d_model)\n",
                "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
                "        \n",
                "        return self.W_o(context), attn_weights\n",
                "\n",
                "# Test\n",
                "mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
                "x = torch.randn(2, 10, 512)\n",
                "out, weights = mha(x, x, x)\n",
                "print(f\"MHA output: {out.shape}, weights: {weights.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Feed-Forward Network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FeedForward(nn.Module):\n",
                "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(d_model, d_ff)\n",
                "        self.fc2 = nn.Linear(d_ff, d_model)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
                "\n",
                "# Test\n",
                "ffn = FeedForward(512, 2048)\n",
                "out = ffn(x)\n",
                "print(f\"FFN output: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Encoder Layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EncoderLayer(nn.Module):\n",
                "    \"\"\"Single Transformer encoder layer.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
                "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
                "        self.norm1 = nn.LayerNorm(d_model)\n",
                "        self.norm2 = nn.LayerNorm(d_model)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(self, x, mask=None):\n",
                "        # Self-attention with residual\n",
                "        attn_out, _ = self.self_attn(x, x, x, mask)\n",
                "        x = self.norm1(x + self.dropout(attn_out))\n",
                "        \n",
                "        # FFN with residual\n",
                "        ffn_out = self.ffn(x)\n",
                "        x = self.norm2(x + self.dropout(ffn_out))\n",
                "        \n",
                "        return x\n",
                "\n",
                "# Test\n",
                "encoder_layer = EncoderLayer(512, 8, 2048)\n",
                "out = encoder_layer(x)\n",
                "print(f\"Encoder layer output: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Decoder Layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DecoderLayer(nn.Module):\n",
                "    \"\"\"Single Transformer decoder layer.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
                "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
                "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
                "        self.norm1 = nn.LayerNorm(d_model)\n",
                "        self.norm2 = nn.LayerNorm(d_model)\n",
                "        self.norm3 = nn.LayerNorm(d_model)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(self, x, encoder_out, src_mask=None, tgt_mask=None):\n",
                "        # Masked self-attention\n",
                "        attn_out, _ = self.self_attn(x, x, x, tgt_mask)\n",
                "        x = self.norm1(x + self.dropout(attn_out))\n",
                "        \n",
                "        # Cross-attention to encoder\n",
                "        attn_out, _ = self.cross_attn(x, encoder_out, encoder_out, src_mask)\n",
                "        x = self.norm2(x + self.dropout(attn_out))\n",
                "        \n",
                "        # FFN\n",
                "        ffn_out = self.ffn(x)\n",
                "        x = self.norm3(x + self.dropout(ffn_out))\n",
                "        \n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Full Transformer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Transformer(nn.Module):\n",
                "    \"\"\"Complete Transformer from scratch.\"\"\"\n",
                "    \n",
                "    def __init__(self, src_vocab, tgt_vocab, d_model=512, num_heads=8,\n",
                "                 num_layers=6, d_ff=2048, dropout=0.1, max_len=5000):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.d_model = d_model\n",
                "        \n",
                "        # Embeddings\n",
                "        self.src_embed = nn.Embedding(src_vocab, d_model)\n",
                "        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)\n",
                "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
                "        \n",
                "        # Encoder & Decoder stacks\n",
                "        self.encoder_layers = nn.ModuleList([\n",
                "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
                "            for _ in range(num_layers)\n",
                "        ])\n",
                "        self.decoder_layers = nn.ModuleList([\n",
                "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
                "            for _ in range(num_layers)\n",
                "        ])\n",
                "        \n",
                "        # Output\n",
                "        self.fc_out = nn.Linear(d_model, tgt_vocab)\n",
                "    \n",
                "    def generate_mask(self, src, tgt):\n",
                "        \"\"\"Generate source padding mask and target causal mask.\"\"\"\n",
                "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, src_len)\n",
                "        \n",
                "        tgt_len = tgt.size(1)\n",
                "        tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len)).bool().to(tgt.device)\n",
                "        tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
                "        tgt_mask = tgt_mask & tgt_pad_mask\n",
                "        \n",
                "        return src_mask, tgt_mask\n",
                "    \n",
                "    def encode(self, src, src_mask):\n",
                "        x = self.pos_encoding(self.src_embed(src) * math.sqrt(self.d_model))\n",
                "        for layer in self.encoder_layers:\n",
                "            x = layer(x, src_mask)\n",
                "        return x\n",
                "    \n",
                "    def decode(self, tgt, encoder_out, src_mask, tgt_mask):\n",
                "        x = self.pos_encoding(self.tgt_embed(tgt) * math.sqrt(self.d_model))\n",
                "        for layer in self.decoder_layers:\n",
                "            x = layer(x, encoder_out, src_mask, tgt_mask)\n",
                "        return self.fc_out(x)\n",
                "    \n",
                "    def forward(self, src, tgt):\n",
                "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
                "        encoder_out = self.encode(src, src_mask)\n",
                "        return self.decode(tgt, encoder_out, src_mask, tgt_mask)\n",
                "\n",
                "# Test\n",
                "transformer = Transformer(src_vocab=1000, tgt_vocab=1000, d_model=256, num_heads=4, num_layers=2)\n",
                "src = torch.randint(1, 1000, (2, 10))\n",
                "tgt = torch.randint(1, 1000, (2, 8))\n",
                "out = transformer(src, tgt)\n",
                "print(f\"Transformer output: {out.shape}\")\n",
                "print(f\"Parameters: {sum(p.numel() for p in transformer.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. ðŸ”¥ Real-World Usage\n",
                "\n",
                "### PyTorch Built-in\n",
                "```python\n",
                "# Use PyTorch's implementation\n",
                "transformer = nn.Transformer(\n",
                "    d_model=512, nhead=8, num_encoder_layers=6,\n",
                "    num_decoder_layers=6, dim_feedforward=2048\n",
                ")\n",
                "```\n",
                "\n",
                "### Transformer Variants\n",
                "\n",
                "| Model | Architecture | Use Case |\n",
                "|-------|--------------|----------|\n",
                "| BERT | Encoder only | Understanding |\n",
                "| GPT | Decoder only | Generation |\n",
                "| T5, BART | Encoder-Decoder | Seq2Seq tasks |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Interview Questions\n",
                "\n",
                "**Q1: Why use positional encoding?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Self-attention is permutation-invariant. Without positional info, \"cat chased dog\" = \"dog chased cat\". Sinusoidal encoding adds position info without additional parameters.\n",
                "</details>\n",
                "\n",
                "**Q2: What is multi-head attention?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Run attention multiple times in parallel with different learned projections. Each head can focus on different aspects (syntax, semantics, etc.). Outputs are concatenated.\n",
                "</details>\n",
                "\n",
                "**Q3: What is the causal mask in decoder?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Prevents decoder from \"cheating\" by looking at future tokens. Position i can only attend to positions <= i. Implemented as lower triangular mask.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Summary\n",
                "\n",
                "- **Positional encoding**: Sinusoidal, adds position info\n",
                "- **Multi-head attention**: Multiple parallel attention heads\n",
                "- **Encoder**: Self-attention + FFN, repeated N times\n",
                "- **Decoder**: Masked self-attn + cross-attn + FFN\n",
                "- **Residual + LayerNorm**: Around every sublayer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. References\n",
                "\n",
                "- [Attention Is All You Need (2017)](https://arxiv.org/abs/1706.03762)\n",
                "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
                "- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
                "\n",
                "---\n",
                "**Next:** [Module 15: BERT](../15_bert/15_bert.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}