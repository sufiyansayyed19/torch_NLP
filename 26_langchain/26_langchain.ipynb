{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 26: LangChain\n",
                "\n",
                "**Building LLM Applications with Composable Components**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- ✅ Understand LangChain architecture\n",
                "- ✅ Master chains and prompt templates\n",
                "- ✅ Build agents with tools\n",
                "- ✅ Implement conversation memory\n",
                "- ✅ Create RAG pipelines with LangChain"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 19: Prompt Engineering](../19_prompt_engineering/19_prompt_engineering.ipynb)\n",
                "- [Module 21: RAG](../21_rag/21_rag.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. What is LangChain?\n",
                "\n",
                "### Core Concept\n",
                "\n",
                "LangChain provides **composable building blocks** for LLM applications:\n",
                "\n",
                "```\n",
                "┌─────────────────────────────────────────────────────────────┐\n",
                "│                    LangChain Architecture                    │\n",
                "├─────────────────────────────────────────────────────────────┤\n",
                "│                                                              │\n",
                "│  [Prompts] ──→ [LLM/Chat Model] ──→ [Output Parsers]        │\n",
                "│       ↑                                      ↓               │\n",
                "│  [Memory]                               [Chains]             │\n",
                "│       ↑                                      ↓               │\n",
                "│  [Retrievers] ←── [Vector Stores] ←── [Documents]           │\n",
                "│                                                              │\n",
                "│              [Agents] ←── [Tools]                            │\n",
                "│                                                              │\n",
                "└─────────────────────────────────────────────────────────────┘\n",
                "```\n",
                "\n",
                "### Key Components\n",
                "\n",
                "| Component | Purpose |\n",
                "|-----------|--------|\n",
                "| Models | LLM wrappers (OpenAI, Anthropic, local) |\n",
                "| Prompts | Template management |\n",
                "| Chains | Compose multiple steps |\n",
                "| Memory | Persist conversation state |\n",
                "| Agents | Dynamic tool selection |\n",
                "| Retrievers | Document retrieval for RAG |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install: pip install langchain langchain-openai langchain-community chromadb\n",
                "\n",
                "# Note: Set your API key\n",
                "# import os\n",
                "# os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n",
                "\n",
                "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
                "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
                "\n",
                "print(\"LangChain imported! (v0.2+ with LCEL)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Prompt Templates\n",
                "\n",
                "### Theory\n",
                "\n",
                "Prompt templates separate **structure** from **content**:\n",
                "- Reusable across different inputs\n",
                "- Easy to version and manage\n",
                "- Support for few-shot examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic prompt template\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "\n",
                "template = PromptTemplate.from_template(\n",
                "    \"You are an expert in {domain}. Answer this question: {question}\"\n",
                ")\n",
                "\n",
                "# Format with variables\n",
                "prompt = template.format(\n",
                "    domain=\"machine learning\",\n",
                "    question=\"What is gradient descent?\"\n",
                ")\n",
                "print(\"Formatted prompt:\")\n",
                "print(prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chat prompt template (for chat models)\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "\n",
                "chat_template = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a helpful {role}. Be concise.\"),\n",
                "    (\"human\", \"{input}\")\n",
                "])\n",
                "\n",
                "messages = chat_template.format_messages(\n",
                "    role=\"Python tutor\",\n",
                "    input=\"Explain list comprehensions\"\n",
                ")\n",
                "\n",
                "print(\"Chat messages:\")\n",
                "for msg in messages:\n",
                "    print(f\"  [{msg.type}]: {msg.content}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Few-shot prompt template\n",
                "from langchain_core.prompts import FewShotPromptTemplate\n",
                "\n",
                "examples = [\n",
                "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
                "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
                "    {\"input\": \"big\", \"output\": \"small\"}\n",
                "]\n",
                "\n",
                "example_template = PromptTemplate(\n",
                "    input_variables=[\"input\", \"output\"],\n",
                "    template=\"Input: {input}\\nOutput: {output}\"\n",
                ")\n",
                "\n",
                "few_shot_prompt = FewShotPromptTemplate(\n",
                "    examples=examples,\n",
                "    example_prompt=example_template,\n",
                "    prefix=\"Give the opposite of each word:\",\n",
                "    suffix=\"Input: {word}\\nOutput:\",\n",
                "    input_variables=[\"word\"]\n",
                ")\n",
                "\n",
                "print(few_shot_prompt.format(word=\"fast\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. LCEL (LangChain Expression Language)\n",
                "\n",
                "### Theory\n",
                "\n",
                "LCEL is LangChain's **declarative composition** syntax:\n",
                "\n",
                "```python\n",
                "chain = prompt | llm | output_parser\n",
                "```\n",
                "\n",
                "Benefits:\n",
                "- Streaming support built-in\n",
                "- Async support built-in  \n",
                "- Batch processing\n",
                "- Easy debugging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "\n",
                "# Define components\n",
                "prompt = ChatPromptTemplate.from_template(\n",
                "    \"Write a one-sentence summary of: {topic}\"\n",
                ")\n",
                "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
                "output_parser = StrOutputParser()\n",
                "\n",
                "# Compose with LCEL (pipe operator)\n",
                "chain = prompt | llm | output_parser\n",
                "\n",
                "# Invoke\n",
                "result = chain.invoke({\"topic\": \"machine learning\"})\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Streaming with LCEL\n",
                "for chunk in chain.stream({\"topic\": \"neural networks\"}):\n",
                "    print(chunk, end=\"\", flush=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parallel chains with RunnableParallel\n",
                "from langchain_core.runnables import RunnableParallel\n",
                "\n",
                "summary_chain = ChatPromptTemplate.from_template(\n",
                "    \"Summarize in one sentence: {topic}\"\n",
                ") | llm | StrOutputParser()\n",
                "\n",
                "keywords_chain = ChatPromptTemplate.from_template(\n",
                "    \"Extract 3 keywords from: {topic}\"\n",
                ") | llm | StrOutputParser()\n",
                "\n",
                "# Run both in parallel\n",
                "parallel_chain = RunnableParallel(\n",
                "    summary=summary_chain,\n",
                "    keywords=keywords_chain\n",
                ")\n",
                "\n",
                "result = parallel_chain.invoke({\"topic\": \"Transformers in NLP\"})\n",
                "print(f\"Summary: {result['summary']}\")\n",
                "print(f\"Keywords: {result['keywords']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Memory\n",
                "\n",
                "### Theory\n",
                "\n",
                "Memory persists information across conversation turns:\n",
                "\n",
                "| Type | What it Stores | Best For |\n",
                "|------|---------------|----------|\n",
                "| ConversationBufferMemory | All messages | Short convos |\n",
                "| ConversationSummaryMemory | Summary | Long convos |\n",
                "| ConversationBufferWindowMemory | Last K messages | Medium convos |\n",
                "| VectorStoreRetrieverMemory | Relevant history | Large history |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.memory import ConversationBufferMemory\n",
                "from langchain_core.prompts import MessagesPlaceholder\n",
                "\n",
                "# Create memory\n",
                "memory = ConversationBufferMemory(return_messages=True)\n",
                "\n",
                "# Add to prompt\n",
                "prompt_with_memory = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a helpful assistant.\"),\n",
                "    MessagesPlaceholder(variable_name=\"history\"),\n",
                "    (\"human\", \"{input}\")\n",
                "])\n",
                "\n",
                "# Simulate conversation\n",
                "memory.save_context(\n",
                "    {\"input\": \"My name is Alice\"},\n",
                "    {\"output\": \"Hello Alice! How can I help you today?\"}\n",
                ")\n",
                "\n",
                "memory.save_context(\n",
                "    {\"input\": \"I'm learning Python\"},\n",
                "    {\"output\": \"That's great! Python is a wonderful language to learn.\"}\n",
                ")\n",
                "\n",
                "# Load memory\n",
                "history = memory.load_memory_variables({})\n",
                "print(\"Conversation history:\")\n",
                "for msg in history['history']:\n",
                "    print(f\"  {msg.type}: {msg.content}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary memory for long conversations\n",
                "from langchain.memory import ConversationSummaryMemory\n",
                "\n",
                "summary_memory = ConversationSummaryMemory(\n",
                "    llm=llm,\n",
                "    return_messages=True\n",
                ")\n",
                "\n",
                "# This automatically summarizes when buffer gets long\n",
                "print(\"Summary memory created - automatically condenses long conversations\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Agents and Tools\n",
                "\n",
                "### Theory\n",
                "\n",
                "Agents **dynamically select** which tools to use based on the input:\n",
                "\n",
                "```\n",
                "User Query ──→ Agent ──→ Think: Which tool?\n",
                "                  ↓\n",
                "            [Tool 1] [Tool 2] [Tool 3]\n",
                "                  ↓\n",
                "            Execute Tool ──→ Observe Result\n",
                "                  ↓\n",
                "            Think: Done? ──→ No ──→ Loop back\n",
                "                  ↓ Yes\n",
                "            Final Answer\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents import tool\n",
                "from langchain_core.tools import Tool\n",
                "\n",
                "# Define custom tools\n",
                "@tool\n",
                "def calculator(expression: str) -> str:\n",
                "    \"\"\"Evaluate a mathematical expression. Input should be a valid Python math expression.\"\"\"\n",
                "    try:\n",
                "        return str(eval(expression))\n",
                "    except:\n",
                "        return \"Error: Invalid expression\"\n",
                "\n",
                "@tool\n",
                "def get_word_length(word: str) -> int:\n",
                "    \"\"\"Get the length of a word.\"\"\"\n",
                "    return len(word)\n",
                "\n",
                "@tool  \n",
                "def search_knowledge(query: str) -> str:\n",
                "    \"\"\"Search for information. Returns relevant facts.\"\"\"\n",
                "    # Simulated search\n",
                "    knowledge_base = {\n",
                "        \"python\": \"Python is a programming language created by Guido van Rossum.\",\n",
                "        \"langchain\": \"LangChain is a framework for building LLM applications.\",\n",
                "        \"transformer\": \"Transformers use self-attention for sequence modeling.\"\n",
                "    }\n",
                "    for key, value in knowledge_base.items():\n",
                "        if key in query.lower():\n",
                "            return value\n",
                "    return \"No information found.\"\n",
                "\n",
                "tools = [calculator, get_word_length, search_knowledge]\n",
                "print(f\"Defined {len(tools)} tools:\")\n",
                "for t in tools:\n",
                "    print(f\"  - {t.name}: {t.description[:50]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
                "from langchain import hub\n",
                "\n",
                "# Get a prompt for the agent\n",
                "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
                "\n",
                "# Create the agent\n",
                "agent = create_openai_tools_agent(llm, tools, prompt)\n",
                "\n",
                "# Create executor\n",
                "agent_executor = AgentExecutor(\n",
                "    agent=agent,\n",
                "    tools=tools,\n",
                "    verbose=True  # See the agent's thinking\n",
                ")\n",
                "\n",
                "# Run the agent\n",
                "result = agent_executor.invoke({\n",
                "    \"input\": \"What is 25 * 4 + 10? Also, how long is the word 'artificial'?\"\n",
                "})\n",
                "print(f\"\\nFinal answer: {result['output']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. RAG with LangChain\n",
                "\n",
                "### Complete Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.vectorstores import Chroma\n",
                "from langchain_openai import OpenAIEmbeddings\n",
                "from langchain_core.runnables import RunnablePassthrough\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "# Sample documents\n",
                "documents = [\n",
                "    \"LangChain is a framework for developing LLM applications.\",\n",
                "    \"It provides tools for prompt management, memory, and agents.\",\n",
                "    \"LCEL (LangChain Expression Language) enables chain composition.\",\n",
                "    \"Agents can use tools to interact with external systems.\"\n",
                "]\n",
                "\n",
                "# Create vector store\n",
                "embeddings = OpenAIEmbeddings()\n",
                "vectorstore = Chroma.from_texts(documents, embeddings)\n",
                "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
                "\n",
                "print(f\"Created vector store with {len(documents)} documents\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RAG chain with LCEL\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "\n",
                "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
                "Answer based on the context below. If unsure, say \"I don't know.\"\n",
                "\n",
                "Context: {context}\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\")\n",
                "\n",
                "def format_docs(docs):\n",
                "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
                "\n",
                "rag_chain = (\n",
                "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
                "    | rag_prompt\n",
                "    | llm\n",
                "    | StrOutputParser()\n",
                ")\n",
                "\n",
                "# Query\n",
                "answer = rag_chain.invoke(\"What is LCEL?\")\n",
                "print(f\"Answer: {answer}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Output Parsers\n",
                "\n",
                "Convert LLM text output to structured data:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.output_parsers import JsonOutputParser\n",
                "from pydantic import BaseModel, Field\n",
                "\n",
                "# Define output schema\n",
                "class MovieReview(BaseModel):\n",
                "    title: str = Field(description=\"Movie title\")\n",
                "    rating: int = Field(description=\"Rating out of 10\")\n",
                "    summary: str = Field(description=\"One-line summary\")\n",
                "\n",
                "parser = JsonOutputParser(pydantic_object=MovieReview)\n",
                "\n",
                "prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"Extract movie review info.\\n{format_instructions}\"),\n",
                "    (\"human\", \"{review}\")\n",
                "]).partial(format_instructions=parser.get_format_instructions())\n",
                "\n",
                "chain = prompt | llm | parser\n",
                "\n",
                "result = chain.invoke({\n",
                "    \"review\": \"Inception is mind-bending! The visuals are stunning and the plot keeps you guessing. Solid 9/10.\"\n",
                "})\n",
                "print(f\"Parsed: {result}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Questions\n",
                "\n",
                "**Q1: What is the difference between chains and agents?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- **Chains**: Fixed sequence of steps, deterministic execution\n",
                "- **Agents**: Dynamic tool selection based on LLM reasoning, can loop\n",
                "- Use chains for straightforward pipelines, agents for complex decision-making\n",
                "</details>\n",
                "\n",
                "**Q2: What is LCEL and why use it?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "LCEL (LangChain Expression Language) is declarative chain composition using `|` operator:\n",
                "- Built-in streaming, async, and batching\n",
                "- Easier debugging with `chain.get_graph()`\n",
                "- Cleaner code than legacy `LLMChain`\n",
                "</details>\n",
                "\n",
                "**Q3: How would you handle a long conversation in LangChain?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Use appropriate memory:\n",
                "- `ConversationSummaryMemory`: Summarizes old messages\n",
                "- `ConversationBufferWindowMemory`: Keeps last K messages\n",
                "- `VectorStoreRetrieverMemory`: Retrieves relevant history\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "| Component | Purpose |\n",
                "|-----------|--------|\n",
                "| Prompts | Template management |\n",
                "| LCEL | Declarative chain composition |\n",
                "| Memory | Conversation persistence |\n",
                "| Agents | Dynamic tool selection |\n",
                "| Retrievers | RAG document retrieval |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. References\n",
                "\n",
                "- [LangChain Docs](https://python.langchain.com/docs/)\n",
                "- [LCEL Guide](https://python.langchain.com/docs/expression_language/)\n",
                "- [LangChain Hub](https://smith.langchain.com/hub)\n",
                "\n",
                "---\n",
                "**Next:** [Module 27: LangGraph](../27_langgraph/27_langgraph.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}