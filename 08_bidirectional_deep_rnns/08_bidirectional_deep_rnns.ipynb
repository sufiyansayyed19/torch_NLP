{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 08: Bidirectional & Deep RNNs\n",
                "\n",
                "**Building More Powerful Sequence Models**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand bidirectional RNNs\n",
                "- âœ… Know when to use (and NOT use) bidirectional\n",
                "- âœ… Build deep/stacked RNNs\n",
                "- âœ… Implement residual connections and layer norm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 06: LSTM](../06_lstm/06_lstm.ipynb)\n",
                "- [Module 07: GRU](../07_gru/07_gru.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Intuition & Motivation\n",
                "\n",
                "### Why Bidirectional?\n",
                "\n",
                "For classification, we want context from **both** sides:\n",
                "\n",
                "```\n",
                "\"The movie was not very good but the acting was _____\"\n",
                "\n",
                "Forward only â†’  Limited context about \"acting\"\n",
                "Bidirectional â†’ Knows \"not very good\" AND what comes after\n",
                "```\n",
                "\n",
                "### When You CAN'T Use Bidirectional\n",
                "\n",
                "- **Streaming/Online**: Can't wait for future tokens\n",
                "- **Generation**: Don't have future tokens yet\n",
                "- **Real-time**: Latency constraints"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "\n",
                "print(\"Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Bidirectional RNNs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PyTorch Bidirectional LSTM\n",
                "bilstm = nn.LSTM(\n",
                "    input_size=128,\n",
                "    hidden_size=256,\n",
                "    num_layers=1,\n",
                "    batch_first=True,\n",
                "    bidirectional=True  # <-- Key parameter\n",
                ")\n",
                "\n",
                "x = torch.randn(32, 50, 128)  # (batch, seq, features)\n",
                "output, (h_n, c_n) = bilstm(x)\n",
                "\n",
                "print(f\"Input: {x.shape}\")\n",
                "print(f\"Output: {output.shape}\")\n",
                "print(\"  â†’ Note: hidden_size * 2 = 512 (forward + backward concat)\")\n",
                "print(f\"h_n: {h_n.shape}\")\n",
                "print(\"  â†’ Note: 2 states (forward and backward)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Manual Bidirectional Implementation\n",
                "class BidirectionalLSTM(nn.Module):\n",
                "    \"\"\"Manual bidirectional LSTM.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size, hidden_size):\n",
                "        super().__init__()\n",
                "        self.forward_lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
                "        self.backward_lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # Forward pass\n",
                "        out_fwd, (h_fwd, c_fwd) = self.forward_lstm(x)\n",
                "        \n",
                "        # Backward pass (flip, process, flip back)\n",
                "        x_rev = torch.flip(x, [1])  # Reverse sequence\n",
                "        out_bwd, (h_bwd, c_bwd) = self.backward_lstm(x_rev)\n",
                "        out_bwd = torch.flip(out_bwd, [1])  # Flip back\n",
                "        \n",
                "        # Concatenate\n",
                "        output = torch.cat([out_fwd, out_bwd], dim=-1)\n",
                "        h_n = torch.cat([h_fwd, h_bwd], dim=0)\n",
                "        \n",
                "        return output, h_n\n",
                "\n",
                "# Test\n",
                "model = BidirectionalLSTM(128, 256)\n",
                "out, h = model(x)\n",
                "print(f\"Manual BiLSTM Output: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Combining Bidirectional Outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BiLSTMClassifier(nn.Module):\n",
                "    \"\"\"Bidirectional LSTM for classification.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, combine='concat'):\n",
                "        super().__init__()\n",
                "        self.combine = combine\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
                "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
                "        \n",
                "        # Output dimensions depend on combine method\n",
                "        if combine == 'concat':\n",
                "            fc_input = hidden_dim * 2\n",
                "        else:  # 'sum' or 'mean'\n",
                "            fc_input = hidden_dim\n",
                "        \n",
                "        self.fc = nn.Linear(fc_input, num_classes)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        embedded = self.embedding(x)\n",
                "        _, (h_n, _) = self.lstm(embedded)\n",
                "        \n",
                "        # h_n: (2, batch, hidden) - forward and backward\n",
                "        h_fwd = h_n[0]  # (batch, hidden)\n",
                "        h_bwd = h_n[1]  # (batch, hidden)\n",
                "        \n",
                "        if self.combine == 'concat':\n",
                "            combined = torch.cat([h_fwd, h_bwd], dim=-1)\n",
                "        elif self.combine == 'sum':\n",
                "            combined = h_fwd + h_bwd\n",
                "        elif self.combine == 'mean':\n",
                "            combined = (h_fwd + h_bwd) / 2\n",
                "        \n",
                "        return self.fc(combined)\n",
                "\n",
                "# Test different combine methods\n",
                "for method in ['concat', 'sum', 'mean']:\n",
                "    m = BiLSTMClassifier(5000, 100, 128, 2, combine=method)\n",
                "    x = torch.randint(0, 5000, (32, 50))\n",
                "    out = m(x)\n",
                "    print(f\"{method}: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Deep/Stacked RNNs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stacked LSTM\n",
                "stacked_lstm = nn.LSTM(\n",
                "    input_size=128,\n",
                "    hidden_size=256,\n",
                "    num_layers=3,  # <-- 3 layers stacked\n",
                "    batch_first=True,\n",
                "    dropout=0.3  # Dropout between layers\n",
                ")\n",
                "\n",
                "x = torch.randn(32, 50, 128)\n",
                "output, (h_n, c_n) = stacked_lstm(x)\n",
                "\n",
                "print(f\"Output: {output.shape}\")\n",
                "print(f\"h_n: {h_n.shape} (one per layer)\")\n",
                "print(f\"\\nTotal parameters: {sum(p.numel() for p in stacked_lstm.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Deep LSTM with Residual Connections\n",
                "class ResidualLSTM(nn.Module):\n",
                "    \"\"\"LSTM layer with residual connection.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
                "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        \n",
                "        # Project input if sizes don't match\n",
                "        self.proj = nn.Linear(input_size, hidden_size) if input_size != hidden_size else nn.Identity()\n",
                "    \n",
                "    def forward(self, x):\n",
                "        residual = self.proj(x)\n",
                "        out, _ = self.lstm(x)\n",
                "        out = self.dropout(out)\n",
                "        return self.layer_norm(out + residual)  # Residual connection\n",
                "\n",
                "class DeepResidualLSTM(nn.Module):\n",
                "    \"\"\"Stack of LSTM layers with residual connections.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.layers = nn.ModuleList([\n",
                "            ResidualLSTM(\n",
                "                input_size if i == 0 else hidden_size,\n",
                "                hidden_size,\n",
                "                dropout\n",
                "            ) for i in range(num_layers)\n",
                "        ])\n",
                "    \n",
                "    def forward(self, x):\n",
                "        for layer in self.layers:\n",
                "            x = layer(x)\n",
                "        return x\n",
                "\n",
                "# Test\n",
                "deep_lstm = DeepResidualLSTM(128, 256, num_layers=4)\n",
                "out = deep_lstm(torch.randn(32, 50, 128))\n",
                "print(f\"Deep Residual LSTM Output: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. ðŸ”¥ Real-World Usage\n",
                "\n",
                "### BiLSTM-CRF for NER\n",
                "\n",
                "Classic and still competitive architecture:\n",
                "```\n",
                "Embeddings â†’ BiLSTM â†’ CRF â†’ Entity Tags\n",
                "```\n",
                "\n",
                "### Stack Depth Guidelines\n",
                "\n",
                "| Layers | Use Case |\n",
                "|--------|----------|\n",
                "| 1 | Simple tasks, limited data |\n",
                "| 2-3 | Most NLP tasks |\n",
                "| 4+ | Large data, add residuals |\n",
                "\n",
                "### Tips\n",
                "- Use **dropout between layers** (not within)\n",
                "- Add **residual connections** for deep stacks\n",
                "- **Layer normalization** helps stability"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Interview Questions\n",
                "\n",
                "**Q1: When would you use a bidirectional RNN?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- Classification tasks (have full sequence)\n",
                "- Sequence labeling (NER, POS)\n",
                "- NOT for generation (don't have future tokens)\n",
                "- NOT for streaming (can't wait for full input)\n",
                "</details>\n",
                "\n",
                "**Q2: Why use residual connections in deep RNNs?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- Helps gradient flow in deep networks\n",
                "- Allows identity mapping if layer is not useful\n",
                "- Makes optimization easier\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary\n",
                "\n",
                "- **Bidirectional**: Forward + backward, concat outputs\n",
                "- **Use bidirectional**: Classification, labeling, understanding\n",
                "- **Don't use bidirectional**: Generation, streaming\n",
                "- **Deep RNNs**: Stack layers with dropout\n",
                "- **Residual connections**: Essential for 4+ layers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. References\n",
                "\n",
                "- [Bidirectional RNNs Paper](https://www.cs.toronto.edu/~graves/asru_2013.pdf)\n",
                "- [Deep Residual Learning](https://arxiv.org/abs/1512.03385)\n",
                "\n",
                "---\n",
                "**Next:** [Module 09: Text Classification with RNNs](../09_text_classification_rnns/09_text_classification_rnns.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}