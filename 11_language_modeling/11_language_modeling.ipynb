{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPf3TTY87bDA"
      },
      "source": [
        "# Module 11: Language Modeling\n",
        "\n",
        "**Predicting the Next Word**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHTxbHKh7bDC"
      },
      "source": [
        "## 1. Objectives\n",
        "\n",
        "- âœ… Understand language modeling fundamentals\n",
        "- âœ… Build LSTM language model\n",
        "- âœ… Evaluate with perplexity\n",
        "- âœ… Generate text with different sampling strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDbX4Hbe7bDD"
      },
      "source": [
        "## 2. Prerequisites\n",
        "\n",
        "- [Module 06: LSTM](../06_lstm/06_lstm.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPjQN_c87bDD"
      },
      "source": [
        "## 3. What is Language Modeling?\n",
        "\n",
        "### Task Definition\n",
        "Predict the probability of the next word given previous words:\n",
        "\n",
        "$$P(w_t | w_1, w_2, ..., w_{t-1})$$\n",
        "\n",
        "### Example\n",
        "```\n",
        "Input:  \"The cat sat on the\"\n",
        "Output: P(mat) = 0.3, P(floor) = 0.2, P(dog) = 0.01, ...\n",
        "```\n",
        "\n",
        "### Why It Matters\n",
        "- Foundation of GPT, ChatGPT, etc.\n",
        "- Text generation\n",
        "- Autocomplete\n",
        "- Speech recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoMP2oso7bDD",
        "outputId": "015e9459-07cc-484a-cdaa-2f1f46f79c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Wt12me7bDE"
      },
      "source": [
        "## 4. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG2NixTV7bDF",
        "outputId": "95d4522d-94d1-4fd1-9e33-c691ee99c377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 20\n",
            "Sample: [('<PAD>', 0), ('<UNK>', 1), ('<EOS>', 2), ('the', 3), ('cat', 4), ('dog', 5), ('a', 6), ('sat', 7), ('on', 8), ('mat', 9)]\n"
          ]
        }
      ],
      "source": [
        "# Sample corpus\n",
        "corpus = \"\"\"\n",
        "the cat sat on the mat\n",
        "the dog ran in the park\n",
        "a cat and a dog are friends\n",
        "the cat chased the mouse\n",
        "the dog barked at the cat\n",
        "\"\"\".strip().lower()\n",
        "\n",
        "# Build vocabulary\n",
        "words = corpus.split()\n",
        "word_counts = Counter(words)\n",
        "vocab = ['<PAD>', '<UNK>', '<EOS>'] + [w for w, _ in word_counts.most_common()]\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Sample: {list(word2idx.items())[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzMkdb6L7bDF",
        "outputId": "c7f28fcb-c8aa-4b8c-ffb2-0c488ccccc99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: torch.Size([26, 4]), Y shape: torch.Size([26, 4])\n",
            "\n",
            "Example:\n",
            "  Input:  ['the', 'cat', 'sat', 'on']\n",
            "  Target: ['cat', 'sat', 'on', 'the']\n"
          ]
        }
      ],
      "source": [
        "# Create training sequences\n",
        "def create_sequences(text, word2idx, seq_len=5):\n",
        "    \"\"\"Create (input, target) pairs for LM training.\"\"\"\n",
        "    tokens = [word2idx.get(w, 1) for w in text.split()]\n",
        "    inputs, targets = [], []\n",
        "\n",
        "    for i in range(len(tokens) - seq_len):\n",
        "        inputs.append(tokens[i:i+seq_len])\n",
        "        targets.append(tokens[i+1:i+seq_len+1])  # Shifted by 1\n",
        "\n",
        "    return torch.tensor(inputs), torch.tensor(targets)\n",
        "\n",
        "X, Y = create_sequences(corpus, word2idx, seq_len=4)\n",
        "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
        "print(f\"\\nExample:\")\n",
        "print(f\"  Input:  {[idx2word[i.item()] for i in X[0]]}\")\n",
        "print(f\"  Target: {[idx2word[i.item()] for i in Y[0]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eRdn5rO7bDF"
      },
      "source": [
        "## 5. LSTM Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5KhDXA-7bDF",
        "outputId": "e45245b2-6617-4ba1-fac1-1e81703a530e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: 235,284\n"
          ]
        }
      ],
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    \"\"\"LSTM-based language model.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim, hidden_dim, num_layers,\n",
        "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Tie weights (embedding and output)\n",
        "        if embed_dim == hidden_dim:\n",
        "            self.fc.weight = self.embedding.weight\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x: (batch, seq)\n",
        "        embedded = self.dropout(self.embedding(x))  # (batch, seq, embed)\n",
        "        output, hidden = self.lstm(embedded, hidden)  # (batch, seq, hidden)\n",
        "        logits = self.fc(self.dropout(output))  # (batch, seq, vocab)\n",
        "        return logits, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initialize hidden state.\"\"\"\n",
        "        h = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        c = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return (h, c)\n",
        "\n",
        "model = LSTMLanguageModel(len(vocab), embed_dim=64, hidden_dim=128, num_layers=2).to(device)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMae5T1g7bDF"
      },
      "source": [
        "## 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wozo5wvy7bDF",
        "outputId": "571b2ce4-9328-489e-dc34-8584535743e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Loss=2.5967, Perplexity=13.42\n",
            "Epoch 40: Loss=1.9175, Perplexity=6.80\n",
            "Epoch 60: Loss=1.0862, Perplexity=2.96\n",
            "Epoch 80: Loss=0.6107, Perplexity=1.84\n",
            "Epoch 100: Loss=0.4108, Perplexity=1.51\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    hidden = model.init_hidden(X.size(0))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits, _ = model(X, hidden)\n",
        "\n",
        "    # Reshape for loss: (batch*seq, vocab) vs (batch*seq,)\n",
        "    loss = criterion(logits.view(-1, len(vocab)), Y.view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        perplexity = torch.exp(loss).item()\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss.item():.4f}, Perplexity={perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZCJNm4d7bDF"
      },
      "source": [
        "## 7. Perplexity (Evaluation Metric)\n",
        "\n",
        "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_{<i})\\right)$$\n",
        "\n",
        "- Lower is better\n",
        "- Perplexity of 10 â‰ˆ \"choosing from 10 equally likely words\"\n",
        "- GPT-2: ~20-30 on standard benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyDIYl7h7bDF",
        "outputId": "4d421a5e-19e2-4157-bb09-2b4afc790b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 1.39\n"
          ]
        }
      ],
      "source": [
        "def compute_perplexity(model, X, Y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        hidden = model.init_hidden(X.size(0))\n",
        "        logits, _ = model(X, hidden)\n",
        "        loss = F.cross_entropy(logits.view(-1, len(vocab)), Y.view(-1))\n",
        "        return torch.exp(loss).item()\n",
        "\n",
        "ppl = compute_perplexity(model, X, Y)\n",
        "print(f\"Perplexity: {ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcjDuxti7bDF"
      },
      "source": [
        "## 8. Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peawnIPP7bDG",
        "outputId": "906861d3-f588-4182-9368-0c4d1a340318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy (temp=0.1):\n",
            "  the cat chased the mouse the dog barked at the cat chased the mouse the dog barked at the cat chased the\n",
            "\n",
            "Creative (temp=1.5):\n",
            "  the cat the dog in the a the park a cat and a dog are friends the cat chased the mouse the\n",
            "\n",
            "Top-k=3:\n",
            "  the cat chased the mouse the dog ran at the park a cat and a dog are friends the cat chased the\n"
          ]
        }
      ],
      "source": [
        "def generate(model, prompt, max_len=20, temperature=1.0, top_k=None):\n",
        "    \"\"\"\n",
        "    Generate text from prompt.\n",
        "\n",
        "    Args:\n",
        "        temperature: Higher = more random, Lower = more deterministic\n",
        "        top_k: Sample from top k tokens only\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tokens = [word2idx.get(w, 1) for w in prompt.lower().split()]\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        x = torch.tensor([tokens[-4:]]).to(device)  # Last 4 tokens\n",
        "        logits, hidden = model(x, hidden)\n",
        "\n",
        "        # Get next token logits\n",
        "        next_logits = logits[0, -1] / temperature\n",
        "\n",
        "        # Top-k sampling\n",
        "        if top_k:\n",
        "            values, indices = torch.topk(next_logits, top_k)\n",
        "            next_logits = torch.full_like(next_logits, float('-inf'))\n",
        "            next_logits.scatter_(0, indices, values)\n",
        "\n",
        "        # Sample\n",
        "        probs = F.softmax(next_logits, dim=0)\n",
        "        next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "        tokens.append(next_token)\n",
        "\n",
        "        if next_token == word2idx.get('<EOS>', -1):\n",
        "            break\n",
        "\n",
        "    return ' '.join([idx2word[t] for t in tokens])\n",
        "\n",
        "# Generate with different settings\n",
        "print(\"Greedy (temp=0.1):\")\n",
        "print(f\"  {generate(model, 'the cat', temperature=0.1)}\\n\")\n",
        "\n",
        "print(\"Creative (temp=1.5):\")\n",
        "print(f\"  {generate(model, 'the cat', temperature=1.5)}\\n\")\n",
        "\n",
        "print(\"Top-k=3:\")\n",
        "print(f\"  {generate(model, 'the cat', top_k=3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4tRsbEv7bDG"
      },
      "source": [
        "## 9. ðŸ”¥ Real-World Usage\n",
        "\n",
        "### LSTM LM â†’ Transformers\n",
        "\n",
        "| Era | Model | Perplexity |\n",
        "|-----|-------|------------|\n",
        "| 2017 | LSTM LM | ~50-100 |\n",
        "| 2018 | Transformer LM | ~30-50 |\n",
        "| 2019 | GPT-2 | ~20-30 |\n",
        "| 2020+ | GPT-3/4 | Even lower |\n",
        "\n",
        "### Key Concepts for LLMs\n",
        "- Same objective: predict next token\n",
        "- Same loss: cross-entropy\n",
        "- Same sampling: temperature, top-k, top-p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_0tjLDD7bDG"
      },
      "source": [
        "## 10. Interview Questions\n",
        "\n",
        "**Q1: What is perplexity?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "Exponentiated average cross-entropy loss. Lower is better. Perplexity of N means the model is \"as confused as choosing from N equally likely options.\"\n",
        "</details>\n",
        "\n",
        "**Q2: How does temperature affect generation?**\n",
        "<details><summary>Answer</summary>\n",
        "\n",
        "- Temperature < 1: More deterministic, picks high-probability tokens\n",
        "- Temperature = 1: Original distribution\n",
        "- Temperature > 1: More random, flatter distribution\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEdPEnbG7bDG"
      },
      "source": [
        "## 11. Summary\n",
        "\n",
        "- **Language Model**: P(next word | previous words)\n",
        "- **Training**: Cross-entropy loss, predict next token\n",
        "- **Perplexity**: exp(loss), lower is better\n",
        "- **Generation**: Temperature, top-k, top-p sampling\n",
        "- **Foundation**: Same principles power GPT, ChatGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjQBu_H67bDG"
      },
      "source": [
        "## 12. References\n",
        "\n",
        "- [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "- [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "---\n",
        "**Next:** [Module 12: Sequence-to-Sequence](../12_seq2seq/12_seq2seq.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}