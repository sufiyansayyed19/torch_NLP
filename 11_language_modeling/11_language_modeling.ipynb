{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 11: Language Modeling\n",
                "\n",
                "**Predicting the Next Word**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Objectives\n",
                "\n",
                "- âœ… Understand language modeling fundamentals\n",
                "- âœ… Build LSTM language model\n",
                "- âœ… Evaluate with perplexity\n",
                "- âœ… Generate text with different sampling strategies"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prerequisites\n",
                "\n",
                "- [Module 06: LSTM](../06_lstm/06_lstm.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. What is Language Modeling?\n",
                "\n",
                "### Task Definition\n",
                "Predict the probability of the next word given previous words:\n",
                "\n",
                "$$P(w_t | w_1, w_2, ..., w_{t-1})$$\n",
                "\n",
                "### Example\n",
                "```\n",
                "Input:  \"The cat sat on the\"\n",
                "Output: P(mat) = 0.3, P(floor) = 0.2, P(dog) = 0.01, ...\n",
                "```\n",
                "\n",
                "### Why It Matters\n",
                "- Foundation of GPT, ChatGPT, etc.\n",
                "- Text generation\n",
                "- Autocomplete\n",
                "- Speech recognition"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "from collections import Counter\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample corpus\n",
                "corpus = \"\"\"\n",
                "the cat sat on the mat\n",
                "the dog ran in the park\n",
                "a cat and a dog are friends\n",
                "the cat chased the mouse\n",
                "the dog barked at the cat\n",
                "\"\"\".strip().lower()\n",
                "\n",
                "# Build vocabulary\n",
                "words = corpus.split()\n",
                "word_counts = Counter(words)\n",
                "vocab = ['<PAD>', '<UNK>', '<EOS>'] + [w for w, _ in word_counts.most_common()]\n",
                "word2idx = {w: i for i, w in enumerate(vocab)}\n",
                "idx2word = {i: w for w, i in word2idx.items()}\n",
                "\n",
                "print(f\"Vocabulary size: {len(vocab)}\")\n",
                "print(f\"Sample: {list(word2idx.items())[:10]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create training sequences\n",
                "def create_sequences(text, word2idx, seq_len=5):\n",
                "    \"\"\"Create (input, target) pairs for LM training.\"\"\"\n",
                "    tokens = [word2idx.get(w, 1) for w in text.split()]\n",
                "    inputs, targets = [], []\n",
                "    \n",
                "    for i in range(len(tokens) - seq_len):\n",
                "        inputs.append(tokens[i:i+seq_len])\n",
                "        targets.append(tokens[i+1:i+seq_len+1])  # Shifted by 1\n",
                "    \n",
                "    return torch.tensor(inputs), torch.tensor(targets)\n",
                "\n",
                "X, Y = create_sequences(corpus, word2idx, seq_len=4)\n",
                "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
                "print(f\"\\nExample:\")\n",
                "print(f\"  Input:  {[idx2word[i.item()] for i in X[0]]}\")\n",
                "print(f\"  Target: {[idx2word[i.item()] for i in Y[0]]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. LSTM Language Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTMLanguageModel(nn.Module):\n",
                "    \"\"\"LSTM-based language model.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
                "        super().__init__()\n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.num_layers = num_layers\n",
                "        \n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
                "        self.lstm = nn.LSTM(\n",
                "            embed_dim, hidden_dim, num_layers,\n",
                "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
                "        )\n",
                "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        \n",
                "        # Tie weights (embedding and output)\n",
                "        if embed_dim == hidden_dim:\n",
                "            self.fc.weight = self.embedding.weight\n",
                "    \n",
                "    def forward(self, x, hidden=None):\n",
                "        # x: (batch, seq)\n",
                "        embedded = self.dropout(self.embedding(x))  # (batch, seq, embed)\n",
                "        output, hidden = self.lstm(embedded, hidden)  # (batch, seq, hidden)\n",
                "        logits = self.fc(self.dropout(output))  # (batch, seq, vocab)\n",
                "        return logits, hidden\n",
                "    \n",
                "    def init_hidden(self, batch_size):\n",
                "        \"\"\"Initialize hidden state.\"\"\"\n",
                "        h = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
                "        c = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
                "        return (h, c)\n",
                "\n",
                "model = LSTMLanguageModel(len(vocab), embed_dim=64, hidden_dim=128, num_layers=2).to(device)\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "X, Y = X.to(device), Y.to(device)\n",
                "\n",
                "for epoch in range(100):\n",
                "    model.train()\n",
                "    hidden = model.init_hidden(X.size(0))\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    logits, _ = model(X, hidden)\n",
                "    \n",
                "    # Reshape for loss: (batch*seq, vocab) vs (batch*seq,)\n",
                "    loss = criterion(logits.view(-1, len(vocab)), Y.view(-1))\n",
                "    loss.backward()\n",
                "    \n",
                "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "    optimizer.step()\n",
                "    \n",
                "    if (epoch + 1) % 20 == 0:\n",
                "        perplexity = torch.exp(loss).item()\n",
                "        print(f\"Epoch {epoch+1}: Loss={loss.item():.4f}, Perplexity={perplexity:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Perplexity (Evaluation Metric)\n",
                "\n",
                "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_{<i})\\right)$$\n",
                "\n",
                "- Lower is better\n",
                "- Perplexity of 10 â‰ˆ \"choosing from 10 equally likely words\"\n",
                "- GPT-2: ~20-30 on standard benchmarks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_perplexity(model, X, Y):\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        hidden = model.init_hidden(X.size(0))\n",
                "        logits, _ = model(X, hidden)\n",
                "        loss = F.cross_entropy(logits.view(-1, len(vocab)), Y.view(-1))\n",
                "        return torch.exp(loss).item()\n",
                "\n",
                "ppl = compute_perplexity(model, X, Y)\n",
                "print(f\"Perplexity: {ppl:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Text Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate(model, prompt, max_len=20, temperature=1.0, top_k=None):\n",
                "    \"\"\"\n",
                "    Generate text from prompt.\n",
                "    \n",
                "    Args:\n",
                "        temperature: Higher = more random, Lower = more deterministic\n",
                "        top_k: Sample from top k tokens only\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    tokens = [word2idx.get(w, 1) for w in prompt.lower().split()]\n",
                "    \n",
                "    hidden = model.init_hidden(1)\n",
                "    \n",
                "    for _ in range(max_len):\n",
                "        x = torch.tensor([tokens[-4:]]).to(device)  # Last 4 tokens\n",
                "        logits, hidden = model(x, hidden)\n",
                "        \n",
                "        # Get next token logits\n",
                "        next_logits = logits[0, -1] / temperature\n",
                "        \n",
                "        # Top-k sampling\n",
                "        if top_k:\n",
                "            values, indices = torch.topk(next_logits, top_k)\n",
                "            next_logits = torch.full_like(next_logits, float('-inf'))\n",
                "            next_logits.scatter_(0, indices, values)\n",
                "        \n",
                "        # Sample\n",
                "        probs = F.softmax(next_logits, dim=0)\n",
                "        next_token = torch.multinomial(probs, 1).item()\n",
                "        \n",
                "        tokens.append(next_token)\n",
                "        \n",
                "        if next_token == word2idx.get('<EOS>', -1):\n",
                "            break\n",
                "    \n",
                "    return ' '.join([idx2word[t] for t in tokens])\n",
                "\n",
                "# Generate with different settings\n",
                "print(\"Greedy (temp=0.1):\")\n",
                "print(f\"  {generate(model, 'the cat', temperature=0.1)}\\n\")\n",
                "\n",
                "print(\"Creative (temp=1.5):\")\n",
                "print(f\"  {generate(model, 'the cat', temperature=1.5)}\\n\")\n",
                "\n",
                "print(\"Top-k=3:\")\n",
                "print(f\"  {generate(model, 'the cat', top_k=3)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. ðŸ”¥ Real-World Usage\n",
                "\n",
                "### LSTM LM â†’ Transformers\n",
                "\n",
                "| Era | Model | Perplexity |\n",
                "|-----|-------|------------|\n",
                "| 2017 | LSTM LM | ~50-100 |\n",
                "| 2018 | Transformer LM | ~30-50 |\n",
                "| 2019 | GPT-2 | ~20-30 |\n",
                "| 2020+ | GPT-3/4 | Even lower |\n",
                "\n",
                "### Key Concepts for LLMs\n",
                "- Same objective: predict next token\n",
                "- Same loss: cross-entropy\n",
                "- Same sampling: temperature, top-k, top-p"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Questions\n",
                "\n",
                "**Q1: What is perplexity?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "Exponentiated average cross-entropy loss. Lower is better. Perplexity of N means the model is \"as confused as choosing from N equally likely options.\"\n",
                "</details>\n",
                "\n",
                "**Q2: How does temperature affect generation?**\n",
                "<details><summary>Answer</summary>\n",
                "\n",
                "- Temperature < 1: More deterministic, picks high-probability tokens\n",
                "- Temperature = 1: Original distribution\n",
                "- Temperature > 1: More random, flatter distribution\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n",
                "\n",
                "- **Language Model**: P(next word | previous words)\n",
                "- **Training**: Cross-entropy loss, predict next token\n",
                "- **Perplexity**: exp(loss), lower is better\n",
                "- **Generation**: Temperature, top-k, top-p sampling\n",
                "- **Foundation**: Same principles power GPT, ChatGPT"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. References\n",
                "\n",
                "- [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
                "- [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
                "\n",
                "---\n",
                "**Next:** [Module 12: Sequence-to-Sequence](../12_seq2seq/12_seq2seq.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}